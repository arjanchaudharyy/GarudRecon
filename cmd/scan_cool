#!/bin/bash

# Cool Scan Mode - Medium-level comprehensive scan
# Includes subdomain enumeration and extensive vulnerability checks

CONFIG_PATH="configuration/garudrecon.cfg"
source "$CONFIG_PATH" 2>/dev/null || true

DOMAIN=""
OUTPUT_DIR=""

# Parse arguments
while [[ "$#" -gt 0 ]]; do
    case "$1" in
        -d|--domain)
            DOMAIN="$2"
            shift 2
            ;;
        -o|--output)
            OUTPUT_DIR="$2"
            shift 2
            ;;
        *)
            echo "Unknown parameter: $1"
            exit 1
            ;;
    esac
done

# Validate input
if [[ -z "$DOMAIN" ]]; then
    echo "Error: Domain is required (-d)"
    exit 1
fi

if [[ -z "$OUTPUT_DIR" ]]; then
    OUTPUT_DIR="scans/cool-$(date +%Y%m%d-%H%M%S)-$DOMAIN"
fi

# Create output directory
mkdir -p "$OUTPUT_DIR"

# Initialize results JSON
RESULTS_FILE="$OUTPUT_DIR/results.json"
echo "{" > "$RESULTS_FILE"
echo "  \"scan_type\": \"cool\"," >> "$RESULTS_FILE"
echo "  \"domain\": \"$DOMAIN\"," >> "$RESULTS_FILE"
echo "  \"start_time\": \"$(date -Iseconds)\"," >> "$RESULTS_FILE"
echo "  \"findings\": {" >> "$RESULTS_FILE"

# Logging function
log() {
    echo "[$(date +'%Y-%m-%d %H:%M:%S')] $1"
}

log "Starting COOL scan for $DOMAIN"
log "Output directory: $OUTPUT_DIR"

# Step 1: Subdomain Enumeration
log "[1/10] Subdomain Enumeration..."
SUBDOMAINS_FILE="$OUTPUT_DIR/subdomains.txt"
touch "$SUBDOMAINS_FILE"

# Use multiple tools for better coverage
if command -v subfinder &> /dev/null; then
    log "Running subfinder..."
    subfinder -d "$DOMAIN" -silent -o "$OUTPUT_DIR/subfinder.txt" 2>/dev/null
    cat "$OUTPUT_DIR/subfinder.txt" >> "$SUBDOMAINS_FILE" 2>/dev/null
fi

if command -v assetfinder &> /dev/null; then
    log "Running assetfinder..."
    assetfinder --subs-only "$DOMAIN" 2>/dev/null >> "$SUBDOMAINS_FILE"
fi

if command -v amass &> /dev/null; then
    log "Running amass (passive mode)..."
    timeout 300 amass enum -passive -d "$DOMAIN" -o "$OUTPUT_DIR/amass.txt" 2>/dev/null
    cat "$OUTPUT_DIR/amass.txt" >> "$SUBDOMAINS_FILE" 2>/dev/null
fi

# Deduplicate and sort
sort -u "$SUBDOMAINS_FILE" -o "$SUBDOMAINS_FILE"
SUBDOMAIN_COUNT=$(wc -l < "$SUBDOMAINS_FILE")
log "Found $SUBDOMAIN_COUNT unique subdomains"

# Step 2: DNS Resolution
log "[2/10] Resolving subdomains..."
if command -v dnsx &> /dev/null && [[ -s "$SUBDOMAINS_FILE" ]]; then
    cat "$SUBDOMAINS_FILE" | dnsx -silent -a -resp -o "$OUTPUT_DIR/resolved_subdomains.txt" 2>/dev/null
    RESOLVED_COUNT=$(wc -l < "$OUTPUT_DIR/resolved_subdomains.txt" 2>/dev/null || echo "0")
    log "Resolved $RESOLVED_COUNT subdomains"
else
    cp "$SUBDOMAINS_FILE" "$OUTPUT_DIR/resolved_subdomains.txt" 2>/dev/null
fi

# Step 3: HTTP Probing
log "[3/10] HTTP Probing..."
if command -v httpx &> /dev/null && [[ -s "$OUTPUT_DIR/resolved_subdomains.txt" ]]; then
    cat "$OUTPUT_DIR/resolved_subdomains.txt" | cut -d' ' -f1 | httpx -silent -title -tech-detect -status-code -o "$OUTPUT_DIR/httpx.txt" 2>/dev/null
    LIVE_HOSTS=$(wc -l < "$OUTPUT_DIR/httpx.txt" 2>/dev/null || echo "0")
    log "Found $LIVE_HOSTS live hosts"
fi

# Step 4: Port Scanning
log "[4/10] Port Scanning..."
if command -v naabu &> /dev/null && [[ -s "$OUTPUT_DIR/resolved_subdomains.txt" ]]; then
    cat "$OUTPUT_DIR/resolved_subdomains.txt" | cut -d' ' -f1 | naabu -silent -top-ports 100 -o "$OUTPUT_DIR/ports.txt" 2>/dev/null
    log "Port scan complete"
elif command -v nmap &> /dev/null; then
    nmap -iL "$OUTPUT_DIR/resolved_subdomains.txt" -p 80,443,8080,8443,22,21,3389 -T4 --open -oN "$OUTPUT_DIR/ports.txt" &> /dev/null
    log "Port scan complete"
fi

# Step 5: URL Discovery
log "[5/10] URL Discovery..."
URLS_FILE="$OUTPUT_DIR/urls.txt"
touch "$URLS_FILE"

if command -v waybackurls &> /dev/null && [[ -s "$SUBDOMAINS_FILE" ]]; then
    cat "$SUBDOMAINS_FILE" | head -20 | waybackurls 2>/dev/null | head -5000 >> "$URLS_FILE"
fi

if command -v gau &> /dev/null && [[ -s "$SUBDOMAINS_FILE" ]]; then
    cat "$SUBDOMAINS_FILE" | head -20 | gau --threads 10 2>/dev/null | head -5000 >> "$URLS_FILE"
fi

if command -v katana &> /dev/null && [[ -s "$OUTPUT_DIR/httpx.txt" ]]; then
    cat "$OUTPUT_DIR/httpx.txt" | head -10 | katana -silent -d 2 2>/dev/null | head -2000 >> "$URLS_FILE"
fi

sort -u "$URLS_FILE" -o "$URLS_FILE"
URL_COUNT=$(wc -l < "$URLS_FILE")
log "Found $URL_COUNT unique URLs"

# Step 6: JavaScript Files Discovery
log "[6/10] JavaScript Discovery..."
if command -v subjs &> /dev/null && [[ -s "$SUBDOMAINS_FILE" ]]; then
    cat "$SUBDOMAINS_FILE" | head -30 | subjs 2>/dev/null | sort -u > "$OUTPUT_DIR/js_files.txt"
    JS_COUNT=$(wc -l < "$OUTPUT_DIR/js_files.txt")
    log "Found $JS_COUNT JavaScript files"
fi

# Step 7: XSS Testing
log "[7/10] XSS Vulnerability Testing..."
if [[ -f "$URLS_FILE" ]] && command -v dalfox &> /dev/null; then
    grep "=" "$URLS_FILE" | head -100 | dalfox pipe --silence --no-color -o "$OUTPUT_DIR/xss_findings.txt" 2>/dev/null &
    XSS_PID=$!
fi

# Step 8: SQL Injection Testing
log "[8/10] SQL Injection Testing..."
if [[ -f "$URLS_FILE" ]] && command -v sqlmap &> /dev/null; then
    grep "=" "$URLS_FILE" | head -30 > "$OUTPUT_DIR/urls_with_params.txt"
    if [[ -s "$OUTPUT_DIR/urls_with_params.txt" ]]; then
        while IFS= read -r url; do
            timeout 45 sqlmap -u "$url" --batch --level=2 --risk=1 --threads=3 --answers="follow=N" 2>/dev/null | grep -i "parameter.*vulnerable" >> "$OUTPUT_DIR/sqli_findings.txt" 2>/dev/null
        done < "$OUTPUT_DIR/urls_with_params.txt" &
        SQLI_PID=$!
    fi
fi

# Step 9: Subdomain Takeover Check
log "[9/10] Subdomain Takeover Check..."
if command -v subzy &> /dev/null && [[ -s "$SUBDOMAINS_FILE" ]]; then
    subzy run --targets "$SUBDOMAINS_FILE" --hide_fails --timeout 5 2>/dev/null | grep -i "vulnerable" > "$OUTPUT_DIR/subdomain_takeover.txt" 2>/dev/null &
    TAKEOVER_PID=$!
elif command -v nuclei &> /dev/null && [[ -s "$SUBDOMAINS_FILE" ]]; then
    nuclei -l "$SUBDOMAINS_FILE" -t takeovers/ -silent -o "$OUTPUT_DIR/subdomain_takeover.txt" 2>/dev/null &
    TAKEOVER_PID=$!
fi

# Step 10: Technology Detection & Security Headers
log "[10/10] Security Analysis..."
if [[ -s "$OUTPUT_DIR/httpx.txt" ]]; then
    # Extract unique URLs for security check
    cat "$OUTPUT_DIR/httpx.txt" | head -20 > "$OUTPUT_DIR/targets_for_security.txt"
    
    # Security headers check
    {
        echo "Security Headers Analysis"
        echo "=========================="
        while IFS= read -r target; do
            url=$(echo "$target" | awk '{print $1}')
            echo ""
            echo "Target: $url"
            curl -sI "$url" 2>/dev/null | grep -i -E "(strict-transport-security|x-frame-options|x-content-type-options|content-security-policy|x-xss-protection)"
        done < "$OUTPUT_DIR/targets_for_security.txt"
    } > "$OUTPUT_DIR/security_headers.txt" 2>/dev/null
fi

# Wait for background jobs
log "Waiting for vulnerability scans to complete..."
[[ -n "$XSS_PID" ]] && timeout 120 wait $XSS_PID 2>/dev/null
[[ -n "$SQLI_PID" ]] && timeout 120 wait $SQLI_PID 2>/dev/null
[[ -n "$TAKEOVER_PID" ]] && timeout 120 wait $TAKEOVER_PID 2>/dev/null

# Generate Summary
log "Generating summary..."

# Count findings
XSS_COUNT=0
SQLI_COUNT=0
TAKEOVER_COUNT=0
JS_COUNT=0
[[ -f "$OUTPUT_DIR/xss_findings.txt" ]] && XSS_COUNT=$(wc -l < "$OUTPUT_DIR/xss_findings.txt")
[[ -f "$OUTPUT_DIR/sqli_findings.txt" ]] && SQLI_COUNT=$(wc -l < "$OUTPUT_DIR/sqli_findings.txt")
[[ -f "$OUTPUT_DIR/subdomain_takeover.txt" ]] && TAKEOVER_COUNT=$(wc -l < "$OUTPUT_DIR/subdomain_takeover.txt")
[[ -f "$OUTPUT_DIR/js_files.txt" ]] && JS_COUNT=$(wc -l < "$OUTPUT_DIR/js_files.txt")

# Complete results JSON
cat >> "$RESULTS_FILE" << EOF
    "subdomains": $SUBDOMAIN_COUNT,
    "resolved_hosts": $RESOLVED_COUNT,
    "live_hosts": $LIVE_HOSTS,
    "urls_found": $URL_COUNT,
    "js_files": $JS_COUNT,
    "xss_findings": $XSS_COUNT,
    "sqli_findings": $SQLI_COUNT,
    "subdomain_takeover": $TAKEOVER_COUNT
  },
  "end_time": "$(date -Iseconds)",
  "status": "completed"
}
EOF

# Summary
cat > "$OUTPUT_DIR/summary.txt" << EOF
========================================
GarudRecon - COOL Scan Summary
========================================
Domain: $DOMAIN
Scan Type: Cool (Medium-Level)
Completed: $(date)

FINDINGS:
---------
Subdomains: $SUBDOMAIN_COUNT
Resolved Hosts: $RESOLVED_COUNT
Live Hosts: $LIVE_HOSTS
URLs Discovered: $URL_COUNT
JavaScript Files: $JS_COUNT
XSS Findings: $XSS_COUNT
SQLi Findings: $SQLI_COUNT
Subdomain Takeover: $TAKEOVER_COUNT

FILES GENERATED:
---------------
$(ls -1 "$OUTPUT_DIR" | sed 's/^/  - /')

========================================
EOF

cat "$OUTPUT_DIR/summary.txt"
log "Cool scan completed successfully!"
log "Results saved to: $OUTPUT_DIR"

exit 0
